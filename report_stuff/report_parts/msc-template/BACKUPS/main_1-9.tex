\documentclass[12pt,twoside]{report}
\usepackage{graphicx}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Definitions for the title page
% Edit these to provide the correct information
% e.g. \newcommand{\reportauthor}{Timothy Kimber}

\newcommand{\reporttitle}{Recurrent Neural Networks Applied for Human Movement in Subjects with Duchenne Muscular Dystrophy}
\newcommand{\reportauthor}{Daniel Heaton}
\newcommand{\supervisor}{Aldo Faisal}
\newcommand{\degreetype}{MSc Computing (Machine Learning)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% load some definitions and default packages
\input{includes}

% load some macros
\input{notation}

\date{September 2019}

\begin{document}

% load title page
\input{titlepage}


% page numbering etc.
\pagenumbering{roman}
\clearpage{\pagestyle{empty}\cleardoublepage}
\setcounter{page}{1}
\pagestyle{fancy}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
Your abstract.
\end{abstract}

\clearpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section*{Acknowledgments}
%Comment this out if not needed.

%\clearpage{\pagestyle{empty}\cleardoublepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%--- table of contents
\fancyhead[RE,LO]{\sffamily {Table of Contents}}
\tableofcontents 


\clearpage{\pagestyle{empty}\cleardoublepage}
\pagenumbering{arabic}
\setcounter{page}{1}
\fancyhead[LE,RO]{\slshape \rightmark}
\fancyhead[LO,RE]{\slshape \leftmark}













%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\part{Background and Basis of Research}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Project Overview and Background\\}


%%%%%%%%%%%%%%%%%%
\section{Motivation for Project and Purpose of Work}

\quad Modern machine learning algorithms and methodologies has seen great success in regards to being applied to biomedical data in order to provide insights that assist specialists and help diagnose potential conditions that may afflict subjects. One example of this is DeepMind’s recent progress with AI-assisted eye scans to detect over 50 different types of diseases potentially in a subject’s eye as accurately as world-leading expert doctors [1]. Deep learning techniques have also been utilized by HeartFlow to help build 3D models of subjects’ hearts and assess the impacts of blockages on blood flow to the heart [2]. Based on these breakthroughs, among others, in recent years, it is very probable that AI-assistance will become the new norm in various clinics and hospitals around the world within the next decade [3]. Taking note of the prominent applicability of artificial intelligence to the analysis of human movement in particular, Imperial College London have undertaken a research initiative in collaboration with Great Ormond Street Hospital to investigate the applicability of various AI techniques to the analysis of subjects with Duchenne muscular dystrophy [4], a form of muscular dystrophy that predominantly affects young males under the age of 12 and severely impacts their movement ability to varying degrees. The hope is that great strides in treatments can be achieved by utilizing artificial intelligence to inform and make decisions on a subject-by-subject basis and potentially draw insights about the condition. \\

\quad With regards to this project specifically that is undertaken as part of this research initiative, we wish to investigate the applicability of recurrent neural networks (RNNs) to the features of human movement data provided by body suit measurements captured from subjects with Duchenne muscular dystrophy (DMD). This will hopefully provide not only evidence on the applicability of these models to this sort of data, but also should provide important insights into the data itself and of the subjects providing it. These insights from the project will hopefully have a direct positive impact in the ability to assess subjects via the North Star Ambulatory Assessment (NSAA) and possibly provide insights into other features of the condition. Generally, in this project we are trying to gain insights about the body suit data that is captured as ‘.mat’ files (MATLAB data files) through the different measurements that are captured by the 17 sensors of the suit (joint angles, position, accelerometer values, etc.) of NSAAs or 6-minute walks assessments of the subjects by means of sequence modelling using RNNs. This use of sequence modelling is necessary to model the dependencies through time of measurements, and we are more likely to have a robust model if measurement values are treated as NON-independent with respect to time.\\

\quad The overall goal of the project is therefore to provide a deliverable that includes a complete system that works with varying forms of suit data, learns from it, and provides insights about it, while hopefully being able to be adapted to new subjects, new NSAA assessments of existing subjects, and help inform specialists of the severity of their condition. A significant hope, therefore, is to provide a software solution that positively and directly impacts the lives of those with DMD through hopefully making their assessments easier and more accurate.

%%%%%%%%%%%%%%%%%%
\section{Aims and Objectives}

\quad With the overall aim of the project outlined and with the motivation for undertaking the work provided above, we now turn our attention to covering some of the main aims of the project. These include:\\

\begin{itemize}
	\item Building a reasonably good model (with surrounding supporting scripts that are outlined later on) that, when presented with new, unseen ‘.mat’ files of body suit data, can give a reasonably good approximation of individual NSAA activity scores and an overall NSAA score (i.e. the accumulation of all individual activity scores). A prominent limitation currently, however, is the overall lack of data files: we have no more than 50 complete ‘.mat’ files in total for each of the  6-minute walk and NSAA assessments. This is primarily due to the fact that the data collection is currently an ongoing process and a large repository of previously-collected suit data from other subjects with DMD does not appear to exist that’s publicly available. Hence, an implicit requirement of the project is to be able to make the most out of the data we have available, such as using it to train a model to predict different things, use different measurements contained within the ‘.mat’ files, look at applying statistical analysis on the raw data, and so on.
	\item Being able to use trained model(s) to gain insights into the most influential activities and measurements from the ‘.mat’ files on overall NSAA score and to identify activities that correlate highly with overall assessment. In doing so, it could possibly enable the reduction of 17 activities needed for accurate overall NSAA assessment to far fewer if only a few are needed to correctly assess the subject. The conclusions that we could possibly draw from the project, therefore, hopefully have the potential to aid specialists in the practical undertaking of the assessments through minimizing the amount of testing the subjects have to do.
	\item Investigating the impact of training models on different types of source data directly. For example, we’d like to see whether or not it’s possible to train models on natural movement behaviour data sets to the same standard as if we were using NSAA data sets when training towards overall and individual NSAA scores. If this were to be the case, then there exists a real possibility of not requiring the NSAA assessments to be completed by subjects at all, and instead simply requiring the subject to undertake natural movement instead, which may be significantly easier and/or more practical for subjects.
	\item Building models that are trained only on one ‘version’ of assessments of subjects and attempt to generalise to subsequent versions. Here, by ‘version’ we mean an assessment of a subject that takes place at a certain time, with subsequent versions being the same assessment but taken 6-months later on. For example, a subject’s initial NSAA assessment would be stored as the subject name ‘D4’, and when that subject returns 6-months later to undertake their subsequent NSAA assessment the resultant data file would be stored as ‘D4V2’. The hope is therefore to train models on non-‘V2’ files and generalise to newly presented ‘V2’ files. This should provide an advisory tool for any specialists wishing to assess how a subject’s conditioned has progressed during the time between assessments.
	\item Looking into how possible it is to build models that generalise well to new subjects and the system settings needed to achieve this; by this, we mean models that are able to assess subjects that they have never come across before during training (which differs to the previous bullet point, which looks at new data from existing subjects). Therefore, if this were to be the case then we would be able to extend the applicability of this system to not only new assessments of the existing subjects but brand new subjects to the overall research initiative. There are numerous techniques that we would have to look into if the models have a problem generalizing, and so a large amount of the model predictions sets will focus on this aim.
	\item Package all the scripts and models necessary for a specialist or any other researcher wishing to use any of the built tools in a way that is easy to use and gives intuitive output. This requires us to construct the system in a way where it is possible to be uses by others outside of the development environment in order to be practically applicable to achieve the aims outlined above.\\
\end{itemize}

\quad With our overall project aims outlined above, it’s also useful to cover some of the objectives that we intend to achieve in order to complete these aims, many of which will be investigated within their own experiment set or model predictions set. These include:

\begin{itemize}
	\item Comparing models built from different measurements (e.g. joint angles, acceleration values, computed statistical values, etc.) on their performance of evaluating unseen sequences of data to an accurate D/HC classification and overall/single-act regression of NSAA scores.
	\item Evaluating the ideal values for different sequence setups for the data going into the model with respect to the performance for various output types. This includes finding the ideal sequence length, sequence overlap, and discard proportion of frames within the sequences.
	\item Investigating the ideal number of features needed for the raw measurements and computed statistical values to train a model. This will involve a trade-off, with more features providing more of the inherent variance within the data and fewer features making it easier for the model to learn from.
	\item Looking into how well models performed when evaluated on files from a different source directory than their own. For example, we’d like to investigate the potential to models built from NSAA files and assess subject files from the natural movement behaviour data set. If this is possible, then with the finished models we could use these to assess a subject based solely on their natural movement, which might be much more practical than requiring the subject to undertake the NSAA assessment.
	\item Investigating how well models perform when they are familiar with the subject as opposed to when the model has never seen the subject before in training (even if it was trained on different data from the same subject than was used for assessing the subject).
	\item Assessing the applicability of generalisation techniques that includes downsampling the data, adding Gaussian noise to the data set, concatenation of features for multiple measurement types, and the leaving-out of anomalies within the subjects.
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Basis of Research Project\\}

%%%%%%%%%%%%%%%%%%
\section{Duchenne Muscular Dystrophy}

\quad The data that we shall be working with for this project is from subjects who have varying severities of Duchenne muscular dystrophy (DMD). DMD is a genetic disorder that is characterized by progressive muscle degeneration and weakness and is caused by the absence of dystrophic, a protein that helps keeps muscle cells intact [5]. This leads to increasing levels of disability and is a progressive condition, meaning that it gets worse over time. It’s classified as a rare disease, with around 2,500 patients in the UK and an estimated 300,000 sufferers worldwide [6]. There are currently no known cures for any form of muscular dystrophy (MD), though there are treatments available to help manage the conditions [7].\\

\quad DMD is one of the more severe forms of MD and generally affects boys in their early childhoods, and those with the condition generally only live into their 20s or 30s. The muscle weakness starts in early childhood and symptoms are usually first noticed between the ages of 2 and 5. The weakness mainly affects the muscles near the hips and shoulders, so among the first signs of the disorder are when the child has difficulty getting up off the floor, walking, or running. The weakness progresses to eventually affect all muscles used for moving and also those involved in breathing and the heart muscle. Many are confined to a wheelchair by 12 years of age, with those in their late teens generally losing the ability to move their arms and experiencing progressive problems with breathing.\\

\quad With the aim to increase the life span and movement options of sufferers of DMD, a range of treatments are available. These range from steroids to increase muscle strength, physiotherapy to assist with mobility, and surgery to correct postural deformities. And thanks to advances in cardiac and respiratory care, life expectancy for sufferers is increasing and many are able to survive into their 30s and 40s with careers and families, with there even being cases of men with the condition living into their 50s. Additionally, there is ongoing research looking into ways to repair the genetic mutations and damaged muscles associated with various forms of MD.\\

%%%%%%%%%%%%%%%%%%
\section{The North Star Ambulatory Assessment}

\quad The North Star Ambulatory Assessment (NSAA) is a 17-item rating scale that is used to measure functional motor abilities in subjects with DMD and is generally used to monitor the progression of the disease and the effects of treatments. The tests are to be completed without the use of any thoracic braces or any equipment assistance that may help the subject to complete the activities. To carry out the assessments, the assessor conducting the assessments needs a mat, a stopwatch, a box step, a size-appropriate chair, and at least 10-metres of pathway [8].\\

\quad To carry out the assessment, the assessor gets the subject to carry out 17 sequential tasks. Each task is graded as follows: ‘2’ if there is no obvious modification of activity, ‘1’ if the subject uses a modified method but achieves the goal independent of physical assistance from another, and ‘0’ if the subject is unable to complete the activity independently. The 17 activities involved in the assessment with the requests to the subject are given below:

\begin{enumerate}
	\item \textbf{Stand}: "Can you stand up tall for me for as long as you can and as still as you can?”
	\item \textbf{Walk}: “Can you walk from A to B (state to and where from) for me?”
	\item \textbf{Stand up from chair}: “Stand up from the chair, keeping your arms folded if you can”
	\item \textbf{Stand on one leg – right}: “Can you stand on your right leg for as long as you can?”
	\item \textbf{Stand on one leg – left}: “Can you stand on your left leg for as long as you can?”
	\item \textbf{Climb box step – right}: “Can you step onto the top of the box using your right leg first?”
	\item \textbf{Climb box step – left}: “Can you step onto the top of the box using your left leg first?”
	\item \textbf{Descend box step – right}: “Can you step down from the box using your right leg first?”
	\item \textbf{Descend box step – left}: “Can you step down from the box using your left leg first?”
	\item \textbf{Gets to sitting}: “Can you get from lying to sitting?”
	\item \textbf{Rise from floor}: “Get up from the floor using as little support as possible and as fast as you can (from supine).”
	\item \textbf{Lifts head}: “Lift your head to look at your toes keeping your arms folded.”
	\item \textbf{Stand on heels}: “Can you stand on your heels?”
	\item \textbf{Jump}: “How high can you jump?”
	\item \textbf{Hop right leg}: “Can you hop on your right leg?”
	\item \textbf{Hop left leg}: “Can you hop on your left leg?”
	\item \textbf{Run (10m)}: “Run as fast as you can to….(give point).”
\end{enumerate}

\quad The NSAA assessment has been shown to be a quick, reliable, and clinically relevant method to measure the functional motor ability of ambulant children with DMD, and is also considered to be suitable to be used in research. It has also been shown to have high intra-observer reliability and high inter-observer reliability [9]. This means that NSAA is generally fairly reliable so as to be used as part of research assuming adequate training is provided to assessors. Furthermore, the hierarchy of items within NSAA was shown to be supported by clinical expert opinion, with items in the NSAA assessment being listed based on their level of difficult which is agreed upon by most experts, while a questionnaire-based study shows that clinicians generally consider NSAA as clinically relevant [10].\\

%%%%%%%%%%%%%%%%%%
\section{The KineDMD Research Initiative}

\quad The project undertaken as described in this report is part of a wider research initiative known as the ‘KineDMD’ study, conducted by Imperial College London in collaboration with Great Ormond Street Hospital (GOSH). The study involves around 20 DMD subjects (‘D’) subjects and 10 healthy control (‘HC’) subjects who participate in the study for 12 months, who are assessed wearing a sensor suit on selected days during clinical assessments at GOSH, along with using fitness tracker bracelets in the form of Apple watches throughout the trial, which collect data of everyday movements while the subjects are at home or school. A broad aim of the research initiative is to make use of AI to make sense of the data patterns collected from the suit and watches for each of the subjects which, from there, would aid doctors in being able to monitor disease progression with more precision [11]. The initiative has been funded with £320,000 through the Duchenne Research Fund to develop and test the bodysuit that captures the motions of subjects suffering with DMD [4].\\

\quad The hope is that insights found would cut down the time taken to test new treatments and thus drive down the costs of future clinical trials. A further aim of the initiative is that the developed suit and associated AI techniques and research projects undertaken as part of the initiative will help determine whether any new treatment regimes are working, which would be able to help inform doctors on future treatments. This is particularly useful for specialists, as the condition can be difficult to treat due to relatively slow progression and each subject responds uniquely; furthermore, many of the assessments are done by ‘eye’ instead of using measurement and objective methods. The initiative hopes that bringing AI techniques into the assessments will take a lot of the human fallibility elements out of the assessments and give a more informed perspective that is better able to understand the progression of the condition in the subjects.\\




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Overview of Recurrent Neural Networks\\}

%%%%%%%%%%%%%%%%%%
\section{Outline}

\quad As recurrent neural networks (RNNs) will form the basis of the engineering aspect of the project through implementations using Python and supporting libraries, it’s necessary to outline some of the theory and applications prior to implementing them; in doing so, we can explore why exactly they are useful when adapted to work with the body suit data we’re concerned with and to solve the problems we’re looking to solve. We begin by exploring the shortcomings of feedforward neural networks (FFNNs) and how using RNNs in their place works to overcome them (and, in particular, why they’re applicable here). We then touch on the mathematical basis of how the hidden nodes’ states change with respect to time and the input, along with how we use long short-term memory (LSTM) units to help deal with the vanishing/exploding gradient problems. Finally, we look at how we can implement this type of network within a Python script by means of the TensorFlow framework, including how we build the model, train it, and test it on unseen data.\\

%%%%%%%%%%%%%%%%%%
\section{Motivation, Architecture, and Training}

\quad A problem with FFNNs is that they don’t handle the order of data that is fed into the network: each sample fed through and classified or regressed is independent of all other samples. For example, if a convolutional neural network is trained to determine whether images are either of a cat or of a dog, then its assessment of each image’s label is independent (rightly so) of the images its seen before. This is appropriate in many situations; however, here we’re dealing with time-series data from the suit, where each row (‘frame’) of joint angle values, position values, and so on that are contained in a ‘.mat’ file produced by the body suit is a single sample in time. We also know instinctively that these values in real-life are dependent on previous values: for example, for a person in movement, the position of their various body parts are influenced by what they were a short time ago. FFNNs don’t handle order of the values of data that are fed in. For example, if there were inputted numerous frames of data from the body suit, then it would treat each as independent entities with regard to the network’s predictions.\\

\quad This lack of memory with FFNNs is something that RNNs attempt to fix in the following way: rather than the values of the hidden nodes of the FFNN being only affected by the values that feed into it (e.g. the network inputs or the values from the previous layer), hidden layers in RNNs are also affected by their own previous values. In contrast with FFNNs, RNNs share their weight parameters across different time steps: this allows a sequence to extend and apply the model to examples of different forms and generalize across them (which allows a sentence like “I went to Nepal in 2009” and “In 2009 I went to Nepal” to recognize the year, ‘2009’, in the same way)[12]. The resultant core architectural difference between the two can be seen in the image below:\\

\begin{center}
\includegraphics[scale=1.4]{project_figures/fig3_1}
\end{center}

\quad In this sense, the RNN can be seen to contain a memory of sorts that enforces time-dependencies of data that is fed through it. If we considered a sequential model where the state of a hidden node $h^t$ is modified by not only the input $x$ but also the layer’s previous state $h^{t+1}$, we can essentially ‘unfold’ this dependency with respect to time to get:\\

\begin{center}
\includegraphics[scale=0.8]{project_figures/fig3_2}
\end{center}

\quad In other words, the output of a hidden node at time $t$ is given as $h^{t}$ and is a function $f$ of the input at this time from the previous layer $x^t$ and the output of the same layer at the previous time step $h^{t-1}$. This can therefore be seen as the ‘memory’ aspect of an RNN: values from previous parts of a sequence carry over to influence the subsequent parts. It should be noted, however, that this is only within sequences and subsequent sequences are considered to be independent of each other (in the same way that images fed through a convolutional neural network are independent of each other); hence, the choosing of the correct time-dependency in the form of the sequence length is a key aspect of experimentation which we further look into in our experimentation. This idea of unfolding in time can be extended to apply to multiple layers and multiple nodes per layer and can be seen in the general equation that the hidden nodes in an RNN use to calculate their output:\\

\begin{center}
\includegraphics[scale=0.4]{project_figures/fig3_3}
\end{center}

\quad Note that the above function $\phi_h$ is equivalent to $f$ in the above diagram, as in both cases they represent the activation function for layer $h$. We interpret this as the output of a hidden layer at time step $t$ being a combination of the previous hidden layer output and the current input to the hidden layer, with both being modified by their respective weight matrices. It’s also important to note that, not only is there a weight matrix $W_{xh}$ that is learned through training to map from the previous layer’s values to the current one, but there is an additional weight matrix $W_{hh}$ that is learned to control how much of the same layer’s previous values impact the current layer. Alternatively, a way to look at it is the $W_{hh}$ matrix controls the influence each left-to-right arrow in the unfolded sequence image has on the state it points to, while the $W_{xh}$ controls how much influence up down-to-up arrow in the unfolded sequence image has on the state it points to.\\

\quad This requirement of using the additional weight matrix for the states of the hidden layers is also reflected in the backpropagation through time (BPTT) equation for the updating of the $W_{hh}$ matrix via gradient descent:\\

\begin{center}
\includegraphics[scale=0.27]{project_figures/fig3_4}
\end{center}

\quad The idea is that the overall loss $L$ is the sum of all the loss functions from times $t=1$ to $t=T$, and since the loss at time $t$ is dependent on the hidden units at all previous time steps, the gradient is as seen above. Note that its chain rule structure is still very similar to standard backpropagation used in FFNNs, with a primary difference coming from the impact of all previous values of the hidden layer prior to time $t$ on the overall derivative of loss with respect to $W_{hh}$. Below, we can also see how these derivative values propagate backwards through time:\\

\begin{center}
\includegraphics[scale=1.0]{project_figures/fig3_5}
\end{center}

\quad However, a large problem arises from the $\frac{\partial h^t}{\partial h^k}$  terms having $t-k$ multiplications in the above equation, which therefore multiplies the weight matrix $W_{hh}$ as many times. If this weight matrix is less than 1, this factor becomes very small, which results in a vanishing gradient, severely impacting the ability of the network to train on data and thus learn anything useful (the opposite happens if the weight matrix is greater than 1, which results in the network having an exploding gradient and never converging). To counteract this, a common way to implement sequence models is by using gated RNNs and, for this project, we chose to use LSTM units.\\



%%%%%%%%%%%%%%%%%%
\section{The Long Short-Term Memory RNN Architecture}

\quad The idea of using gated RNNs, which includes the LSTM architecture, is that we are able to create paths through time that have derivatives that neither vanish nor explode and involve connection weights that may change at each time step. Gated RNNs are also automatically able to decide when to clear a hidden state (i.e. set it to 0), and a core idea of LSTMs is to introduce loops within themselves to produce paths where the gradient can flow for long durations of time, while the weight on this internal path loop is conditioned on context, rather than fixed as in the standard RNN. The weight of this path is controlled by another unit and thus the time scale can be changed based on the input sequence [12]. A diagram of a single LSTM network cell and how it interacts with the wider RNN can be seen in the image below, with the LSTM unit itself being the central part of the three green boxes below:\\

\begin{center}
\includegraphics[scale=0.7]{project_figures/fig3_7}
\end{center}

\quad The central idea of this input is the horizontal line running through the top of the unit which allows the data to run along the unit relatively unchanged if required. The gates, represented in the above small yellow boxes within the central green box, allow the unit to let information in (we shall denote these as gates 1 to 4, where gate 1 is the leftmost yellow box and gate 4 is the rightmost box). These gates are there to protect and control the cell state [13].\\

\quad Gate 1 is the ‘forget gate’, which decides which information to ‘throw away’ from the cell state and is a combination of $h_{t-1}$ and $x_t$ and outputs a number via the sigmoid function $\sigma$ between 0 (which signifies to ‘get rid of this completely’) and 1 (‘keep this completely’). This could see applicability with a word sequence (i.e. a sentence) where we wish forget older parts of a sequence in order to make a more accurate assessment of the next word of the sequence based on more recently occurred words. The output of this gate $f^t$ is given as:\\

\begin{center}
\includegraphics[scale=0.7]{project_figures/fig3_8}
\end{center}

\quad Gate 2 is known as the ‘input gate’, which decides the values we’ll update within the cell in order to store new information in the cell state. This is used in conjunction with Gate 3, which is a ‘tanh’ gate that creates a vector of new candidate values that can be added to the state. The equations governing the outputs of each of these two parts are given as:\\

\begin{center}
\includegraphics[scale=0.7]{project_figures/fig3_9}
\end{center}

\quad These two are multiplied together to get the new information that we wish to store in the cell, which replaces the old information that we have lost via the ‘forget gate’.\\

\quad With the information we wish to discard having been forgotten and the new information that we wish to replace it with having been calculated, we then turn to modifying the old cell state $C_{t-1}$ into the new cell state $C_t$. This is done by multiplying the previous cell state by the forget gate output to forget the things we decided earlier to forget, followed by adding the new proposed candidate values scaled by how much we wish to update each value, and is given by the equation that governs the new cell states information:\\

\begin{center}
\includegraphics[scale=0.7]{project_figures/fig3_10}
\end{center}

\quad Now that we have the cell state obtained, we finally decide how much of this information to output from the cell (i.e. a filtered version of the cell). We then use Gate 4 (the ‘output gate’) to decide which parts of the cell we shall output, which we multiply by the ‘tanh’ of the new cell state $C_t$ (which makes sure the cell state outputs between -1 and 1). This ensures that we only output the parts we decided to output (e.g. in the case of the language model it allows one to only output information pertinent to verbs if that is what comes next in the sequence). The output of the output gate and of the cell itself is thus given as:\\

\begin{center}
\includegraphics[scale=0.7]{project_figures/fig3_11}
\end{center}

\quad In using LSTMs as part of our architecture, we enable the models to condition themselves on sequences with some parts forgotten within the cell and also being able to chose which parts of hidden states it wishes to output to the next layer and the next state of the same hidden layer. As a result, this architecture of the hidden units is much more conducive to modelling long-term relationships within a sequence and also being able to train via backpropagation with much reduced effects from the vanishing and exploding gradient problems.\\



%%%%%%%%%%%%%%%%%%
\section{Implementing an RNN using TensorFlow}

\quad With all that said, there still must be a practical way of implementing RNNs using LSTM units as part of the project for the RNN architecture to actually be useful for us. Fortunately, there exists numerous open source APIs and Python libraries that handles much of the underlying details of a neural network that simply needs the user to design the architecture. For this project, TensorFlow was chosen to be the API of choice due to previous experience in using it for implementing RNNs in time-series data in other work, along with excellent supporting documentation being available and the ability to easily utilize a GPU to help with training the model. Further justification can be found in the ‘System Choices’ chapter of the report.\\

\quad A detailed guide on building an RNN using TensorFlow is beyond the scope of this report; however, it was felt worthwhile to outline some of the central elements of the models that are built in ‘rnn.py’ and how they relate to the concepts outlined above. While we shall give a detailed breakdown of the script itself in the ‘Script Ecosystem Overview’ chapter of the report, we now turn our attention to specific sections of code within ‘rnn.py’ that are particularly significant to the architectural makeup of the models:\\

\begin{itemize}
	\item The input shape of the model is setup with a placeholder variable that sets the input size equal to (x, y, z), where $x$ is the batch size (i.e. number of sequences per training batch), $y$ is the sequence length (i.e. the number of frames of data that is ‘pushed through’ the model per sequence; generally either 60 for raw measurement data or 10 for computed statistical values), and $z$ is the dimensionality of the frames itself (e.g. 66 for joint angle data).
\begin{center}
\includegraphics[scale=0.8]{project_figures/fig3_12}
\end{center}
	The equivalent is also done for the $y$ data, depending on the output type the model is training towards.
	\item We define the LSTM cells, with their size and number of cells (i.e. equivalent to the number of nodes per hidden layer and number of hidden layers, respectively, if we were using a ‘traditional’ RNN) in a single line of code: we define multiple \textit{BasicLSTMCell} objects with a size set as \textit{self.lstm\_size} (a hyperparameter that we can tune), a dropout percentage given as \textit{tf\_keepprob}, and create multiple of these in a loop, with the number of these given as \textit{self.num\_layers}. These multiple cells (i.e. hidden layers of the model) are then used to create a \textit{MultiRNNCell} object, which acts as a wrapper for all the hidden layers of the model:
\begin{center}
\includegraphics[scale=0.6]{project_figures/fig3_13}
\end{center}
	\item Finally, we set up the model architecture so that the input $x$ held in the placeholder \textit{tf\_x} feeds into the ‘cells’ (i.e. the hidden layers), which modifies the initial state of the model throughout the application of the input sequence $x$ to the hidden layers to give us an output \textit{lstm\_outputs} and a final state of the layers, \textit{self.final\_state}:
\begin{center}
\includegraphics[scale=0.6]{project_figures/fig3_14}
\end{center}
	\item These steps are defined within the \textit{build()} method for the ‘RNN’ class which is called upon by the constructor of the object at object creation. Hence, when we create an RNN object as…
\begin{center}
\includegraphics[scale=0.5]{project_figures/fig3_15}
\end{center}
…it results in setting the attributes of the RNN object, including most of the hyperparameters that influence the architecture of the object, and calling the \textit{build()} method of the object that uses many of these hyperparameters. Thus, the above statement sets up the computational graph that defines our RNN model which is now ready to have data inputted through it for the training process.
	\item To train the model, the method \textit{train()} is called by the ‘rnn’ object, which results in splitting the training data components \textit{x\_train} and \textit{y\_train} into batches, whereupon each batch-pair (i.e. a batch of \textit{x\_train} with a batch of \textit{y\_train}) is placed into a dictionary that matches train components to batches (i.e. it would match a batch of \textit{x\_train} to the \textit{tf\_x} placeholder variable described above, which ensures that the \textit{x\_train} batch is used as input to the model). Each of these dictionaries are then fed through via the \textit{session.run()} method that specifies we are training the model (which hence calls upon the optimizer within \textit{build()} to train the model) and takes in the dictionary to train the model on each batch:
\begin{center}
\includegraphics[scale=0.6]{project_figures/fig3_16}
\end{center}
	\item A similar process is then used when we wish to test the model via the \textit{predict()} method called by the ‘rnn’ object. Much like \textit{train()}, it splits the \textit{x\_test} data into batches, which it adds to the ‘feed’ dictionary (setting the dropout probability to 0\% this time via \textit{tf\_keepprob:0: 1.0} and the session to use the \textit{test\_state} of the model) and calls the \textit{sess.run()} method to push this dictionary through the model to get the predictions made on the batch. We retrieve the predictions based on the output type we are working towards and adds the predictions obtained to the list of predicted values for the \textit{x\_test} input:
\begin{center}
\includegraphics[scale=0.6]{project_figures/fig3_17}
\end{center}
\end{itemize}

\quad While there are, however, many other steps in the process of using the ‘rnn.py’ to build the models, these are some of the crucial steps where we applied knowledge of RNNs and their usefulness to sequence modelling to create a deep learning solution in TensorFlow. And with easy access to other libraries that make reading in data from ‘.csv’ and ‘.xlsx’ files and manipulating it as matrix data easy (e.g. via ‘pandas’ and ‘numpy’) and general-purpose machine learning libraries such as ‘sk-learn’ to help with other tasks (such as the splitting and shuffling of sequences for training/testing, the evaluating of various metrics like mean squared error, and so on), we have all the resources needed to build RNN models using LSTM units using the applicable preprocessing steps to tailor it towards working with our data pipeline and produce results that can be observed and compared within experiment sets and model predictions sets.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\part{System Preparation: Choices and Setup}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{System Choices: Language, IDE, and Libraries\\}


%%%%%%%%%%%%%%%%%%
\section{Python}

\quad As with most software-related projects, one of the primary choices that must be made is what programming language to implement the components of the system in, along with what development environment it is to be built using. Both of these have a large impact in the time and ease it will take to develop the system, as well as how optimal it will be running in its final variation. For the choice of programming language, we chose to use \textbf{Python (3.6)} to build all scripts from for the following reasons:


\begin{itemize}
	\item It takes very few lines to implement many things when compared with other languages like Java; for example, the code below shows how a neural network can be implemented in Python in only 9 lines using the Keras library (a wrapper for TesorFlow):
\begin{center}
\includegraphics[scale=0.8]{project_figures/fig4_1}
\end{center}
	This enables faster development and easier testing of new ideas and concepts than other languages, as we can afford to care less about problems of tricky syntax (e.g. with using C++) and can instead move towards development ‘at the speed of thought’.
	\item The open-source nature of Python's community means that there are very often libraries that others have built that fit the profile of what we need, so we don't need to 'reinvent the wheel' by creating our own version of it; this can be seen in the system’s reliance on external functions from ‘scikit-learn’ to compute metrics, along with ‘pandas’ to handle the reading and writing to and from ‘.csv’ files, as opposed to writing our own functions to carry out this functionality.
	\item Python has seen extensive use for building and testing machine learning and deep learning models by research and business communities; thus, it is easily the most well-developed with regards to open-source libraries such as TensorFlow, with TensorFlow's Python API being most complete of its various language implementations [14].
	\item Many of the frameworks and libraries that power the system we have built (e.g. NumPy and SciPy) build upon lower-layer Fortran and C implementations for fast and vectorized operations for multidimensional arrays, which helps overcome the inferior speed of a scripting language like Python when compared to these lower-level languages [15].
	\item Further development of the system is easier, as being written in a language like Python (which is close to English) lends itself to the easier understanding of what is going on within each script. Coupled with variables having intuitive names and commenting where necessary, this helps with anyone undertaking edits or rewrites to one or more of the scripts in the future.
\end{itemize}

%%%%%%%%%%%%%%%%%%
\section{Integrated Development Environment}

\begin{center}
\includegraphics[scale=1]{project_figures/fig4_2}
\end{center}
\quad With regards to where we shall develop the Python programs for this project, we have chosen to use the \textbf{PyCharm Community Edition 2016} integrated development environment. We can see above a screenshot of how the IDE is setup for this project. This has been chosen for the following reasons over a text editor or another IDE:

\begin{itemize}
	\item \underline{\textit{Multiple program tabs}}: This enables the easy transitions between different scripts. This is particularly useful when we are making changes to multiple scripts simultaneously (e.g. adding the same optional argument to both ‘rnn.py’ and ‘mode\_predictor.py’ that ensures certain models built by ‘rnn.py’ are then retrieved correctly by ‘model\_predictor.py’).
	\item \underline{\textit{In-built terminal}}: This allows us to run programs from command line within the IDE itself without having to open a separate terminal window and navigating to the script directory every time it’s reopened. This is a major quality of life improvement, as most of the scripts are run from the command line with required and optional arguments.
	\item \underline{\textit{Good compatibility with git}}: This has two main benefits. The first is that changes made to scripts and their specific lines of change from a previous commit are highlighted in blue, which helps with accounting for modifications made when writing commit messages and keeping track of all recent changes made. The other  benefit is the GUI approach to committing to the GitHub repo (as can be seen below) which is a much easier way to make regular commits and also highlights easier more subtle changes made (such as writing lines to output files).
\begin{center}
\includegraphics[scale=1]{project_figures/fig4_3}
\end{center}
	\item \underline{\textit{Previous experience}}: We’ve used it before for many previous Python projects, including in two professional roles, for a final-year undergraduate individual project, and for many pieces of coursework involving the use of several machine learning and deep learning libraries such as ‘scikit-learn’ and ‘TensorFlow’. Hence, this previous experience and the resultant familiarity with the environment helps make development of this project a more expedient and easier experience.
	\item \underline{\text{Debugging and error handling}}: The layout of PyCharm makes for writing and immediate testing and modifying of code very simple, with debug options showing locations of compilation errors very easily and with clarity. This minimizes the time lost in development due to basic syntax errors and other basic software-engineering-related issues.
	\item \underline{\text{Package implementation}}: It’s easy to add additional packages via 'Available Packages' in the 'Project Interpreter', which is useful as we need to add many additional libraries, from TensorFlow to simple libraries like ‘pyexcel’.
\end{itemize}


%%%%%%%%%%%%%%%%%%
\section{TensorFlow}

\quad For programming in Python, there are numerous options for which library we can use to implement our central RNN models. One option is the ‘Keras’ wrapper that wraps the TensorFlow framework. Although this is a lot simpler to use and has fewer aspects to manually code, there are numerous advantages that TensorFlow has over this that includes the following:

\begin{itemize}
	\item More extensive and highly detailed documentation and examples for TensorFlow.
	\item Higher amount of direct control over the RNN models with things such as weights and optimizers.
	\item Better performance with TensorFlow through threading and queues to speed up the training process.
	\item Availability of the TensorBoard visualization tool to help understand our models.
\end{itemize}

\quad Thus, using TensorFlow will hopefully lead to more successful RNN models that can better learn from raw measurements and computed statistical values that can thus better operate on newly-presented subject data. Preparatory work for using this framework includes prior use as the engine for an undergraduate individual project, along with reading Chapter 14 and 16 of [15] where we learned:

\begin{itemize}
	\item The benefits of using TensorFlow for neural network training performance in utilizing GPU cores, where using a high-end GPU resulting in ~15 times more floating-point calculations per second than using an equivalently-priced CPU.
	\item Concepts of graphs, sessions, ranks, tensors and operations, which gave clarity to the concepts of the computational graph structure used by TensorFlow.
	\item The 'placeholder' concept of  TensorFlow where a variable is an 'empty' variable that expects data input (in our case, these 'placeholders' will be implemented for \textit{x\_train}/\textit{x\_test}).
	\item How aspects specific to an RNN works in TensorFlow, such as implementing layers as LSTM cells and initial/final states for the variables.
\end{itemize}

\quad With this obtained knowledge from the above textbook, along with examination of other examples found primarily on GitHub or the TensorFlow documentation website, we felt confident enough in our knowledge of the TensorFlow library that, coupled with prepared input data, our RNN models could now be created.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{System Setup: Software and Data Preparation\\}

%%%%%%%%%%%%%%%%%%
\section{Overview}

\quad A necessity to either continue further work on this system, validate the results we obtained in experimentation and elaborate upon in this report, or produce one’s own results through system use is to setup the required components to run the system. Before being able to modify or run parts of the system, there are several things that must be setup beforehand. This includes:

\begin{itemize}
	\item Obtaining relevant system resources.
	\item Downloading the requisite data sets and setting them up in the correct directories.
	\item Setting up Python and ‘pip’, along with the IDE and necessary packages.
	\item Running all the setup scripts that are run via the‘setup.cmd’ script.
	\item Setup of TensorFlow to use a GPU.
\end{itemize}

\quad Once these steps are all completed, the editing of scripts, building of models, testing of files, and so on can be done by the user. In this part of the report, we shall be going through all the necessary steps to run the system on a different workstation; the hope is that, with the steps completed in this section, any user with the necessary computational resources can build models, reproduce results, and carry out additional experiments using the suit data captured that is used as part of the project.

%%%%%%%%%%%%%%%%%%
\section{Necessary System Resources}

\quad As this system works with large amounts of data and requires a heavy computational workload in order to build and test models, among other tasks, anyone running parts of this system requires a workstation setup with the necessary resources to execute many of the scripts, store the data, and so on. The vast majority of the work done on this project has been undertaken on a ‘Dell XPS 15 (9570)’ laptop with the following specs:

\begin{itemize}
	\item \underline{\textit{CPU}}: 'Intel Core i7-8750H’
	\item \underline{\textit{RAM}}: ‘16GB DDR4, 2666MHz’
	\item \underline{\textit{GPU}}: ‘Nvidia GeForce GTX 1050Ti with 4GB GDDR5’
	\item \underline{\textit{Storage}}: ‘512GB SSD’
\end{itemize}

A system with similar specs should be adequate to run the system; however, the following is ideal:

\begin{itemize}
	\item \underline{\textit{CPU}}: At least a 7th gen Intel i5 or i7 (or AMD equivalent). A lot of the data preparation, computing of statistical values, reading from and writing to ‘.csv’s, and so on are done using the CPU (i.e. anything the system does that’s not including the training, testing, and assessing of models); hence, a good CPU will enable the user to run these scripts in reasonable time.
	\item \underline{\textit{RAM}}: Minimum of 10GB needed; some of the larger datasets we look at (e.g. when multiple raw measurements’ data are combined into one data set for a data shape of (~16000, 60, ~180)) require in excess of 8GB of memory just for the data, not including resources required for the IDE and other parts of the script being run. Any less than 10GB, therefore, may risk system instability or limit the user from carrying out certain parts of the experimentation outlined in this report. Additionally, DDR4 is recommended so as to increase the speed at which data is able to be written and read from memory.
	\item \underline{\textit{GPU}}: We make heavy use of TensorFlow running using the inbuilt GPU; the alternative would be to use the CPU, which by our estimation is approximately 10x slower to train models than using the GPU. Hence, to realistically build models in this system we need a good GPU. Ideally, the user would use an ‘Nvidia’ card as that is the easiest to setup with TensorFlow and, preferably, the card would have many CUDA cores (the 1050ti has 768) to enable faster parallel processing when training models.
	\item \underline{\textit{Storage}}: As of writing, the total storage required for all data sets contained within the ‘local directory’ (including ‘.mat’ files for NSAA assessments, 6-minute walks, natural movement behaviour, and the intermediate data extracted from all files via the data pipeline) amounts to over 170GB, while the system itself contained within the ‘project directory’ requires approximately 725MB of space; hence at least this much storage is required, ideally on an SSD to enable fast read speed from storage (which will happen a lot during the setup scripts). See the chapter on the ‘Project and Local Directories’ for more information about what these specific directories contain.
\end{itemize}


%%%%%%%%%%%%%%%%%%
\section{Data Sets Setup}

\quad With a workstation obtained with the requisite resources, the next step is obtaining the data sets required by the system. There are two approaches that can be taken: the first involves being able to access the link shown below (which should be possible for anyone with Imperial College London credentials), which contains all the data used as part of this project (i.e. that also contains all the intermediate data constructed via the scripts that make up the data pipeline). Hence, a user that downloads all the data from this link would not need to run the Python scripts contained within the ‘setup.cmd’ script, only the pip installation commands. The second approach should be taken if the user either doesn’t have Imperial College London credentials or otherwise can’t access the files, or if one wishes to run the pipeline ‘from scratch’ (i.e. computing ones own intermediate data files via the data pipeline); this does require the user to fully run the ‘setup.cmd’ script. Note that the below explanations assume the user already has access to the complete ‘project directory’ if one is reading this report; however if not, consult the chapter on ‘Project and Local Directories’ for guidance on how to access this.\\

The first approach is as follows:

\begin{enumerate}
	\item Setup a base directory in the user’s storage directory; the default name for this that has been used thus far has been ‘C:\textbackslash msc\_project\_files\textbackslash ’; however, a more intuitive name can be chosen by the user if desired. This becomes the user’s ‘local directory’ for the project.
	\item Download each of the directories contained within the OneDrive link: \url{https://imperiallondon-my.sharepoint.com/:f:/g/personal/djh18\_ic\_ac\_uk/Euymu00dXG1Cmmeoz3xxq24BekH57ZuDmU9uZtoSr60xfg?e=L5C62Z} and place each directory in the ‘msc\_project\_files’ directory (e.g. ‘left-out’, ‘allmatfiles’, etc.) within the user’s created ‘local directory’.
	\item Modify the requisite line within ‘$<$project directory$>$\textbackslash source\textbackslash settings.py’ to point to this new location. For example, if the user has setup the ‘local directory’ as ‘example’ in ‘C:\textbackslash’, then they should modify the ‘local\_dir’ variable (line 7) to now contain: ‘local\_dir=”C:\textbackslash example\textbackslash”. In doing this, it ensures that all other scripts that need to access the data files in ‘example’ are correctly pointed to it.
	\item Once the steps outlined in the section below on Python, pip, PyCharm, and the necessary packages have been undertaken, open ‘$<$project directory$>$\textbackslash source\textbackslash\\ batch\textbackslash setup.cmd’ for editing in a text editor, comment out line 19 and onward (as these create the intermediate data from the pipeline which we now already have), save the file, and run it to setup the Python packages.
\end{enumerate}

The second approach is as follows:

\begin{enumerate}
	\item Setup a base directory in the user’s storage directory; the default name for this that has been used thus far has been ‘C:\textbackslash msc\_project\_files\textbackslash’; however, a more intuitive name can be chosen by the user if desired. This becomes the user’s ‘local directory’ for the project.
	\item Obtain permission from the owners of the data sets used as part of the KineDMD research initiative to access and download the directories.
	\item Create another directory within the ‘local directory’ called ‘output\_files’. This shall contain a number of things produced by the scripts and by the models, including the ‘.csv’ files of computed statistical values, the constructed models themselves, among other parts of the system.
	\item The user should have links to the following data sets (though if they don’t the requisite permission for each must be obtained by the relevant parties): ‘NSAA’, ‘NMB’, ‘allmatfiles’, ‘6MW-matFiles’, and ‘6minutewalk-matfiles’. Each of these directories should then be downloaded and directly placed within ‘local\_dir’.
	\item Once the steps outlined in the section below on Python, pip, PyCharm, and the necessary packages have been undertaken, run the ‘setup.cmd’ in full to obtain the necessary Python packages for the project and compute the intermediate data used as part of the project.
\end{enumerate}

\quad With these steps done, the data should now be in a form in which all the scripts that form the system should be able to access with the necessary directories constructed.


%%%%%%%%%%%%%%%%%%
\section{Setup of Python, Pip, Necessary Packages, and PyCharm}

\quad We now turn our attention to the setting up of the language, the ‘pip’ tool for package installation, and the libraries required to run the scripts. The first step is the installation of Python; the version this system runs on is ‘3.6.0’ and, while installing any subsequent versions should be acceptable, we shall install this version to avoid any possible complications to do with the language further down the line. Version ‘3.6.0’ can be downloaded from \url{https://www.python.org/downloads/release/python-360/} and by selecting the relevant installer from ‘Files’. With the installation window open, ensure that the ‘Add Python 3.6 to PATH’ box is checked; this shall ensure that the user is able to run Python commands from the command line or terminal:

\begin{center}
\includegraphics[scale=0.8]{project_figures/fig5_1}
\end{center}

\quad With this installed, we can ensure that Python is setup correctly with the required version by entering ‘python --version’ at the command line:

\begin{center}
\includegraphics[scale=0.8]{project_figures/fig5_2}
\end{center}

\quad It should be noted that the installation window for the user should also say that it is installing ‘pip’. This can be asserted to have been installed for the user by entering ‘pip’ at the command line, which should show output of something like this:

\begin{center}
\includegraphics[scale=0.8]{project_figures/fig5_3}
\end{center}

\quad If, for whatever reason, ‘pip’ has not been installed, follow the steps outlined at \url{https://pip.pypa.io/en/stable/installing/}, which includes the modification of the PATH environment variable to ensure that we are able to use ‘pip’ correctly.\\

\quad Once ‘pip’ has been installed and/or asserted to be setup ready to use, we are able to install the required packages. Navigate to the ‘$<$project directory$>$\textbackslash source\textbackslash batch\_files’ directory and execute the ‘setup.cmd’ script. This will setup the required packages to run the project. Not only that, but it will ensure that the versions of each of the packages installed for this system will also be setup by the user; this should minimize the chances that the user will encounter dependency issues between packages (different versions of ‘tensorflow’ have been shown to interact badly with certain versions of ‘numpy’, for example). It should be noted that ‘setup.cmd’ will also run all the Python scripts within the ‘source’ directory that are used to setup the files for other scripts such as ‘rnn.py’ and ‘model\_predictor.py’, so if the user doesn’t wish to run these at this time, the recommended way is to comment out each of these lines. This can be done by commenting out each of these setup lines (line 19 and onward) by inserting ‘REM ‘ at the beginning of each line. Note that this means that ‘setup.cmd’ must be run later when the user wishes to prepare the data for the system and if the user doesn’t already have the intermediate data.


%%%%%%%%%%%%%%%%%%
\section{Installation of PyCharm (IDE)}

\quad If the user is intending to do any long-term modifications to the system, or if they simply want a more convenient place to launch the scripts from, then it is recommended that they install an IDE for the project; specifically, PyCharm Community Edition. Using this provides several advantages to the user:

\begin{itemize}
	\item An in-built terminal to run the scripts of the system with the necessary arguments.
	\item Easy interaction with Git to work from the project code in GitHub/GitLab.
	\item Syntax assistance when editing any files.
	\item Multiple tabs to help with editing multiple files simultaneously along with the script variable explorer.
\end{itemize}

To setup the IDE, the following steps should be undertaken:

\begin{enumerate}
	\item Download the community edition of PyCharm, the link for which can be found at: \url{https://www.jetbrains.com/pycharm/download/#section=windows}.
	\item In the ‘Installation Options’ window, it’s recommended that the user selects the ‘Add “Open Folder as Project”’ option (to allow opening the ‘project directory’ as a PyCharm project) and the ‘.py’ association (so Python files open in PyCharm as default).
	\item Launch PyCharm and select ‘Do not import settings’.
	\item From the ‘Customize PyCharm’ window, click ‘Skip Remaining and Set Defaults’.
	\item Prior to continuing with PyCharm setup, we first must setup Git if the user doesn’t have it already. The following section applies to Windows, but the equivalent can easily be done for iOS or Linux.
	\begin{enumerate}
		\item Download Git from \url{https://git-scm.com/download/win}.
		\item Run the installation, making note of where Git is installed, with the default settings.
	\end{enumerate}
	With this now completed (or not, depending on whether Git is already installed), the user should launch PyCharm and in the ‘Welcome to PyCharm’ window, click ‘Configure’ and ‘Settings’, followed by navigating to ‘Version Control’ and Git. In the ‘Path to Git executable’, enter the location of the ‘git.exe’ program that was just installed (or previously installed). This can be found within the ‘Git\textbackslash bin\textbackslash’ directory within the location where Git was setup. Click ‘Apply’ and ‘OK’.
	\item In the ‘Welcome to PyCharm’ window, select ‘Check out from Version Control’ and Git. This is because we will be directly installing the ‘project directory’ directly from ‘GitHub’. Note that it’s recommended doing this even if the source directory has already been downloaded and setup elsewhere, as doing it this way ensures that it is easily to modify and commit to git if any changes to the scripts are to be made. Within this new window, enter the URL: \url{https://github.com/dan-heaton/MSc_indiv_project} within the URL window and click ‘Clone’ to clone the repository. Alternatively, if one wishes to clone from GitLab instead, swap the URL above with: \url{https://gitlab.doc.ic.ac.uk/djh18/MSc_indiv_project}.
	\item The final step is to associate PyCharm with the Python interpreter we have previously installed. To do so, with the project opened in PyCharm, navigate to ‘File’ and ‘Settings’. Under ‘Project: MSc\_indiv\_project’ and ‘Project Interpreter’, click the settings icon and ‘Add...’. Under ‘System Interpreter’ the Python executable should already be detected within ‘Python36\textbackslash’ as ‘python.exe’. However, modify this path to the ‘python.exe’ file if has not done so already. Click ‘OK’, ‘Apply’, and ‘OK’. In the main PyCharm window, it may take a few seconds to configure this interpreter but, once done, the user will be able to edit and run the scripts of the system within PyCharm.
\end{enumerate}


%%%%%%%%%%%%%%%%%%
\section{Configuring of TensorFlow to Use the GPU}

\quad The following section works on the assumption that the user is working on a workstation containing a GPU. This is more or less a necessity to build models using ‘rnn.py’: while a reasonable GPU with $>$700 CUDA cores builds a typical model in 5-15 minutes, building these using an equivalently-priced CPU would take 1-3 hours. While the necessary steps to undertake the complete setup of a GPU are somewhat arduous, there exists a useful guide to doing so at \url{https://www.tensorflow.org/install/gpu}, along with a CUDA installation guide at \url{https://docs.nvidia.com/cuda/} for multiple OS’s. This includes details on setting up the CUDA toolkit and the CuDNN (CUDA Deep neural Network library), which are required to run TensorFlow on the GPU. With this setup, TensorFlow with subsequently run as default in all scripts using the GPU to create models as opposed to using the CPU.











%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\part{System Overview and Explanation} 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Project and Local Directories: Overview and Explanations\\}


%%%%%%%%%%%%%%%%%%
\section{Background}

\quad Prior to undertaking an in-depth discussion of the individual scripts, their uses, and the various experiment and model prediction sets we undertook as part of this project, it’s worth giving a brief explanation of the two types of directories that we are concerned with for this project. There are two directories that we are concerned with: the ‘project directory’ (containing the source files, the documentation used and written to, among other things) and the ‘local directory’ (containing the source ‘.mat’ files that we use as inputs to scripts and some of the outputs of the models, including the models themselves). Below, we cover each of the two in turn, how to access and/or set them up, and so on. The aim is thus to educate any users on how the project is laid out and how each part of the system connects to each other.


%%%%%%%%%%%%%%%%%%
\section{The Project Directory}

\quad The project directory is the directory containing all of the source code, the information required about the NSAA subjects (‘nsaa\_6mw\_info.xlsx’), the documents containing the results of the experiment sets and model prediction sets, and other reports and presentations constructed throughout the duration of the project. Hence, this is where the majority of the deliverables of the project lie, with the exception of the majority of the models that were created throughout the project and the data that is required to train the models. The reason we keep these apart (i.e. why project directory and local directory are not synonymous) is as follows:

\begin{itemize}
	\item The local directory requires $>$170GB of storage for all the data sets that is used in the project. However, we’ve been making extensive use of storing the project directory within DropBox and as a Git repository and, as it would be not possible and very impractical, respectively, to store the local directory on both DropBox and within GitHub or GitLab, we chose to keep these separate.
	\item When we run several of the data pipeline scripts (e.g. ‘comp\_stat\_vals.py’ or ‘ext\_raw\_measures.py’) we end up producing a lot of new files within the local directory). Therefore, to avoid constant DropBox synchronizations or many new or deleted files to appear in each Git commit, it was felt that it would be easier to simply keep the data aspects apart from the source code and other documentation.
	\item Permission may be required to deal with certain data directories contained within the local directory, as this contains data about ongoing subjects of a research initiative that is not freely available. Hence, it was the desire to keep the project directory available to whoever wished to access it, without predicating access on also being able to access the data required to populate the local directory (e.g. if one wished simply to browse or borrow ideas from the scripts within the project directory); this also enables and encourages an easier transition to applying the project to other data sets possibly for other domains.
\end{itemize}

\quad To access the complete project, the advisable way to obtain it would be to clone it via GitHub. The repo can be accessed at \url{https://github.com/dan-heaton/MSc\_indiv\_project} and cloned via \url{https://github.com/dan-heaton/MSc\_indiv\_project.git}. Alternatively, one can access it via GitLab at \url{https://gitlab.doc.ic.ac.uk/djh18 /MSc\_indiv\_project} and clone with \url{https://gitlab.doc.ic.ac.uk/djh18/MSc\_indiv\_project.git}.  The current name for the project directory as used in its local form for development has been ‘indiv\_proj; however, one could rename this freely without requiring any other changes to the scripts. However, it’s recommended that the directories without the project directory should not be changed (e.g. ‘source’ or ‘documentation’), as doing so would require rewriting of several of the scripts that hardcoded paths to access things outside of its own directory.

Broadly speaking, the directories can be broken down as the following:

\begin{itemize}
	\item \textbf{background\_research}: this folder contains some preparatory programs that were written (with help from sources cited in the scripts themselves) to familiarize oneself with the building of RNNs with the chosen libraries in order to make using them for the actual project later that much easier; hence, these aren't used by the rest of the project at all and are included for historical documentation reasons only.
	\item \textbf{documentation}: contains a number of files pertaining to the outputs of the data pipeline for the project. This includes the following:
	\begin{itemize}
		\item \textit{'RNN Results.xlsx'}, which covers the performances of various model setups on test data (i.e. a large proportion of the experimentation that covers different types of raw measurements, sequence lengths, overlaps, etc., source their results from here). These are what form the basis of our later discussion on the experiment sets.
		\item \textit{'model\_predictions.csv'}, which (unlike 'RNN Results.xlsx') shows the performance of using 'model\_predictor.py' to assess the performance of pretrained models on whole data files. These provide the information needed for the model predictions sets, as discussed later. 
		\item \textit{'model\_shapes.xlsx'}, which is just to be used by 'model\_predictor.py' to set the sequence length to the correct value (and is not particularly relevant to the user).
		\item \textit{'nsaa\_6mw\_info.xlsx'}, which contains a table of the subject names and their corresponding single-act and overall NSAA scores (this provides the necessary \textit{y\_labels} for many RNN models).
		\item \textit{'nsaa\_17subtasks\_matfiles.csv'}, which is the Google annotations sheet that was collaboratively created by others within the wider research initiative that contains the file names and times within said files where the 17 NSAA activities are performed by each of subjects. This is determined by watching the source videos of the ‘.mov’ files of the subjects performing the activities and recording at what times in the video these activities occur, along with making use of the ‘dis\_3d\_pos.py’ script; this sheet is then used by ‘mat\_act\_div.py’ to create the single-act files used in later model predictions sets.\\
	We also have several other subdirectories in ‘documentation’:
		\item \textbf{Graphs}: all graphs created by 'graph\_creator.py' are placed in here. These source from 'RNN Results.xlsx' and 'model\_predictions.csv' to create graphs that are easier to display the results of groups of experiments done than it would be to display the same information using a table. We see many of these graphs later on within the discussion of the experiment sets.
		\item \textbf{Script Explanations}: collection of 'READMEs' for each of the scripts within 'project\_files\textbackslash source'. The idea is that, if one wishes to find out what each script does, why it was written, etc., then reading its relevant 'README' should provide sufficient detail. Much of these READMEs form the basis of our script overview later on.
	\end{itemize}
	\item \textbf{paper\_reviews}: contains paper reviews done of research papers that are believed to be relevant to the project. These predominantly focus on the use of RNNs when applied to real-world human movement data, and each paper consists of a slightly-shortened bullet-pointed version of the paper and then a section of the most significant points from these bullet points. Hence, these papers are useful in justifying decisions taken with respect to model choices, experimentation directions, etc.
	\item \textbf{presentations}: contains a collection of presentations that have been created to display to group members about the project's progress thus far (which were kept in order to aid in final report writings).
	\item \textbf{report\_stuff}: contains several initial reports and other documentations of project progress thus far, and also 'MSc Project Plan.ods', which is where the already-completed and upcoming task lists are stored; this is particularly useful if one wishes to see what is currently being worked on or has recently been completed. The vast majority of the contents of this directory, however, are contained within this report.
	\item \textbf{source}: contains all of the scripts that are needed by the project pipeline to run. This includes the core Python scripts (such as 'comp\_stat\_vals.py' and 'rnn.py'), along with some 'supporting' scripts, such as 'settings.py' (to contain global variables that are used across several scripts). For information about how to run each of these scripts, run the script of interest through the command line/terminal with the ‘--help' optional argument set (e.g. ‘python comp\_stat\_vals.py --help'). This will display each of the arguments that are available to be set, the significance of each, how they interact with other arguments (if relevant), etc.
	\begin{itemize}
		\item \textbf{Batch files}: within this, we contain the batch scripts that are used to automate some of the running of the scripts. For further info about the significance of any or all of the scripts, consult the 'README(s)' for the relevant scripts in '$<$project directory$>$\textbackslash documentation\textbackslash Script Explanations\textbackslash', the script ecosystem overview in 'plans\_and\_presentations', or the diagram of the scripts and their connections to each other found at the beginning of the ‘Script Ecosystem Overview’ chapter. Along with some of the simpler automation of the tasks, we also run each of the model predictions sets from their respective batch files, as many of them require building many models and testing many different combinations of files, which require many runs of ‘rnn.py’ and ‘model\_predictor.py’; hence, the automation of this makes the process of replication hopefully much easier for the user.
	\end{itemize}
\end{itemize}


%%%%%%%%%%%%%%%%%%
\section{The Local Directory}

\quad The local directory is considered to have two purposes: to store the data sets that we make use of in this project, and to store the direct outputs of the ‘rnn.py’ scripts that include the models themselves and the ‘.csv’ output predictions that are written directly on a sequence by sequence basis (e.g. for a model created with ‘rnn.py’ using a test set of 1000 sequences, there will exist within this ‘.csv’ each 1000 predicted value and true value, depending on the output type set by the user). There are three reasons why we include this ‘rnn.py’ output within the local directory:

\begin{itemize}
	\item As we create many models as part of this project, the size of this directory has become an issue and thus we would prefer to keep it separate from the project directory for space reasons. We therefore want to keep a lot of this data ‘clutter’ apart from the what is considered the ‘core’ of the project with the project directory.
	\item We also want to keep a consistent philosophy with which we consider to be ‘intermediate data’, which is data that exists as a product of one script and that is used by other scripts: in this case, the models produced are intermediate data in that they are created by ‘rnn.py’ and used by ‘model\_predictor.py’. This holds for other forms of intermediate data such as data produced by ‘comp\_stat\_vals.py’ and ‘ext\_raw\_measures.py’, and so we wish to do the same for the models and ‘rnn.py’ predictions output.
	\item The majority of these models are only used once as part of one particular experiment set or model predictions set, and therefore they don’t form a part of the ‘complete’ system (with the exception of the final selected models that are contained within the ‘source’ directory, though this is only a small number of the overall number of created models). Thus, we keep these models separate from the ones constituting the completed system at the end of all experimentation; in other words, the models that are intended to be used by a user to do assessments with specific subjects are contained within the project directory, while the models created during all experimentation are contained within the local directory.
\end{itemize}

\quad To access the complete local directory of files that we have been used for this project, use the OneDrive link given as \url{https://imperiallondon-my.sharepoint.com/:f:/g/personal/djh18\_ic\_ac\_uk/Euymu00dXG1Cmmeoz3xxq24BekH57ZuDmU9uZtoSr60xfg?e=L5C62Z} where one can find a directory given as ‘msc\_project\_files’. This is the local directory as used during the development of the project. Download and store it while modifying the local variable in ‘settings.py’ (as directed in the ‘System Setup’ chapter). Note that the total directory is in excess of 170GB, so sufficient space may need to be made for it beforehand.


%%%%%%%%%
\subsection{The Local Directory: 'rnn.py' Outputs}

\quad We first look at the two sub-directories within the local directory containing the outputs of ‘rnn.py’:

\begin{itemize}
	\item \textbf{output\_files\textbackslash rnn\_models}: this contains all models that have been produced by ‘rnn.py’ throughout the course of the project, including the final models used that are contained within the project directory. Each model’s contents are the product of the TensorFlow library working through ‘rnn.py’ and each model consists of a directory that looks something like this:
\begin{center}
\includegraphics[scale=0.3]{project_figures/fig6_1}
\end{center}

\quad These files constitute a single model in the eyes of ‘model\_predictor.py’, which is the only script that makes used of these models. For instance, within these files contains the model input and output shapes, the numbers of hidden layers, other hyperparameter settings, and the weights of each of the neuron connections. In other words, it’s a fully trained model that is ready to be used by ‘model\_predictor.py’ to be used on a whole subject’s data file.\\\\
\quad The names of the models may seem unnecessarily long and complex, but they are in fact simply the exact arguments used to invoke the instance of ‘rnn.py’ that creates this specific model, excluding the always-necessary ‘python rnn.py’ parts of the argument sequence. There are two reasons why we do this:
	\begin{itemize}
		\item This creates the names of the directories automatically, which takes much of the human-element of labelling each directory out of the process based on what experiment set or model prediction set it is used for.
		\item In having them written by name based on the ‘rnn.py’ arguments, we can ensure that ‘model\_predictor.py’ will always use the correct models during experimentation by writing a simple set of rules within ‘model\_predictor.py’. For instance, if we want ‘model\_predictor.py’ to use models that have been created with added Gaussian noise, we can ensure that it uses only model directories that contain ‘--noise' within their names. Hence, it’s an easier way to determine the correct models to use rather than analysing the contents of the model directory (e.g. which would mean looking into the ‘model.ckpt.meta’ file) to determine if it’s a model we need to use.
	\end{itemize}
	\item \textbf{output\_files\textbackslash RNN\_outputs}: each model that is built by ‘rnn.py’ that is also tested on a test partition of data (i.e. if the ‘--no\_testset’ argument is not set) writes a separate ‘.csv’ file to this directory. Each file contains, for a given created model, the arguments used to invoke it (in the form of the ‘.csv’ file name), the ‘overall’ results of the test set on the model (e.g. MAE of data for the ‘overall’ output type, accuracy of data for the ‘dhc’ output type, etc.), the individual predictions made for each test sequence versus their true values, and the model hyperparameter settings. An example of this can be seen below:
\begin{center}
\includegraphics[scale=0.2]{project_figures/fig6_2}
\end{center}
\end{itemize}

\quad The contents of this file are entirely optional to use, however, as all the requisite information about the test set performance (that is to be inputted into ‘RNN Results.xlsx’ and then used as part of experiment sets) are also produced as console output, which is easier to copy over for the user. However, this files serve as a log of model performance through time as we continue to create more models if we wish to use them as a reference at any point.



%%%%%%%%%
\subsection{The Local Directory: Data Sets}

\quad With the directories containing model outputs having been covered, we shall now look at the directories containing the raw data sets, what is contained within each directory, and what ‘type’ of data these directories contain. It should be noted that the ‘source’ scripts assume that each of these directories are a constant (i.e. that any other users don’t modify the names of the directories); any changes made to the names here require modifications to the necessary variables within ‘settings.py’ (e.g. the ‘sub\_dirs’ variable).\\

\quad Prior to looking at each data set in turn, it’s preferable to briefly discuss the general locations of raw source data (e.g. as source ‘.mat’ files) and intermediate data. This becomes particularly relevant when we shall shortly be discussing the contents of each data directory, and knowing what produced intermediate data and where is conducive towards understanding the data pipeline. Below, we can see how each script within the data pipeline produces data and where the data is stored:

\begin{enumerate}
	\item \textbf{'comp\_stat\_vals.py'}: This script takes the data stored as source ‘.mat’ files from within the data directories within the local directory and outputs data to the corresponding path within 'output'. For example, if ‘comp\_stat\_vals.py’ intends to operate on ‘NSAA’ (based on the ‘dir’ argument passed to it), it sources its data from '$<$local directory$>$\textbackslash NSAA\textbackslash matfiles' as ‘.mat’ files and produce the computed statistical values in '$<$local directory$>$\textbackslash output\_files\textbackslash NSAA\textbackslash AD' as ‘.csv’ files. This functions in the same way for other source data sets (e.g.' 6minwalk-matfiles') where the path to the computed statistical value files is more-or-less the same as the source directory path with 'output\_files' appended after the path to the local directory.
	\item \textbf{'ext\_raw\_measures.py'}: Unlike computed statistical values, the produced raw measurement files are instead stored within subdirectories of the source data set directory that they are sourced from, as opposed to a subdirectory of 'output\_files'. For example, if ‘ext\_raw\_measures.py’ intends to extract the raw measurements from the '6minwalk-matfiles' directory, it retrieves files from '$<$local directory$>$\textbackslash 6minwalk-matfiles\textbackslash all\_data\_mat\_files' and writes each measurement extracted per file to a sub-directory with a name matching the measurement name (e.g. ‘D4’s joint angle data is written to '$<$local directory$>$\textbackslash 6minwalk-matfiles\textbackslash all\_data\_mat\_files\textbackslash jointAngle' as ‘D4-6MinWalk-jointAngle.csv’ while its position data is written to '$<$local directory$>$\textbackslash 6minwalk-matfiles\textbackslash all\_data\_mat\_files\textbackslash position' as ‘D4-6MinWalk-position.csv’, and so on).
	\item \textbf{'mat\_act\_div.py'}: It should be noted first that, as this script extracts single-act files from complete-act files, it therefore only expects to be used on the 'NSAA' directory, as only files from 'NSAA' contain NSAA activities. When activities are divided, they are placed either within 'act\_files' or 'act\_files\_concat' (depending on whether ‘--single\_act\_concat’ was set for the script) within the 'NSAA' directory itself, much like ‘ext\_raw\_measures.py’. For example, ‘mat\_act\_div.py’ would pull source ‘.mat’ files from '$<$local directory$>$\textbackslash NSAA\textbackslash matfiles' and write single-act files (non-concatenated) to '$<$local directory$>$\textbackslash NSAA\textbackslash matfiles\textbackslash act\_files' as source ‘.mat’ files but only containing single-activities within each. Note that if ‘ext\_raw\_measures.py’ then operates on these single-act files, they get written to a subdirectory within here, much like how it would operate on complete-act files; hence, drawing the joint angle files from the above case would write the files to '$<$local directory$>$\textbackslash NSAA\textbackslash matfiles\textbackslash act\_files\textbackslash jointAngle'. Similarly, when we compute the statistical values of single-act files, they get written to the 'output\_files' directory; in the above case, the computed statistical values of the single-act NSAA files would be written to '$<$local dir$>$\textbackslash output\_files\textbackslash NSAA\textbackslash AD\textbackslash act\_files'.
	\item \textbf{'ft\_sel\_red.py'}: To simplify the process of storing the feature-reduced variants of the ‘.csv’ outputs of ‘comp\_stat\_vals.py’, we decided to store the files in the exact same directory as the files they operate on. For example, if we wish to reduce the dimensions of the computed statistical value files for the 'NMB' source data directory, we would write the feature-reduced files to '$<$local directory$>$\textbackslash output\_files\textbackslash NMB\textbackslash AD', where the computed statistical values are already stored. The difference here, however, is that the feature-reduced equivalents will have ‘FR\_’ appended to the front of each of the files. For example, the computed statistical values for the ‘D4’ subject whose data is from 'NMB' will be stored within the above path as ‘AD\_D4\_stats\_features.csv’, and when ‘ft\_sel\_red.py’ operates on this subject, it will take the data from this file and write to a file stored in the above path as ‘FR\_AD\_D4\_stats\_features.csv’ (alternatively, if the feature-reduced-concatenation option is set within ‘ft\_sel\_red.py’ it will instead be stored as ‘FRC\_AD\_D4\_stats\_features.csv’). This naming convention ensures that ‘rnn.py’ fetches the feature reduced variants of files to ensure that models of input nodes size >4000 isn’t required.
	\item \textbf{'rnn.py'}: Although this is considered part of the data pipeline, this simply fetches the data from all of the above locations dependent on the arguments given, while we have already discussed the output locations of ‘rnn.py’ in the previous section.
\end{enumerate}

\quad With the relationship between the data pipeline scripts and the data set directories having been established, we now move on to examining the data that’s contained within each of these directories. The data directories are as follows:

\begin{itemize}
	\item \textbf{6minwalk-matfiles}: This contains the 6-minute walk data of many (but not all) of the subjects within two sub-directories within this directory: 'all\_data\_mat\_files' and 'joint\_angles\_only\_matfiles'. The former contains the source ‘.mat’ files for all available measurements for the subjects’ walking assessments, while the latter contains source ‘.mat’ files with only joint angles. Therefore, what we consider ‘JA’ and ‘DC’ files (‘Joint Angle’ and ‘Data Cube’, respectively) that are referenced in experiment set 1 comes from 'joint\_angles\_only\_matfiles', while in most other cases when we use the data of 6-minute walk assessments, we use the 'all\_data\_mat\_files' directory.
	\item \textbf{6MW-matFiles}: This directory also contains some of the 6-minute walk assessment files for some of the subjects and, while some of the files overlap with '6minwalk-matfiles', it also contains assessment data that is not found in the other directory. Additionally, we don’t see any ‘joint angle only’ data within this data set, and so all source ‘.mat’ files are stored directly within this directory.
	\item \textbf{allmatfiles}: This contains the natural movement behaviour as source ‘.mat’ files that contains only the joint angle data (much like '6minwalk-matfiles\textbackslash joint\_angles\_only\_matfiles' files). As we only had the natural movement in ‘joint angle only’ form for quite a while (until we were given the 'NMB' data set), we had to make use of this for several of the later model predictions sets, though we later received the natural movement behaviour in true ‘AD’ form (i.e. containing all the measurements data for each subject) as 'NMB'.
	\item \textbf{left-out}: This is a small sample of data files that have been excluded from the main data sets that can act as data that is left-out of any of the data sets used to train models. It should be noted the difference between assessing using ‘model\_predictor.py’ on files from left-out as opposed to 'left-out' subjects (as used in many model predictions sets): assessing a file from the 'left-out' directory will assess a model that is familiar with the subject (through having been trained on other files of the same subject), while assessing a complete left-out subject will assess a model that is not familiar with \underline{any} files for the specific subject.
	\item \textbf{NMB}: This is the complete ‘AD’ data for the natural movement behaviour, as opposed to 'allmatfiles' which only contains the joint angle data in source ‘.mat’ file form. As a result of receiving this data late in the project lifecycle, we are only able to use this data set in later model predictions sets. It should be noted the sheer number of files per subject: many of the subjects have up to 30 files captured from each of them that will have captured a variety of ‘natural’ activities, such as sitting, playing, eating, and so on. The data contained within these files, therefore, is much more ‘unstructured’ than either the 6-minute walk or NSAA assessment data.
	\item \textbf{NSAA}: This contains the data for each of the subjects’ full NSAA assessments, and the source ‘.mat’ files specifically are contained within the 'NSAA\textbackslash matfiles' subdirectory. It should be noted that, for some subjects, their assessments are split over several files. We account for this when extracting raw measurements and computing statistical values by simply concatenating these source files with respect to time, while we select the correct file to use to get the single-act files via ‘mat\_act\_div.py’ by referencing the file name columns within the Google annotations sheet.
\end{itemize}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Reference Documents Explanation\\}

%%%%%%%%%%%%%%%%%%
\section{Background}

\quad As we can see from the system overview diagram shown at the beginning of the ‘Script Ecosystem Overview’ chapter, as well as having many Python and batch scripts that play integral roles within the system we also make heavy use of several documents that provide much needed information for training models and serve as places to store the results of experiments. These include the Google sheet of single-act annotations, the reference document containing the overall and individual scores of each subject used in the project, the ‘model\_predictions.csv’ file, and the ‘RNN Results.xlsx’ file. Each of these files in turn can be found within the project source directory within the ‘$<$project directory$>$\textbackslash documentation’ directory. In the section below, we will discuss file each in turn, from the information they contain to how and where they are used within the system (i.e. what other scripts depend on them and what scripts feed into the documents). The aim here, therefore, is to give a more concrete understanding of what these documents contain prior to seeing them referenced in the results discussion and general system overview sections.

%%%%%%%%%%%%%%%%%%
\section{Google Annotations Sheet ('nsaa\_17subtasks\_matfiles.csv')}

\begin{center}
\includegraphics[scale=0.4]{project_figures/fig7_1}
\end{center}

\quad As part of the wider research initiative, we collaboratively undertook to analyse and record the times of each activity undertaken by the subjects as part of the initiative. The information that was collected into this document, therefore, was intended to be used as part of several projects that relied on the start and end times of each activity undertaken by the subjects. As a result, the subjects’ corresponding activity videos were divided into three parts and each of us determined the activity times for our given subjects.\\

\quad The process to undertake these annotations was as follows:

\begin{enumerate}
	\item Load either the source video that corresponds to the ‘.mat’ file (which is provided in the data sets as ‘.mov’ files) OR use the corresponding ‘.mat’ file with the ‘dis\_3d\_pos.py’ script to load a basic 3D dynamically updating image through the ‘matplotlib’ library (more on this in the ‘Script Ecosystem Overview’ chapter).
	\item For each activity of the NSAA assessment set (e.g. ‘raise from floor’, ‘run’, ‘step up using right foot’, etc.), observe the time in seconds (or frames) the activity starts and finishes in the file. Note that we only count the \textbf{first completed} activity within the source file and we try to be slightly accommodating of the start time (for example, if the activity started between 3s and 4s in the file, we record it as having started at 3s to ensure we capture the complete activity with a bit of ‘slack’).
	\item For the subject in question and for the activity in question, record the start and end times in \textbf{frames} in the ‘start’ and ‘end’ columns for the activity (note that if the time was observed in seconds, simply multiply this by 60 as the suit samples at 60Hz), along with recording the name of the file that the activity occurred in (as this will be the same name as the corresponding ‘.mat’ file the suit data for this video will be in, just with a ‘.mat’ file extension instead of ‘.mov’).
\end{enumerate}

\quad In the image above, we can see a snapshot of several activities that have been recorded for some of the subjects, including the file names containing the activity in question for the subjects, along with the start and end times within the respective files. It should be noted that not every activity could be drawn from each of the subjects’ files. This could be due to the subject simply not performing the activity they were told to perform or were otherwise unable to perform the activity. In these cases, the start and end times will be marked with either a ‘-1’ or ‘0’, with a ‘0’ sometimes signifying an incomplete activity annotation done by the annotator.\\

\quad For our project, the main use of this document is as a tool for the ‘mat\_act\_div.py’ script. For each of the source files that the script wishes to divide up, it will look in the table for the name of the subject in question for that source file, find the row corresponding to that file and, for each activity, get the start and end times for that activity and extract the corresponding frames from the source file (making sure its name matches that of the activity’s ‘filename’ column entry). So while these single-act ‘.mat’ files will be further processed by other scripts (such as ‘comp\_stat\_vals.py’ and ‘ext\_raw\_measures.py’), ‘mat\_act\_div.py’ is the only file that directly relies on the Google annotations sheet, and no file within the system modifies it in any way.


%%%%%%%%%%%%%%%%%%
\section{NSAA Scores Reference Document (‘nsaa\_6mw\_info.xlsx’)}

\begin{center}
\includegraphics[scale=0.4]{project_figures/fig7_2}
\end{center}

\quad The next script in the system is the reference file which we use to create the $y$ labels used by every model that is built in the system. As a result of it being the only place that contains the $y$ label information for each of the subjects, it’s referenced by several scripts including ‘ft\_sel\_red.py’, ‘rnn.py’, and ‘model\_predictor.py’. The script itself was a collaboration of several other scripts found within the source data directories for ‘NSAA’ and ‘6minwalk-matfiles’, including ‘KineDMD data updates Feb 2019.xlsx’ and ‘nsaa\_matfiles.xlsx’. Some of these scripts contain information about certain subjects that others don’t; hence, rather than having the scripts checking each of the files in turn, it was felt that it would be easier to combine the information from all of them into one file.\\

\quad Each of the models that are built by the ‘rnn.py’ script are built to target one output type, which will be either ‘dhc’ (the D/HC label), ‘overall’ (the overall NSAA score), ‘acts’ (the 17 individual activity scores), or ‘indiv’ (the score of the individual activity assuming the input are single-act files) for each of the sequences the model is training and subsequently assessing on (see the ‘Data Forms and Types’ chapter for more information on each of these output types). For the ‘dhc’ output type, we don’t need to reference this file, as this can be determined simply by the file name. For example, in preprocessing the data into $x$ and $y$ components within ‘rnn.py’, if the given data that is currently being preprocessed for a given file is from a file called ‘D4\_position.csv’, then we know that the ‘dhc’ label would be ‘D’ (or 1 when being fed into the network), while if the data came from the ‘HC6\_position.csv’ file the label would be ‘HC’ (or 0).\\

\quad However, for the other types of $y$ labels that we use to train our ‘rnn.py’ to target the other output types, it’s not as simple as observing the name of the file. This is where the NSAA score reference file comes in. As we can see in the above image, it contains the information for every subject we have (that was collected by the initial assessors of the subjects) that includes their individual activity scores and their overall NSAA score. Hence, by finding the relevant row within the document, we get all the information we need for the other output types. For example, let’s say the preprocessing function of ‘rnn.py’ is extracting data from the file corresponding to subject ‘D16’.\\

\quad Once the data file has been separated into their sequences with necessary sequence overlap and an optional proportion of the sequence dropped (more on these later in the discussion of experiment sets), we then need to get the corresponding label for this sequence. If the model is being trained for the ‘overall’ output type, then we check the table for the value in the ‘D16’ subject’s ‘NSAA’ column, which corresponds to ‘23’. This is the $y$ label that each sequence extracted from the ‘D16’ source file would get if we are training for the ‘overall’ output type. Alternatively, if the model output type was instead ‘acts’, the $y$ label would be a list of values ‘[2, 2, 1, 2, 2, 1, 1, 1, 1, 2, 0, 2, 1, 2, 1, 1, 1]’ or, if the source file was a specific single act file for the subject, e.g. ‘D16\_position\_act4.csv’, then it would get the label ‘2’ (both of which are determined by the above table. This process of label extraction is identical across multiple scripts, be it for training models in ‘rnn.py’ or getting the true values of assessing subjects in ‘model\_predictor.py’, and so on.


%%%%%%%%%%%%%%%%%%
\section{Results for Experiment Sets (‘RNN Results.xlsx’)}

\begin{center}
\includegraphics[scale=0.35]{project_figures/fig7_3}
\end{center}
\begin{center}
(continued)
\end{center}
\begin{center}
\includegraphics[scale=0.35]{project_figures/fig7_4}
\end{center}

\quad With the two main reference files used by the project covered, we now move onto the first of the two documents containing the results of the various experiment sets and model predictions sets that we run. The first of those, ‘RNN Results.xlsx’, contains the information of building various models within the ‘Experiment Set’s section of the results discussion which we cover later on. These include testing different measurements with which to build models, examining different sequence lengths and sequence overlap proportions, and so on. What separates this document from the ‘model\_predictions.csv’ document that we shall discuss shortly is that ‘RNN Results.xlsx’ contains the results obtained from just analysing the testing data sets supplied to the ‘rnn.py’: this is generally 20\% of the source data set that is fed into the ‘rnn.py’ script. Hence, the results contained in each cell of the ‘Results’ column for this document covers the performance of various model setups on this testing data. In contrast, for most of the ‘model\_predictions.csv’ sets these are the results of feeding in complete files of subjects to be assessed on prebuilt models via the ‘model\_predictor.py’ script. Therefore, ‘RNN Results.xlsx’ gets its results from the direct console output of building models in ‘rnn.py’, while ‘model\_predictions.csv’ gets its results from running ‘model\_predictor.py’ on models already built in ‘rnn.py’ and assessing on complete files.

\quad Another difference from ‘model\_predictions.csv’ is that ‘RNN Results.xlsx’ has its information manually inserted into the table, as opposed to having it automatically done in ‘model\_predictions.csv’ via the ‘DataFrame.writecsv()’ method called in ‘model\_predictor.py’. The reason this is manually done is that, for each model that is created that we wish to write to a row of in ‘RNN Results.xlsx’, there is additional information that ‘rnn.py’ has no knowledge about and therefore cannot write to the table. This includes a short description of the model that has been created and what prior scripts were needed to have been run in order to preprocess the data that is required for this model (such as ‘comp\_stat\_vals.py’ or ‘ext\_raw\_measures.py’). Therefore, when we wish to write a row of data to ‘RNN Results.xlsx’, we manually fill in parts of the table ourselves and copy/paste in other parts of the console output (here, into the ‘Results’ column). Additionally, along with the description, necessary prior scripts with arguments to have been run, and the results of the model, we also output the general model configuration that includes the shape of the data and several of the more significant hyperparameters. The idea is that anyone examining our results in the results discussion section can refer back to this table fairly easily and see exactly how the models were built.

\quad The way these results from ‘RNN Results.xlsx’ are generally assessed is via the ‘graph\_creator.py’ script. This script reads directly from the file, grabs the requested rows, and plots one or more of the columns against each other depending on the arguments (for example, one configuration of ‘graph\_creator.py’ could plot the results of the MAE of the overall NSAA score against the sequence lengths of the corresponding rows, which we do so in experiment set 8). This is therefore the primary way with which we use the ‘RNN Results.xlsx’ file in this report, though it also serves the purpose of providing results which can be compared to via other users using the system.


%%%%%%%%%%%%%%%%%%
\section{Results for Model Predictions Sets (‘model\_predictions.csv’)}

\begin{center}
\includegraphics[scale=0.4]{project_figures/fig7_5}
\end{center}
\begin{center}
(continued)
\end{center}
\begin{center}
\includegraphics[scale=0.4]{project_figures/fig7_6}
\end{center}

\quad The final document that we make heavy use of in the system is the ‘model\_predictions.csv’ document. This is where any assessments made by the ‘model\_predictor.py’ script is stored (and by extension ‘test\_altdirs.py’ which calls ‘model\_predictor.py’ numerous times) and as such corresponds to the model predictions sets that are discussed later in the ‘Experiments and Results Discussion’ chapter. As all the information that we wish to write to the file is known by the ‘model\_predictor.py’ at run time, this processed is automated via the script itself and does not require the user to enter information into the table manually. As the primary differences between the two documents have already been outlined, it just remains to outline the types of information that is recorded in the table along with how we make use of this when assessing various model setups and investigating performance of different variations of subject files in the model predictions sets.\\

\quad Each assessment made by ‘model\_predictor.py’ writes one row of information to this table. This contains the name of the subject we are assessing on, along with extra strings that signify different types of models that subject was assessed on or any other preprocessing steps that were taken for the subject file(s) (see the relevant model prediction set descriptions to see to what each of these additional strings correspond). The name of the directory that the subject file(s) are sourced from is also included, along with any other directories that act as alternative directories from which to source files (see model prediction set 1 for an example of this). We also include the different measurements extracted from this subject’s file(s) that were used to act as input to the necessary models (the exact models that are chosen therefore depend on the types of measurement that are extracted from the subject file in question).\\

\quad The rest of the columns contain the assessed results of the subject for all the output types we are testing the subject on (which is generally the ‘overall’, ‘acts’, and ‘dhc’ output types). For example, the ‘Predicted ‘D/HC Label’’ reflects the results of the assessing the subject on models built for the ‘dhc’ output type, while the ‘True ‘Overall NSAA Score’’ and ‘Predicted ‘Overall NSAA Score’’ reflects the results of assessing on models built for the ‘overall’ output type. Therefore, we use all the information from subject assessments on various types of models to create tables comparing different subjects and setups (as shown in many model predictions sets) or to be used via ‘graph\_creator.py’, which can access ‘model\_predictions.csv’ in a similar way to ‘RNN Results.xlsx’ in order to create graphs over various rows of the table, based on the arguments passed to ‘graph\_creator.py’.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Data Forms and Types\\}

%%%%%%%%%%%%%%%%%%
\section{Background}

\quad Before moving onto how the data is actually used to train and test an RNN model, it’s worth outlining the forms the data can take, what it actually represents, and how we treat these forms differently. This is necessary so that the RNN model itself is able to treat these data files in the same way (though giving noticeably different results depending on the nature of the data file); this avoids the problem of having to create a unique RNN script for every different type of measurement (‘input type’) and model type we are targeting (‘output type’).

%%%%%%%%%%%%%%%%%%
\section{The Source '.mat' Files}

\quad All the data that we are concerned with is captured by subjects wearing the Xsens MVN inertial motion capture system (i.e. the body suit). Further information on how this suit works can be found in the “MVN User Manual.pdf” file in the base ‘project directory’. Each single file have is captured by one instance of a \textit{single} subject wearing the suit (i.e. one subject’s visit to the hospital). The files themselves are stored as ‘.mat’ files within a tree structure containing the data measurements themselves, among other metadata including segment, sensor, and joint names, the time and date the data was captured, and other relevant metadata. To access this information we are concerned with (i.e. non-metadata), we can first open the ‘.mat’ file in MATLAB before navigating to ‘tree.subject.frames.frame’. An example of what the data values within a ‘.mat’ file look like at this point can be seen below as the aforementioned ‘data values’ within the ‘.mat’ file:

\begin{center}
\includegraphics[scale=0.25]{project_figures/fig8_1}
\end{center}
\begin{center}
\includegraphics[scale=0.25]{project_figures/fig8_2}
\end{center}

\quad Note that each row is a single sample (what we call a ‘frame’) captured by the bodysuit and 60 of these rows correspond to the data captured over 1 second (as the sensors sample at 60 Hz). The columns, meanwhile, primarily consist of measurements that are captured by the suit’s 17 inbuild sensors. Note that there are different aspects that are measured by the suit depending on the measurement; for example, for ‘position’ the sensors measure the positions of 23 segments on the body suit in 3D space which, given that it’s measuring in 3 dimensions, correspond to 69 values for a given time instance (i.e. a single ‘row’ of data), while ‘jointAngle’ measures using the 22 joints of the suit, which gives 66 total values. The result is that, over a single time instance of 1/60th of a second, approximately 739 distinct values are captured by the suit. This is what we mean when we refer to an ‘all data’ file (or ‘AD’ file for short): it contains all the possible data captured by the suit for an instance of the subject wearing the suit.\\

\quad In contrast to this, we are also provided with ‘.mat’ files that correspond to the same subject’s instance of wearing the suit but that only contain the joint angle measurements. The idea behind this is that, as it’s believed that joint angles will be an important measurement for model training in several scenarios (classification of file type, regression of overall NSAA score, etc.), the ‘joint angle’ files (or ‘JA’ files for short) provide a simple jumping-off point to train some preliminary models (the results of which we shall see in the ‘Results’ section. The form of a JA file that corresponds to that of the AD file seen above is the following:

\begin{center}
\includegraphics[scale=0.25]{project_figures/fig8_3}
\end{center}

\quad As can be seen above, the table now looks very similar to how a corresponding ‘.csv’ file would also look, which lends itself to simplicity in reading the data into Python scripts and using it to train a model. Note that the dimensions are (21809, 66), with ‘21809’ corresponding to ~363 seconds worth of data over 66 dimensions (i.e. 3 dimensions of 22 joint angles).\\

\quad These joint angles files are also contained within what we call a ‘data cube’ (or ‘DC’ for short). This single ‘.mat’ file contains the joint angle data for 25 subjects and a table that contains information about them, as seen below:

\begin{center}
\includegraphics[scale=0.25]{project_figures/fig8_4}
\end{center}

\quad The image above shows how the data cube contains cells which each contains a whole joint angle file’s worth of data within them (that look similar to that seen above), while the table below shows the table contained in the data cube that contains useful information (metadata) about the respective joint angle files. Hence, due to the useful structure and provided table, when we look to train an RNN model on raw joint angle data, we use the data cube as standard for the first several experiment sets (though we later extract the joint angle data from ‘AD’ files as is done for other raw measurements in later experiment sets and model predictions sets).

\begin{center}
\includegraphics[scale=0.25]{project_figures/fig8_5}
\end{center}

\quad Finally, an important distinction to make is what sort of real-world activity each ‘.mat’ file actually shows. The first type is ‘6minwalk’, which as the name suggests is 6 minutes’ worth of data (so approximately 21600 frames) of the subject walking around a given area more-or-less continuously. This is provided to us as ‘AD’, ‘JA’ and ‘DC’ files, so for a given subject we can chose how we wish to interpret their data. The second type we are currently concerned with is ‘NSAA’, which are files usually between a length of 3 and 10 minutes that contain the subject carrying out the 17 activities that are set as part of the North Star Ambulatory Assessment. These files, however, are only provided to us as ‘AD’ files and do not come in ‘JA’ or ‘DC’ form (though we can still extract the raw joint angle measurements from an NSAA file via the ‘ext\_raw\_measurements.py’ script; more on that in the ‘Script Ecosystem Overview’ chapter). The final type is the natural movement behaviour data set provided to us in the ‘allmatfiles’ directory (which contains the joint angle data for the natural movement behaviour data of the subjects) and in the ‘NMB’ (which contains all the measurements from the natural movement behaviour files and which we received late in the project, hence why we used ‘allmatfiles’ for a long time). These directory contains numerous files from the same subjects as outlined previously performing various natural movement behaviours, such as sitting and talking, eating lunch, playing, and so on; while this isn’t used in most of the experiment sets to begin with, we do use this later in some of the model predictions sets.\\

\quad With the forms that the data can take now summarized, we can move onto what we actually do with this data prior to it being used to train an RNN.


%%%%%%%%%%%%%%%%%%
\section{The Data Pipeline}

\quad What is referred to as the ‘data pipeline’ is shorthand for four Python scripts (‘comp\_stat\_vals.py’, ‘mat\_act\_div.py’, ‘ext\_raw\_measures.py’, and ‘ft\_sel\_red.py’) that read from data files, manipulate data, and write to new files in the form of ‘intermediate data’. The aim of the pipeline is to convert the data that is specified by the user of the script (via arguments passed to the Python scripts) into a format that is ultimately usable by the RNN model. The specifics of what each script does are not covered here for the sake of brevity (see ‘Script Ecosystem Overview’ for this), so we instead focus on the different shapes and forms the data goes through depending on whether it came in as an ‘AD’ or ‘JA’/’DC’ file (as the ‘DC’ simply contains multiple ‘JA’ files, we treat both ‘DC’ and ‘JA’ files the same way). Refer back to the subsection ‘The Local Directory: Data Sets’ within the ‘Project and Local Directories’ chapter for information about the source and destination folders used for the variations outlined below.

%%%%%%%%%
\subsection{Variation 1: Joint angles from ‘JA’ and ‘DC’}

\quad Below, we can see the pipeline with respect to ‘JA’ or ‘DC’ file(s) as input to the RNN.

\begin{center}
\includegraphics[scale=0.2]{project_figures/fig8_6}
\end{center}

\quad This is what is done when we are dealing with raw joint angle data. Let’s say we wish to feed in the ‘JA’ file for subject ‘D4’, as seen in the diagram above. The data comes in the form of a 22K x 66 matrix, as can be seen in a previous image, which is loaded by the ‘comp\_stat\_vals.py’ Python script and simply written back out in exactly the same way to a ‘.csv’ format. In essence, we are removing some of the metadata from the ‘.mat’ file and then transforming the data values into ‘.csv’ format. The reason we do this is because the ‘RNN’ is expecting to load data from a ‘.csv’ format when dealing with all files regardless of its setup, so this essentially standardises the process so the RNN works in the same way for each different original data file type. An important aspect to note, though, is that this is also what is done with the ‘JA’ or ‘DC’ files: in the case of ‘AD’ files, to get the raw measurements we instead use ‘ext\_raw\_measures.py’.\\

\quad In the ‘JA’/’DC’ case, this data is then loaded by the RNN from ‘JA\_D4.csv’ and has sequences made out of it. In this context, we refer to a sequence as a 2-dimensional grouping of data that the RNN will treat as a single sample of data. In the above case, the sequence is a (60, 66) matrix of values: each row of 66 values are fed into the RNN one after another, repeating for a total of 60 times before the output of the RNN is observed; the RNN is essentially ‘reset’ before the next matrix of values is considered, hence the sequences are essentially independent of each other. In this case, the data is time dependent of each other only in 1 second intervals (given a 60 Hz sampling rate of the body suit), while outside of these sequences the data is treated independently of each other (in the same way as each image being classified by a convolutional neural network is independent of each other).

%%%%%%%%%
\subsection{Variation 2: Computed statistical values from ‘AD’ files}

\quad Next, we can see below the pipeline with respect to ‘AD’ files(s) as input to the RNN:

\begin{center}
\includegraphics[scale=0.2]{project_figures/fig8_7}
\end{center}

\quad The first point to notice is that the data starts with far more dimensions than ‘JA’ or ‘DC’ files. This is because the ‘AD’ files consider ALL the measurements and not just one measurement (the joint angles). With using them as input to ‘comp\_stat\_vals.py’, we process them differently; namely, we don’t just write them directly to a .csv, but rather calculate statistical values. These operations (which including finding the mean, variance, first/second eigenvalues of covariance matrices, fast Fourier transform values, mean sum absolute values, and so on) operate on only a limited number of rows of the data at a time. For example, in the above case, we select a given number of rows of the 22K frames at a time (in this case, 60 to correspond to 1 second’s worth of data), calculate statistical features over every single one of its 620 dimensions (along with between some of the 620 dimensions), and compute this as a row of approximately 4000 statistical values. This is then repeated for every other 60-frame part of the original 22K to produce a total of approximately 360 rows, each containing 4000 statistical values.\\

\quad Much like the sequence length that the RNN processes, the length of time to compute the statistical features over is a parameter that is set as an argument to the script (i.e. a hyperparameter that we set ourselves); hence, the size of ‘60’ is a value that has been determined to produce enough data while calculating over enough rows. A thing to bear in mind, though, is that if we increase this parameter so the statistical values are calculated over a longer time period, we will reduce the number of rows that are outputted from this script, hence reducing the amount of data available at the next stage of the pipeline (i.e. the data going into the feature reduction script outlined below).\\

\quad With this data having its statistical values computed and outputted to a new file (in this case of the above image, ‘AD\_D11\_stats\_values.csv’), we then load this .csv into a new Python script called ‘ft\_sel\_red()’. The purpose of this is to simply reduce the dimensionality of the data while preserving as much useful information as possible and to then rewrite this to a new file. While there are several options in feature reduction and feature selection (including principal component analysis, random forest feature selection, feature agglomeration, among others), we use PCA as standard based on its prevalence in other studies. Hence, when run on the input data and with a target reduction size set to 30 dimensions (again, this is a hyperparameter set as a script argument and is subject to experimentation in experiment set 7), the data is transformed from a dimensionality of (360, 4000) to (360, 30), which is far more conducive to being used as training data for the RNN in the next stage, as this reshaping helps minimize the effects of the ‘curse of dimensionality’.\\

\quad Finally, this is fed into the RNN and sliced up into sequences of length that we defined in the same way as in ‘JA’/’DC’ as previously described. A choice of ‘10’ is used here as the sequence length due to the limited size of the data (360, 30) in comparison to the case of reading from raw joint angle files (22000, 60) Also, it’s worth noting that even though the sequence length is only ‘10’ here, as each of the rows here have a full 60 rows worth of data from the original ‘AD’ file (which corresponds to 1 second’s worth of data embedded within them), each sequence from an ‘AD’ file to the RNN has 10 seconds worth of data encoded into it as statistical values. In comparison, the raw joint angle data has a sequence length of 60 and encodes only 1 second’s worth of data. This evidently plays a role in the difference in their respective results, which we shall discuss shortly.

%%%%%%%%%
\subsection{Variation 3: Extracted raw measurements from ‘AD’ files}

\quad Below we can see the pipeline with respect to using ‘AD’ files as a source of raw measurements:

\begin{center}
\includegraphics[scale=0.35]{project_figures/fig8_8}
\end{center}

\quad Unlike the previous two variations of the data pipeline, here we don’t make use of either ‘comp\_stat\_vals.py’ or ‘ft\_sel\_red.py’. This is because in this setup we neither compute statistical values, nor extract exclusively joint angles, nor do we need to do any dimensionality reduction. Rather, we wish to extract any raw measurements that we need from a given file. This is done by the ‘comp\_stat\_vals.py’ script: it takes the name of a file to extract the raw measurements from (or multiple names to carry out this process on) and the names of the raw measurements we wish to extract. From here, the script will seek out the columns of relevance within the tree structure of the corresponding source ‘.mat’ file for the given subject(s), as each vector of a certain raw measurement at each time instance is stored in its own column of the ‘tree.subject.frames.frame’ table which, when expanded, becomes a matrix of values for the raw measurement. This matrix of values (in the case of the ‘position’ measurement having a rough shape of (22000, 69)) is then written to a new ‘.csv’ file with a name reflecting the subject name the measurements are from along with the measurement name (e.g. ‘D4\_NSAA\_position.csv’).\\

\quad From here, the ‘rnn.py’ script is able to build models from these other measurement types in the same way it has done so for the joint angle data and the computed statistical values from the first two variations of the data pipeline by providing a different raw measurement name (e.g. ‘sensorMagneticField’ or ‘acceleration’) as opposed to just either ‘AD’ for computed statistical values (note here that ‘AD’ references to the computed statistical values; see the Glossary for further clarification about the two ways in which ‘AD’ is referenced) or ‘jointAngle’. After this, the following steps are identical, from the creating of sequences to the training and testing of models. It should also be noted that, like the ‘JA’ and ‘DC’ files, the extracted raw measurement values from ‘AD’ each represent 1/60th of a second’s worth of data; hence, when they are used to create sequences of 60 rows of raw measurement data, this represents 1 second’s worth of data in the same way it does for the ‘JA’/’DC’ files even though it comes from the ‘AD’ files: it’s only when computed statistical values are outputted that the data originating from ‘AD’ files that it comes to represent 1 second for each row rather than only 1/60th of a second.



%%%%%%%%%%%%%%%%%%
\section{Output Types}

\quad Now that the data has been prepared as a series of sequences of data either from a single ‘AD’, ‘JA’, or ‘DC’ file, or multiple of each, we now look at what the recurrent neural network models we have built actually do. It’s important to note that each of the variations works with every type of input file, be they from raw joint angle files or ‘AD’ files capturing either 6-minute walk or NSAA data; that is, after all, a primary purpose of using the pipeline. It’s also necessary to note that the trained model in every variation operates on a sequence-by-sequence basis even for testing; that is to say, it doesn’t classify or provide a regression score for a complete file but rather for a sequence of a pre-specified length within said file. The classifications or regression values of each of the sequences of the file being assessed can then be aggregated together to provide a classification or regression score for the whole file (how this is exactly carried out is subject to further research). Finally, it’s worth noting that each variant is implemented within one Python script called ‘rnn.py’, and the necessary architectural differences are setup based on arguments passed into the script.\\

\quad The three main variants (or ‘output types’) that we have developed are described as follows:

\begin{itemize}
	\item \textit{'dhc' - Classification of sequences of being from ‘DMD’ or ‘HC’ subjects}: The purpose of this variation is to classify sequences as being from a file that is from either a ‘DMD’ or ‘HC’ subject. Consider the previously outlined case where ‘FR\_AD\_D11\_stats\_values.csv’ is fed in from the pipeline to ‘rnn.py’ at the end. When it is loaded into the ‘RNN’ script, it’s divided into ‘36’ sequences of length ‘10’ to give a data shape of (36, 10, 30): this is the \textit{x} data in the sense of a neural network. For the \textit{y} data, we simply look at the title of the file this data originated from to provide a label of either ‘1’ or ‘0’ depending on the nature of the file name: since it’s a ‘D’ file due to it being about patient ‘D11’ it gets a label of 1. This is then repeated for each sequence to obtain a list of ‘1’s of length 36. We then repeat this process for all other files pushed through the data pipeline, some of which will be from ‘D’ subjects and others from ‘HC’ subjects. The result, in the case of doing this over all ‘AD’ files for files corresponding to NSAA subject assessments, is an input shape of (742, 10, 30) for ‘x’ and (742, ) for ‘y’, which contains a mixture of 1’s and 0’s for each sequence. There is also only a single neuron output for the network that contains categorical value of either 0 or 1 for a given sequence: this output will go to 0 if the sequence inputted is predicted to be from a ‘HC’ subject or 1 if predicted to be from a ‘D’ subject.
	\item \textit{'overall' - Overall NSAA regression score}: Here, the RNN is tasked with taking a sequence and trying to predict the overall NSAA score of the subject that it comes from. This score corresponds to the accumulation of the scores of the 17 individual activities done in the subject’s assessment. As each activity is scored either a 0, 1 or 2 (with 2 being a perfect score), the overall NSAA score will range from a 0 for a subject with severe DMD and a 34 for a subject that shows no symptoms (i.e. a ‘HC’ subject). Using the above example, we start with a shape of (742, 10, 30) over all the NSAA AD input files. For each of these 742 sequences, we then check a table that contains the subject information (‘nsaa\_6mw\_info.xlsx’, as described in the previous chapter): this table provides a list of the subjects, their testing details, and crucially their individual NSAA scores (17 of these between 0 and 2) and overall NSAA score (1 of these between 0 and 34). This overall score is then used for every sequence from a given file that is inputted to the ‘RNN’. Using this, we then obtain 742 values between 0 and 34, with each value being the overall NSAA score of the source file of its corresponding \textit{x} component of shape (10, 30). From here, we again have in the RNN architecture a single output neuron, but this time it outputs a regression value (rather than a classification value as in the previous case) between 0 and 34; hence, each sequence that passes through the RNN will result in it making an estimate of the overall NSAA score of the subject that the sequence originates from.
	\item \textit{'acts' - Classification of NSAA single activity scores for all 17 actions}: In a similar vein to predicting the overall NSAA scores, the RNN is also able to train towards predicting individual activity scores; that is to say, given a single sequence, the RNN will output an array of 17 values, each being either a 0, 1, or 2, that corresponds to its prediction of the individual activity scores of the subject that the sequence originates from. Again, given the same \textit{x} data fed through the pipeline, the corresponding \textit{y} values to train and test on are obtained from the ‘nsaa\_6mw\_info.xlsx’ table to obtain the necessary array of 17 values for each sequence. Hence the data fed into the RNN now has a shape (in the case of all NSAA AD files being used) of (742, 10, 30) for ‘x’ and (742, 17) for ‘y’. To account for this, the RNN is modified to predict 17 individual classification labels of either 0, 1, or 2 for 17 output neurons.
\end{itemize}

\quad Currently, all 3 variants have many of the same hyperparameters, including the train-test ratio of data (0.2; excluding the ‘--no\_testset’ optional argument being set which sets it to 0), the number of units in each LSTM cell (128), the number of hidden layers (2), and the learning ratio of the ‘adam’ optimisation algorithm (0.001). The reasoning behind this was that differences in experiment results will hopefully be down to differences in source data file types (e.g. raw joint angle files of 6-minute walks vs ‘AD’ files of 6-minute walks) rather than potentially different architectures (e.g. if one had more hidden layers, increased performance may be resulting from this rather than the source of the data going into the RNN, which is what we want to be able to compare). There are some differences though: a policy decided upon early was that the RNN should train until its loss more-or-less converges. This required different numbers of epochs dependent on the source data type. For example, if the data going into the RNN came from raw joint angles, it only needed approximately 20 training epochs to converge, whereas data from ‘AD’ computed statistical value files needed between 200-300 to converge. Finally, it’s also worth pointing out the main difference in training for classification (either for single or multiple output nodes) and regression was the different loss functions used, in that classification used binary cross entropy and regression used mean-squared error.\\

\quad It should also be noted that there is one more output type that is slightly unlike the others and that is the ‘indiv’ output type. This setups the models to only have one output neuron that computes a score of between 0 and 2, much like the ‘overall’ output type (though models trained for the ‘overall’ output type can range from between 0 and 34 for an output value). However, for the ‘indiv’ output type, we are only concerned with ‘single-act’ input file data, i.e. files that contain data of a certain raw measurement (or computed statistical values) for a single activity for a certain subject that are produced by the ‘mat\_act\_div.py’ script. Hence, the aim of this output type is to compute, for a given subject’s single-act file that has a corresponding true value associated with it (that is contained within the ‘nsaa\_6mw\_info.xlsx’ table), the predicted value between 0 and 2. Note that this is a regression task, and so the model can predict anywhere between 0 and 2, with the final prediction made by ‘model\_predictor.py’ for a given subject based on the average of these predictions for a sequence and then rounding this average to the closest integer to get the predicted value for that complete subject. We should also note that the reason we don’t consider this output type with the others is that it operates on a different file type: while the other three output types can be obtained for any input file, the 'indiv’ output type can only be done for single-act files. This makes intuitive sense, as it’s somewhat pointless to ask a model to predict a single act score corresponding to a specific activity for a sequence from a subject which could have come from anywhere within the source file (i.e. any one of the 17 activities or the ‘in-between’ activities data).


%%%%%%%%%%%%%%%%%%
\section{Additional Data Preprocessing Work and Tools Used}

\quad One of the disadvantages of the NSAA files provided is that all the activities for each subject are provided in the same ‘.mat’ file and the table that provides information of these subjects’ trials with the body suit (‘nsaa\_6mw\_info.xlsx’) contain information about the overall cumulative NSAA score as well as the individual activity scores (among other meta data), but not the specific times of occurrence of each activity. These activity times are needed in order to ‘divide up’ the original ‘AD’ files into ‘AD’ files that contain only the timeframe of a specific activity via the ‘mat\_act\_div.py’ script, which would be useful to have for this project as well as other research initiative projects. Hence, we were tasked as a group to work together to create a table of each subject’s predicted times for each activity. A portion of the results, contained in a shared Google sheet called ‘DMD Start/End Frames’, can be seen below:

\begin{center}
\includegraphics[scale=0.25]{project_figures/fig8_9}
\end{center}

\quad While we have touched on the above file within the ‘Google Annotations Sheet’ section of the ‘Reference Documents’ Chapter, it’s worth at this point clarifying a few things about how it was put together. The above image is only a portion of the table, and there are many more columns that aren’t displayed, but the above can be interpreted fairly simply. For example, for the ‘D11’ subject’s NSAA ‘AD’ file, the file is observed via the ‘dis\_3d\_pos.py’ script (elaborated upon in the ‘Script Ecosystem Overview’ chapter) to show the subject do the ‘standing’ activity between frames 300 and 1380 (corresponding to between 5s to 23s) and the ‘stand from chair’ activity between frames 3300 and 4920. There were several rules that were followed in order to extract these frame times by human observation:

\begin{itemize}
	\item As activities were often repeated by the subject, we consider only the first \underline{completed} activity as the start and end points in the sheet; for example, if the subject tried to do the activity ‘hop on right food’ several times without success before being able to do so, we only count the last successful attempt in the table.
	\item Ideally, we try to give a small amount of ‘leniency’ on the start/end times of the activity; the idea behind this is that no part of the activity is therefore ‘missed’ when it’s divided up into smaller 'AD' files.
	\item Some of the activities were either seemingly not performed or performed alongside another activity, or performed very subtly; for example, the ‘nod head activity’ was particularly tricky to detect in many subjects. In these cases, a best guess on the time of occurrence of the activity was made; the checking of how we divided up these files is further continuation work that can be done for the project and is covered later.
\end{itemize}

\quad So given that we have the ‘rules of thumb’ for our annotation work, it was next necessary to actually ‘visualize’ these ‘AD’ files. As there was no easy way of ‘running’ one of these ‘AD’ files in ‘.mat’ format and as we (at that point) had no access to the ‘.mov’ video files that corresponded to each source ‘.mat’ file, the project necessitated a script that could use the thousands of ‘position’ values of the AD file (‘position’ being one of the measurements in an ‘AD’ file along with ‘jointAngle’, among others) to feed into a function that animated a stick figure as it moved around three-dimensional space. This was the work of the ‘dis\_3d\_pos.py’ Python script and, when the script is run with the required arguments, e.g. for the ‘D11’ subject , the script does the following:

\begin{enumerate}
	\item Loads the position values from the ‘AD’ file for subject ‘D11’ into a massive array.
	\item Group each of the columns corresponding to \textit{x}, \textit{y}, and \textit{z} dimensions of all segments together.
	\item Send these to the ‘animate’ function that animated it at a rate of 60Hz (so the animation would appear as real-time) and closed upon completing the plotting of all values for that file.
\end{enumerate}

\quad The result of this was 3D data in a new window that looked like the following:

\begin{center}
\includegraphics[scale=0.4]{project_figures/fig8_10}
\end{center}

\quad Additionally, the current running time of the animation function was also printing to the console; this enabled us, by observing the movements of the stick figure and what activities it seemed to perform, to get a reasonably accurate idea of the times of occurrence of each activity.\\

\quad With the Google annotations sheet now being complete with the help of the above plotting script, we could now use the sheet as a reference tool for a script that could ‘divide up’ a given source ‘AD’ file; namely, the ‘mat\_act\_div.py’ script. The rough functionality of the script can be summarised in the diagram below:

\begin{center}
\includegraphics[scale=0.23]{project_figures/fig8_11}
\end{center}

\quad When the ‘mat\_act\_div.py’ script is then run with an argument given to be the name(s) of the file(s) to ‘divide up’ based on the annotated Google sheet, it does two things:

\begin{enumerate}
	\item Reads in the Google sheet that we had previously annotated, locates the relevant row given the provided arguments, and extracts a list of the activity times (both start and end times) as pairs of integers and the names of the files that contains the activities.
	\item Loads in the relevant ‘AD’ file for the given argument(s) and, using the pairs of integers in the above list, slices up the file into 17 non-overlapping parts and writes these to a subdirectory of the file(s)’s source data directory with names specific to the activity and subject source file (see the names in the bottom-right image above).
\end{enumerate}

\quad While these divided-up ‘AD’ files are not used to tune the models for the general cases of using the system, these are used instead in later model prediction sets to train RNN models to predict single-activity NSAA scores and evaluate the significance of particular activities with respect to predicting overall subject assessment.






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Script Ecosystem Overview}

%%%%%%%%%%%%%%%%%%
\section{Overview and Script Diagram}

\quad In this chapter, we shall be covering all of the scripts that are used as part of the system. By ‘system’ here, we mean the complete set of scripts and supporting documents that are used to carry out all the experiment sets and model predictions sets in an effort to realise the aims of the project, and that encompass the primary body of work done for the project. Each section covers one Python script in turn (with a small section covering all batch scripts at the end), and can each be divided into two parts. The first, ‘Overview’, describes why we felt it necessary to add the script to the system, what problem it’s supposed to solve, and it’s broad operations. The second, ‘How it works’, describes the script generally as either a sequence of steps or describes its different functions in more detail, depending on which is deemed more appropriate. In conjunction with extensive commenting found in each script, the hope is that by using this chapter, any user could understand in detail the purpose and behaviour of each script of the system.\\

\quad Below, we can see a diagram of all the scripts involved in the system for the project. It covers all the inputs that are needed for the project, how they are processed by the various scripts, and what types of outputs are produced by the system. Note that this is only a vague overview, without any details of how the scripts do work (this is discussed further below) nor details of the names and locations of the source files (which is covered extensively in ‘The Local Directory’ section of the ‘Project and Local Directories’ chapter), with the aim more to show the order things should be run in either by the user or by the batch scripts (not included below) and how the outputs relate to each other. Additionally, it is hoped that it will serve as a useful reference point to understanding the complete system and how each script fits into it.

\begin{center}
\includegraphics[scale=0.8]{project_figures/fig9_1}
\end{center}

%%%%%%%%%%%%%%%%%%
\section{'comp\_stat\_vals.py'}

%%%%%%%%%
\subsection{Overview}

\quad In an effort to experiment with both raw measurement values used in RNN models and also pre-computed features, we needed a separate script that takes in the body of raw measurements from source ‘.mat’ files and computes various statistical values taken over the entire file and the majority of useful measurements. This is the primary purpose of this script: to take in a source ‘.mat’ file of suit data for one or more subjects and, for each of them, produce an output ‘.csv’ file containing rows of statistical value data of the source ‘.mat’ file.\\

\quad Each of the statistical analyses that are used are implemented by a distinct function that performs a bit of syntactical help (e.g. the function to calculate mean includes type changing and rounding of numbers). The statistical features that are computed include: ‘mean’, ‘variance’, ‘mean absolute diff values’, ‘FFT’, ‘covariance components between axes’, ‘mean sum of values across axes’, among others. The bulk of these features have been extracted 'intra-columns'. This is meant by the following: consider a 'JA' (joint angle) file; it's columns correspond to only 1 'measurement', the 'feature' itself ('feature' in this context meaning one of the 17 sensor labels, 22 joint labels, or 23 segment labels, the choice of which depends on which measurement we are referring to), and the 'dimension' of this feature (as we are dealing with 3D position data, this is 3D with each representing the \textit{x}, \textit{y} or \textit{z} dimension). Many of the statistical features are thus computed on the values within each individual column; for example, the mean for a specific column is computed by averaging all the values for a specific measurement's specific feature's specific dimension (e.g. measurement 'joint angle's feature 'jRightWrist's dimension \textit{x}-dimension'), of which there are ~22k total values for this corresponding measurement/feature/dimension for the ~22k frames (or rows) in a file. However, there are also several statistical functions that are applied one-layer up; that is, rather than calculating over a single column representing a single dimension of a feature of a measurement, it calculates over 3 adjacent problems for ALL dimensions of a feature of a measurement. These mainly include operations that calculate features over a 2-dimensional array of data.\\

\quad The final variation of running statistical functions are those that operate on single columns are then reapplied to the calculate the same statistical function over all newly-calculated values. For example, if we are concerned with the variance values for the 'position' measurement over 23 feature names over the \textit{x}-dimension, then we take the variance of these calculated values to form a new value representing the 'position' measurement, the '(over all features)' feature, and the \textit{x}-dimension. This process is repeated with statistical functions that operate over the other 2 axis dimensions.\\

\quad The statistical features that are calculated per column (i.e. over a single axis) include:

\begin{itemize}
	\item Mean
	\item Variance
	\item Absolute mean sample difference
	\item Fast Fourier transform's (1-dimension) largest value
\end{itemize}

\quad The statistical features that are calculated per set of 3 columns (i.e. over all 3 axes of a feature for a given measurement) include:
\begin{itemize}
	\item Mean sum of the values of each dimension.
	\item Mean sum of the absolute values of each dimension.
	\item First eigenvalue of the covariance matrix of the 3 columns.
	\item Second eigenvalue of the covariance matrix of the 3 columns.
	\item \textit{x}- to \textit{y}-axis covariance (i.e. row 1 col 2 value of the 3x3 covariance matrix).
	\item \textit{x}- to \textit{z}-axis covariance (i.e. row 1 col 3 value of the 3x3 covariance matrix).
	\item \textit{y}- to \textit{z}-axis covariance (i.e. row 2 col 3 value of the 3x3 covariance matrix).
	\item Fast Fourier transform (2-dimension) largest 3 values (as 3 separate calculations).
	\item Proportion of samples outside the mean zone in every dimension.
\end{itemize}

\quad Each of these calculations done for a specific measurement, specific feature, and a single dimension are written as a single value as part of a row with the column title:
\begin{center}
\textit{($<$measurement name$>$) : ($<$feature name$>$) : ($<$axis$>$-axis) : (statistical function)}\\
\end{center}

...while, when it is subsequently called to repeat the process over all feature names, the column has the title:
\begin{center}
\textit{($<$measurement name$>$) : (over all features) : ($<$axis$>$-axis) : (statistical function)}\\
\end{center}

\quad For the calculations done for a specific measurement, specific feature, and over all 3 dimensions, they are again written as a single value as part of a row with the column title:
\begin{center}
\textit{($<$measurement name$>$) : ($<$feature name$>$) : ((\textit{x},\textit{y},\textit{z})-axis) : (statistical function)}\\
\end{center}

...while, when it is subsequently called to repeat the process over all feature names, the column has the title:
\begin{center}
\textit{($<$measurement name$>$) : (over all features) : ((\textit{x},\textit{y},\textit{z})-axis) : (statistical function)}\\
\end{center}

\quad The result is then a single row of all of these values for a whole file with \textit{n} columns in the row, with each column corresponding to an above label with associated value computed over the whole file. As we may have many measurements over which to calculate (e.g. 'position', 'velocity', 'angular acceleration', etc.), many features (e.g. 23, 22, or 17 depending on the measurement), 3 dimensions (or 1 dependent on which statistical function we are using), and ~15 statistical functions to compute, a single row for an 'AD' (all data) file can be several thousand columns long. Note that for a ‘JA’ file this is significantly less as we are only concerned with 1 measurement (the 'jointAngle' measurement as this is the only one in the file). Again, it’s important to note that these values are calculated across each of the samples (e.g. 22k) for each of the single columns or collection of 3 columns (depending on the statistical function in question).

%%%%%
\subsubsection{The split files functionality}

\quad An obvious problem from the method described above is that, while we might have plenty of statistical information computed over the single file, it’s only contained within a single row. To work around this, we added in the ‘--split\_size’ functionality. This essentially divides up the files into sections along time (i.e. a certain number of rows) before computing the statistical values over each of these sections rather than the whole file. For example, let’s say that we originally have 22000 rows of a data in a source ‘.mat’ file for a subject. If we provide ‘--split\_size=1’ to the ‘comp\_stat\_vals.py’ script, we interpret this as ‘compute the statistical values over 1 second increments’. This involves dividing up the file into sections of 60 rows (as 60 rows of data correspond to 1 second’s worth of data due to the sampling rate of the suit being 60Hz), followed by computing the statistical values over each of these blocks of rows, and finally vertically concatenating these lines to produce the output as a ‘.csv’.\\

\quad Thus, rather than having an output of shape (1, ~4000), we instead have an output of shape (366, ~4000); the downside of course is that each of these rows now contain computed statistical values calculated only over blocks of 60 rows, as opposed to the whole file. However, it was felt necessary to undertake this process in order to produce a somewhat comparable amount of data to be used alongside raw measurements. It should also be noted that a split size of ‘1’ isn’t set in stone, and is something we shall be experimenting with in later experimentation.

%%%%%%%%%
\subsection{How it works}

\quad The basic operation of the 'comp\_stat\_vals.py' script can be summarized as follows:

\begin{enumerate}
	\item Read in a certain .mat file (either a ‘JA’, ‘AD’, or ‘DC’ file) into Python as an object of either the ‘JointAngleFile’, ‘AllDataFile’, or ‘DataCubeFile’ class. Alternatively, if provided with the ‘all’ name in place of a file’s name, complete this process over all available files in the specified data set.
	\item Apply statistical analysis on each file’s various measurements, features of measurements, and dimension of the features; alternatively, if the ‘--split\_size’ functionality is set, divides the file into sections before computing the statistical values.  Note that this is optional if a joint angle is selected and called with the 'write\_direct\_csv' method, which just translates a joint angle .mat file to .csv format.
	\item Write the computed statistical value  to a .csv file with a name corresponding to the read in file; the aim with this is for it to be an easy-to-digest format for the next stage in the analytics pipeline (e.g. the ‘ft\_sel\_red.py’ script to reduce the dimensionality of the computed statistical value files).
\end{enumerate}



%%%%%%%%%%%%%%%%%%
\section{'ft\_sel\_red.py'}

%%%%%%%%%
\subsection{Overview}

\quad One of the consequences of using the 'comp\_stat\_vals.py' script is that the number of features produced as columns of data for a single subject's ‘AD’ file balloons several fold: for a single subject with ~620 columns (with each being one feature, of ~56,  of one measurement, of ~11) and ~22K rows (360s at 60Hz suit sampling rate), this then becomes ~360 rows (given '--split\_size'=1, i.e. 1 row for every 60 source rows) of approximately 4000 columns. Hence our data shape has been transformed from (22000, 620) to (360,4000) for a single file. This is completely impractical to use as training data for a given model for several reasons:

\begin{enumerate}
	\item The curse of dimensionality means that the models struggle to train at all when dimensionality is this large for the amount of data samples ('360') that we have available.
	\item Many of these computed statistical features may hold not that much useful information in them, or at least less useful information compared to other useful statistical features.
	\item Even if we were to use all these features, it would take a much longer time to train models for most likely very little gain (with it most likely being worse off than smaller dimensioned data), making it even worse from a practical standpoint.
\end{enumerate}

\quad Hence, for the '\_stat\_features.csv' files that are created by the 'comp\_stat\_vals.py' script, its more-or-less necessary to reduce the dimensionality to something a lot smaller prior to using this as training data. Note that this isn't done for raw measurement data for three reasons:

\begin{enumerate}
	\item The dimensionality of these data files is already at a level that is feasible for training (ranging from 51 from sensor measurements to 69 for segment measurements).
	\item There are far more rows of data within each of these files; this is due to the fact that, with using 'comp\_stat\_vals.py' with '--split\_size'=1, we computed stat values over each block of 60 rows and hence reduce the number of actual 'numbers of data' (i.e. numbers that appear in our data set) by 60-fold. This 60-fold comparable increase in data when using raw measurements makes using this data of column size 51-69 a lot more feasible in training models.
	\item Even though we may be computing many redundant features in 'comp\_stat\_vals.py', we are much less likely to have features that are as redundant as these in the raw measurements data. This is because every feature corresponds to a single dimension for a sensor, angle, or segment, which is much more likely to hold important information that many of the computed statistical values, and thus there is more of a motivation to keep all of these.
\end{enumerate}


%%%%%%%%%
\subsection{How it works}

\quad Given a user-specified 'dir' for the directory that we wish to source the stat feature files from, the file type we're interested in (usually set to 'AD'), the 'fn' of the file(s) of which we wish to reduce the dimensions of (set to 'all' to do so over all files in 'dir'), and 'choice' (which is the feature selection/reduction technique to use), the following is undertaken by the script:

\begin{enumerate}
	\item For a given file name in 'dir', read in the file (e.g. 'AD\_D4\_stat\_features.csv') as a DataFrame and divide it into its \textit{x} and \textit{y} components.
	\item Normalize each dimension of the data if the relevant optional argument is set.
	\item Set the number of features to extract from the data if the relevant optional argument is set. As standard, we use '30', as this generally encompasses a vast amount of the variance inherent to each data file while also being a feasible data width for our RNN models.
	\item Based on the 'choice' argument given by the user, use a technique to reduce the dimensionality of the data. This can be done in an unsupervised feature dimensionality reduction manner (e.g. using principal component analysis or Gaussian random projection), unsupervised feature selection manner (e.g. variance thresholding or feature agglomeration), or in a supervised feature selection manner (e.g. by using a random forest for feature selection). 'PCA' has been used up until this point, though further experimentation with other feature selection/reduction techniques is a promising direction to take the project in.
	\item With the newly-reduced data, call the 'add\_nsaa\_scores()' function to add the overall and single-act NSAA scores to each of the rows of reduced-dimensionality data, which is necessary for getting the relevant \textit{y} labels by the 'rnn.py' script, which the output of this script feeds into. The information for these scores comes from the 'nsaa\_6mw\_info.xlsx' file, which contains the scores for every subject that has undertaken the NSAA assessment; hence, all that is required is to select the row in this .xlsx file that corresponds to the subject we are currently dealing with.
	\item The newly-reduced data, with the NSAA scores appended at the beginning of each row, is then written to the same directory as it was sourced, with the exception that an "FR\_" ("feature reduced") prefix is appended to each newly-written file name to differentiate it from the file from which it was sourced.
	\item Repeat this process for every other file name in 'dir' that is required which, if 'fn'=’all’, results in all files in 'dir' having their dimensions reduced.
\end{enumerate}




%%%%%%%%%%%%%%%%%%
\section{'mat\_act\_div.py'}

%%%%%%%%%
\subsection{Overview}

\quad Along with using the full data files of the suit as computed statistical values used in various models and with varying target outputs (e.g. ‘dhc’, ‘overall’, etc.), we also wish to extract the single activities of source ‘.mat’ files from the NSAA directory. As standard, each source ‘.mat’ file in the NSAA directory contains the suit data of one full assessment for a single subject (though on occasion this is divided into two files if the ‘walk or ‘run’ activities happened at a later point). This means that each file usually contains the subject performing all 17 activities within the same file which are separated in time, sometimes by only a second or two in the case of the 'climb/descend box' activities and sometimes by up to a minute in the case of the 'get off the floor' activities. Hence, it would be advantageous for us to extract the data of the individual activities from within each file in order to use them for training for several different model predictions sets that are explored later on.\\

\quad As the data is contained within a very large table and each row is a single time instance of data (collected at 60Hz from the suit, therefore each frame is 1/60th of a second's worth of suit data), to create new single-activity files, all we need to do is the following:

\begin{enumerate}
	\item Determine the start and end rows within the overall file of the activity in question (e.g. if we 	wished to extract the second activity data that we know starts at 13s and ends at 15s in the subject's assessment, we would need to extract rows 780 to 900 of the source ‘.mat’ file).
	\item Slice the relevant rows from the table and create a new '.mat'-friendly tree structure within the script.
	\item Write this data to an 'act\_files' subdirectory of the source directory as a new ‘.mat’ file with a file name reflecting which activity it represents.
\end{enumerate}

\quad From here, we can process these single-activity ‘.mat’ files in the same way as the standard ‘.mat’ files through the data pipeline, including the extracting of raw measurements, computing of statistical values, and training of RNN models.


%%%%%%%%%
\subsection{How it works}

\quad The key requirement for this script to work is the use of the relevant Google annotations sheet contained within the ‘documentation’ directory. This contains the manually assessed activity times of each subject, which was done by several members of the research initiative that analysed each of the videos that corresponds to each subject's ‘.mat’ files and observed roughly at what times these activities started and ended for each subject. Note that these aren't going to be perfect, which is one flaw of using this sheet, as we can't give the exact start and end times of each activity and so tend to overestimate the amount of time the activity takes (i.e. note down a start time that's most likely before the real time and an end time that's most likely after the true end time) so as to ensure the complete capture of the activity. Also, this process is not immune to human error, and therefore it's not impossible to misinterpret what constitutes a 'complete' activity, which will impact how much use these 'single\_act' files are for us when we come to use them later.\\

\quad This Google annotations sheet can either be found within the ‘documentation’ directory. From here, once this is read in by the script, there are two functions that are executed:

\begin{itemize}
	\item \underline{\textit{'extract\_act\_times()'}}: As the name suggests, this function analyses the Google sheet and creates two lists: the first list, 'act\_times', is a list of start and end times (in suit frames, i.e. seconds x 60) in a nested structure (e.g. if there are 10 subjects, each performing the 17 activities, and each have a start and end time, then 'act\_times' has a shape (10, 17, 2)); the second list, 'ids', contain a list of subject names (e.g. 'D4'), each entry of which corresponds to an entry in 'act\_times'.
	\item \underline{\textit{'divide\_mat\_file()'}}: This function then takes the above two lists and, depending on what 'fn' argument the user has selected for the subject (‘all’ if one wishes to divide up all the subjects in the ‘NSAA’ directory), the relevant row within the 'act\_times' list is retrieved. From here, for each activity the subject has completed, each activity ‘pair’ (i.e. two numbers that are the start and end times in the table for each activity) is retrieved along with the name of the file that contains that activity for the subject (generally the same for all activities for a given subject, though there are exceptions). The complete ‘.mat’ file is then then loaded, the table of data within the ‘.mat’ file is extracted, and the table is sliced for that activity pair. These rows then 'replace' the rows of the 'whole' ‘.mat’ file and the ‘.mat’ file is then rewritten to a different file with a name reflecting the activity it is currently concerned with in the 'for' loop. This then repeats for each of the 17 activities for the given 'fn' subject(s).
\end{itemize}



%%%%%%%%%%%%%%%%%%
\section{'ext\_raw\_measures.py'}

%%%%%%%%%
\subsection{Overview}

\quad While the extraction of computed statistical values is an important tool for the data pipeline as an input to the ‘rnn.py’ script, it's also necessary to be able to use different types of raw measurement values; in other words, the values that are recorded by the sensors of the body suit and are within the corresponding source ‘.mat’ files. For a given subject's suit data, each measurement (e.g. 'position', 'jointAngle', 'sensorMagneticField', etc.) is inserted into the .mat file's table of values as a column, with the height of the column equal to the number of frames that were taken of the subject (corresponding to the length of time the suit was recording x 60 samples per second). Within this single column, there are vectors of either 51, 66, or 69 values (depending on whether the suit was recording raw sensor values, anglular values, or segment values, respectively).\\

\quad The idea of this script is fairly simple. For a given subject name in a directory (or all the subject names found in that directory) and for a given measurement (or all raw measurements available), the relevant source ‘.mat’ file is opened, and the relevant column is expanded for the given measurement name so that it becomes a matrix of single values rather than a column of vectors (with a matrix of shape (\# of frames, \# of vector values)). This matrix of data is then to be written to a separate ‘.csv’ file within a directory that reflects the source directory 'dir' and the measurement name that the matrix contains.\\

\quad From here, we can then use this data to train an RNN on these raw measurement values with \textit{y}-labels (i.e. target values) that are determined by the classification of file this ‘.csv’ of data corresponds to (i.e. a 'D' or 'HC' subject), or the overall or single-act NSAA scores that correspond to the subject name of this .csv (e.g. 'D4') that can be found with 'nsaa\_6mw\_info.xlsx'. In doing this, we provide an alternative to the production of RNN-ready data by 'comp\_stat\_vals.py' and ‘ft\_sel\_red.py’ and are able to compare how manually extracted features differ in RNN performance versus raw data (where the RNN does its own feature extraction). This is explored further within the discussion of results.

%%%%%%%%%
\subsection{How it works}

\quad The script runs in a fairly simple way without the necessity of classes or functions and thus just goes through a sequence of steps, which are as follows:

\begin{enumerate}
	\item Takes in arguments for the data set directory from which to retrieve the file(s) for raw measurement(s) extraction and checks them for validity (e.g. makes sure 'dir' is one of the allowed types such as ‘NMB’ or ‘NSAA’).
	\item Retrieves the full file name(s) of the files within 'dir' from which we shall extract the measurements from. If 'fn'='all', retrieves all full file names in 'dir' as a list.
	\item Parse the list of measurements that we wish to extract based on the 'measurements' argument that are comma-separated. If 'measurements'=‘all’, then return a list of all extractable measurements available as a list.
	\item Creates a directory for each raw measurement within 'dir' to store these raw measurements extracted.
	\item For each file in 'dir', load the ‘.mat’ file, extract the table of values within its tree structure, removes any 'wrappers' around these values within the table and, for each measurement to extract, select the column from the ‘.mat’ table that corresponds to the measurement, expand it out as 'measure\_data', and write it to a '.csv' file that reflects the file name and measurement we are currently concerned with.
\end{enumerate}



%%%%%%%%%%%%%%%%%%
\section{'rnn.py'}

%%%%%%%%%
\subsection{Overview}

\quad As the central element of the system insofar as it encompasses the learning and prediction models that are relied upon to produce the results, the importance of this script should be self-evident as it contains the class that defines the RNN's architecture (the ‘RNN’ class), how it trains, predicts, and the instantiation and running of said class. Hence, rather than going through the motivation of writing this script or going through the basics of RNNs and their operation (which has been covered previously in the ‘Overview of Recurrent Neural Networks’ chapter), we instead shall highlight a few important points about the structure of the script that builds these models:

\begin{itemize}
	\item We chose to use LSTM units instead of traditional neurons mainly due to their ability to learn better and the fact that they don't suffer the vanishing or exploding gradient problems.
	\item Other hyperparameters within the RNN itself (number of layers, size of LSTM units, learning rate, etc.) are kept as a constant throughout the experiments. These were found based on prior 'best practices' through prior research projects undertaken by others as well as rudimentary tuning to find 'good enough' parameters. However, further experimentation to find optimal settings could still be looked into; see the chapter on ‘Further Improvements’ for a discussion on this.
	\item The final layer can be either a single node for classification, a single node for regression, or 17 total nodes for single-act classifications; hence, the building of the RNN model depends on the arguments passed to the script.
	\item The performance of the models that are built here are generally viewed by two means: the console output at the end of the running of the 'rnn.py' script (which provides the info we need to fill in the 'RNN Results.xlsx', provided ‘--no\_testset’ is not set) or the 'model\_predictor.py' script (which provides info for 'model\_predictions.csv'). See the later section within this chapter for more information of how 'model\_predictor.py’ works.
\end{itemize}


%%%%%%%%%
\subsection{How it works}

\quad The structure of the script is fairly complicated and slightly convoluted, with numerous conditional statements needed to handle various data processing edge cases and many possible optional argument combinations that sometimes interact with each other in strange ways that must be handled; hence rather that explaining the structure of the script in detail, it's instead worth going through how exactly the script works upon being instantiated from the command line with arguments. This should give the user the a good grasp of what's going on upon script instantiation:

\begin{enumerate}
	\item Reads in all required arguments (e.g. source directory, file name(s), output type, etc.) and optional arguments (e.g. sequence length, sequence overlap, leave out file choice, etc.) and checks each for validity.
	\item Preprocesses the data from the source directory and file name(s) chosen; this includes reading in all source '.csv' files, fetching the relevant \textit{y} labels for the \textit{x} data from the files, splitting the data into sequences, discarding a proportion of the sequences if necessary, splits into train/test components, etc.
	\item Builds the ‘rnn’ object (instantiated from the 'RNN' class) with the necessary feature length, sequence length, size of LSTM units, number of hidden layers, and so on.
	\item Train the RNN on the 'x\_train' and 'y\_train' components and tests on the 'x\_test and 'y\_test' components.
	\item Prints out the performance on the test set to the console.
	\item Write to a ‘.csv’ unique to this model the results of the predictions, the arguments used to run the script, and the results that were printed to the console output. See ‘The Local Directory: ‘rnn.py’ Outputs’ section in the ‘Project and Local Directories’ chapter for more information.
\end{enumerate}

\quad It's also worth touching on a few of the optional arguments. The required arguments should be self-explanatory and in no further need of elaboration. Note that there are several other significant optional arguments that can be set that are not covered below (e.g. ‘--seq\_len’, ‘--seq\_overlap', and ‘--discard\_prop’), though the descriptions of what these do can be found where they are used within either the experiment set or model predictions set which specifically utilizes it. However, some of these optional arguments aren’t covered in any further details in any experiment sets or model predictions sets and so are covered below:

\begin{itemize}
	\item \underline{\textit{'--write\_settings'}}: This gives the user the option to store the results of the RNN that are printed to the output to the 'RNN Results.xlsx' file, rather than the user having to manually copy-paste console results to the file in a new row. This is generally used when new experiments sets with different RNNs are being carried out to save time and minimize the chances of human error; however, we generally don’t set this when a model predictions set is being carried out, as we wish for the outputs to instead be written to ‘model\_predictions.csv’ by ‘model\_predictor.py’, and thus it serves no purpose to write the results on the test set of ‘rnn.py’ to ‘RNN Results.xlsx’ when we don’t reference them.
	\item \underline{\textit{'--create\_graph'}}: This will create a graph of the true values against the predicted values; as these are done in the continuous numerical domain, this is only really useful for the overall NSAA score output type and is generally written to a new file within the 'Graphs' directory to be used in the results discussions.
	\item \underline{\textit{'--epochs'}}: A quick way to modify the number of epochs needed to train a model; this only varies based on the type of file being trained; for example, computed stat values (i.e. the ‘AD’ measurement) files generally need only about 20 epochs to converge, while we generally use >100 epochs for raw measurements. The epoch value therefore isn't kept as a constant like the other hyperparameters but rather fluctuates as necessary to help achieve model convergence.
	\item \underline{\textit{'--other\_dir'}}: This argument is set with the name of another source directory in order to also include files from another directory (or directories) in order to train and test the model; it simply loads in additional files into the preprocessing function. The motivation behind this is further explored in the results discussion of 'model\_predictions.csv'.
	\item \underline{\textit{'--leave\_out'}}: This is the standard way to leave out a specific subject short name (e.g. 'D4') when training the model. This is the workaround instead of removing a subject from the source directory so the model is not exposed to the subject in the training process. This is primarily used in conjunction with 'model\_predictor.py' to test on the left-out subject in question for those particular models in various model predictions sets. See 'model\_predictions.csv' and its section in the results discussions for more information on using this argument.
	\item \underline{\textit{'--balance'}}: This is the way that we can either upsample or downsample the data set loaded in by calling the relevant functions within 'data\_balancer.py'. The motivation for rebalancing the data set and how it works is covered extensively in the section for that script and thus is not worth repeating here.

\end{itemize}




%%%%%%%%%%%%%%%%%%
\section{'model\_predictor.py'}

%%%%%%%%%
\subsection{Overview}

\quad While gaining insights into various types of model parameters, source data types, data preprocessing options, and so on are an important and useful output of the project, one of the primary aims is to be able to assess complete files on models built by ‘rnn.py’; for example, we may wish to see how the model performs when tested with a subject it has never seen before and record the results in 'model\_predictions.csv'. Alternatively, we may want to be using models in their 'production' form to help inform specialists about subjects based solely on model results. To do this, we need a separate script that not only preprocesses a single subject's file(s) for testing, but also loads the relevant models from a specified source.\\

\quad The 'model\_predictor.py' script was written with this in mind. While it may work with predictions and the preprocessing of data, it's unlike the 'rnn.py' script in that it does not create any models; rather, it uses the models that have been created by 'rnn.py' already. Hence, the script is only useable after 'rnn.py' has created the required models. The arguments to 'model\_predictor.py' primarily serve three purposes: to load the data from the relevant source directories based on the file types (e.g. the ‘AD’ and ‘jointAngle’ measurements) in '.csv' format (created by either the 'comp\_stat\_vals.py' and 'ft\_sel\_red.py' scripts or the 'ext\_raw\_measures.py' script), to load the models that have been created that have been trained on the directory the file in question is sourced from and with the relevant file types and for all output types, and finally to assess the '.csv' data files on the models that have been loaded and aggregate the results to make assessments.


%%%%%%%%%
\subsection{How it works}

\quad The execution of 'model\_predictor.py' runs in a fairly procedural manner; hence, it's more intuitive to describe the program as a sequence of steps that call functions when necessary rather than a series of functions that are connected together as needed (e.g. 'comp\_stat\_vals.py'). The execution is as follows:

\begin{enumerate}
	\item Checks the validity of each passed in argument.
	\item For a given file name, loads in the '.csv' files for each of the file types provided; for example, if ‘fn’='D4' and ‘ft’='AD,jointAngle,sensorMagneticField', then the 'FR\_AD\_D4\_stat\_features.csv', 'D4\_jointAngle.csv' and 'D4\_sensorMagneticField.csv' files are loaded in (the names of which might slightly vary in practice due to naming conventions).
	\item Identify the directories that contain the models that we require to use for the files' assessment; note that these are all contained within the '$<$local directory$>$\textbackslash output\_files\textbackslash rnn\_models' directory (unless the ‘--final\_models’ optional argument is used which uses the ‘$<$project directory$>$\textbackslash source\textbackslash rnn\_models\_final’ directory instead), and have names that reflect how the models were built and on what data. This is done for all three output types as well. For example, if ‘dir’='NSAA' and ‘ft’='AD' are used, then 'NSAA\_AD\_all\_dhc\_--seq\_len=10\_--seq\_overlap=0.9\_--epochs=300', 'NSAA\_AD\_all\_acts\_--seq\_len=10\_--seq\_overlap=0.9\_--epochs=300' and 'NSAA\_position\_all\_dhc\_--seq\_len=600\_--seq\_overlap=0.9\_--discard\_prop=0.9' are loaded as the model directory names containing the models.
	\item Preprocesses the data from the '.csv' files so that they will fit into the pre-trained models (e.g. by having the expected batch size and sequence length) along with fetching the requisite \textit{y} labels for the data in the same way as is done for 'rnn.py'.
	\item For each output type and for each of the '.csv' files of the data for the subject in question, put all the data through the model that corresponds to the '.csv's file type (e.g. ‘jointAngle’ or ‘AD’) and its output type in prediction mode and have the predictions collected.
	\item For a given output type, average together all predictions made over every sequence prediction for every file type to get a prediction for that output type for the whole file. For example, for the NSAA overall score output type, we average the scores for every sequence from a given file input type's predictions, repeat this for the other input types, and finally average these scores to get a prediction of the overall score that takes into account all predictions made for every sequence of all the input types (i.e. measurements) we are assessing on.
	\item Outputs these scores to the user and appends these results to a new line within the 'model\_predictions.csv' file, along with the name of the subject in question as well as the file types used, the source directory, etc.
\end{enumerate}

\quad Special attention should be paid to some of the optional arguments. Some are used exclusively by other calling scripts (e.g. '--handle\_dash' and '--file\_num' are exclusively used by the 'test\_altdirs.py' script) and others are fairly simple and self-explanatory (e.g. '--show\_graph' shows the true and predicted overall NSAA scores made for the subject, while '--single\_act' is used when the input to the models are single-act files); however, there are a few others that each require a brief explanation:

\begin{itemize}
	\item \underline{\textit{'--alt\_dirs'}}: Provide this with a name of a directory that is not the same as 'dir' to test files on models that haven't been trained on the same directory; for example, if dir='allmatfiles' and 'alt\_dirs'='NSAA' then subject files will be loaded from the 'allmatfiles' directory but tested on models trained on files originating from the 'NSAA' directory. The motivation and results of this are explored in more depth in the results discussion and can be seen in MPS 1 and MPS 22.
	\item \underline{\textit{'--use\_seen'}}: For a given file name (e.g. matching or deriving from a subject short name like 'D2-009'), the default behaviour of the script is to seek out model directories where the subject has been completely left out of the training and testing process; in other words, the subject who we're assessing is completely new to the models assessing it. This is done by specifically seeking model directories with names containing '--leave\_out=<file name>' (along with the other required directory and file type arguments). This is extensively used in model predictions sets involving the assessing of models’ generalization performance to new subjects. Sometimes, we may not want to do this specifically: for example, when we want to compare a subject being tested on a model familiar with the subject to one that isn't; see MPS 6 for an example of this.
	\item \underline{\textit{'--use\_balanced'}}: In a similar way that '--use\_seen' seeks out model directories that haven't got something in their names, this optional argument specifically seeks model directories to use that have got '--balanced'='<up/down>' in the name (depending on the value given to '--use\_balanced') and therefore have been created with upsampled or downsampled data sets. Hence, this allows us to test complete files on models that have trained on an upsampled or downsampled data set. For more information on the data balancing process, consult the README for 'data\_balancer.py' or, for more info on how well this performed on complete files, see MPS 10.
\end{itemize}



%%%%%%%%%%%%%%%%%%
\section{'test\_altdirs.py'}

%%%%%%%%%
\subsection{Overview}

\quad A key motivation of this project is investigating how well, if at all, models that are built on one type of data can be adapted to be used to assess other types of data; for example, models trained on the NSAA data set assessing on files from the NMB data set. Furthermore, to get a good idea of how well this is done, it's necessary to test numerous files on pre-trained models. In the case of testing natural movement files on models that are trained on NSAA and 6-minute walk files, this would require running 'model\_predictor.py' manually over 400 times and each time with a different file name from within 'allmatfiles' or ‘NMB’. To get around this, 'test\_altdirs.py' was created to automate this process.\\

\quad Crucially, this script only allows 'model\_predictor.py' to work on assessing models' performances on unseen files that also have aren't trained on the same type of data. This allows us to see the strength of the correlation between different types of assessment for subjects wearing the suits and also whether or not predicting the assessment scores by models trained on one type of assessment can be used to infer assessment scores of data in a form that the models haven't been trained on. In other words attempting to answer the question: “Can we have subjects just do natural movement activities and then use the models that have been trained on NSAA and/or 6-minute walk assessments to determine their D/HC classification, NSAA overall scores, etc., just as well as if they had instead done the NSAA and 6-minute walk assessments instead?” The results of this are explored later in the relevant results discussions in MPS 1 and 22.


%%%%%%%%%
\subsection{How it works}

\quad The 'test\_altdirs.py' script is executed as a series of steps does the following when run:

\begin{enumerate}
	\item Reads in the name of a directory from which we wish to source the files that we wish to use for assessment, and also the names of the directories that will have been used to train certain models (for example, supplying 'NSAA\_6minwalk-matfiles' here will ensure that each time 'model\_predictor.py' is then called it retrieves the models that are trained on NSAA and 6-minute walk files).
	\item Retrieves a list of ‘.mat’ file names from within the source directory (i.e. if 'allmatfiles' was passed as the 'dir' argument then the names of all ‘.mat’ files from within 'allmatfiles' are retrieved and stored in a list).
	\item For every file name within this list of file names, create a unique string that corresponds to the input string to run the 'model\_predictor.py' script with the required arguments. This string includes the short file name of the file in question, the file types that the models will have been trained on (for example, if 'allmatfiles' was chosen as 'dir', then this must be 'jointAngle' as this is the only type of information that can be extracted from this type of data), the assessment file directory, and the source file directories that were used to train the models.
	\item From here, all functionality is passed on to 'model\_predictor.py' for the give file, which runs once for every file with the source directory as specified by the 'dir' argument. For further information on how this runs and what it produces, refer to section on ‘model\_predictor.py’.
\end{enumerate}



%%%%%%%%%%%%%%%%%%
\section{'graph\_creator.py'}

%%%%%%%%%
\subsection{Overview}

\quad There are several different ways that the outputs of experiments can be stored as 'results', along with appearing in several locations. For example, the results of different RNN setups and its tests on the test sets will appear in the 'RNN Results.xlsx' file. Additionally, for each model run (i.e. each row in 'RNN Results.xlsx'), there is a whole file of true and predicted values over the test set stored in a single '.csv' file with a name that corresponds to the predictions. Meanwhile, the results of whole file predictions (i.e. through the use of 'model\_predictor.py' and it's wrapper script 'test\_altdirs.py') are written as a row per file prediction into the 'model\_predictions.csv'.\\

\quad However, none of the scripts that write to these files do any sort of plotting or graphing of the data. This is for two reasons:

\begin{enumerate}
	\item Many times where we are running the scripts, we don't want to see the immediate plotting results or, rather, we can't. For example, when we run the 'model\_predictor.py' script once, it's only concerned with writing a single line to 'model\_predictions.csv', in the same way that 'rnn.py' only writes one line to 'RNN Results.xlsx', so for these to plot any results over several lines, the scripts would need additional user arguments to tell the script which lines it wishes to use for plotting, which adds to the already-high complexity of the scripts. Additionally, we often run 'rnn.py' via a batch script with many slight differences (to easily create several models to test on) and 'model\_predictor.py' via 'test\_altdirs.py', so stopping to produce a graph for every line that is written to an output file would be very inconvenient and would slow down the process.
	\item In separating the functionality, we keep a large degree of modularity amongst the scripts. In other words, the scripts that write the output to the output files ('model\_predictions.csv', 'RNN Results.xlsx, etc.) have nothing to do with the actual plotting of results in graphs. This helps in debugging (i.e. a problem in displaying the data will usually be isolated to 'graph\_creator.py') and also allows us to choose when we wish to do the plotting (i.e. after the data that we determine we need has been collected, not after a predetermined point in the running of each 'rnn.py' or 'model\_predictor.py' run). Furthermore, this sort of setup opens up the possibility for an easier collaborative effort: if others were to contribute to the output files (e.g. by adding experiment results done on other types of data that is still written to the output files in the same format), then it's possible to use 'graph\_creator.py' as a standalone script without the need to have previously run any of the other scripts.
\end{enumerate}

%%%%%%%%%
\subsection{How it works}

\quad The direction that 'graph\_creator.py' takes in terms of running entirely depends on the first argument as ‘choice’. Based on this, the script calls one of five functions that processes the other given arguments in a certain way. Note that, as each function operates on the arguments given differently, some of them are given generic names such as 'arg\_one' and 'arg\_two'. Also note that, as each function requires different numbers of arguments, every argument other than the first one ('choice') is optional; hence, when 'choice' is set to 'model\_preds\_single\_acts', it won't throw an error when we only give it values for 'arg\_one' and not the other three positional arguments.\\

\quad Rather than going over things sequentially, we instead go over below each of the functions that are called by their associative 'choice' argument value:

\begin{itemize}
	\item \underline{\textit{‘plot\_trues\_preds()’}}: This is a very simple function insofar as it just takes in the name of the '.csv' output that is produced by every run of 'rnn.py' that contains the test true and predicted values and are contained within the '$<$local directory$>$\textbackslash outputs\textbackslash RNN\_outputs' directory. Hence, the only argument needed is 'arg\_one' and this is to be the full name (not including directories and the file extension) of the file we wish to use. This is then read in, the predicted and true values are read in, and these are plotted against each other in 2 dimensions, with a $y=x$ line going through them to signify their 'ideal' positions.
	\item \underline{\textit{‘plot\_model\_preds\_altdirs()’}}: Reads in the 'model\_predictions.csv' as a DataFrame object; from here, we then wish to determine which rows in the DataFrame object that we wish to use. This is then based on rows that have their 'Source dir' column set to the value of 'arg\_one' and the 'Model trained dir(s)' column set to the value of 'arg\_two'. For example, if we wish to plot the rows in 'model\_predictions.csv' where a complete file from a specific source directory (e.g. 'allmatfiles') is then assessed on models trained on 'NSAA' and '6minwalk-matfiles' files, we set ‘arg\_one’='allmatfiles' and ‘arg\_two’='NSAA,6minwalk-matfiles'. This then selects the lines from the DataFrame object that we are concerned with. From here, with these lines we extract the true and predicted overall NSAA values from both the model trained on NSAA directory files and the model trained on ‘6minwalk-matfiles’ data set files. These values are then plotted with the true values along the $x$-axis and the predicted values along the $y$-axis and is done for both models. We also extract the 'percentage of correctly predicted D/HC label for sequences' for each file and model this percentage distribution as both cumulative and non-cumulative distributions for both source directories. We then repeat the same process but for the columns representing the percentage of individual acts correctly determined, and finally plot some useful statistical values computed over the lines.
	\item \underline{\textit{‘plot\_model\_preds\_trues\_preds()’}}: This is essentially the same as ‘plot\_trues\_preds()’ but operates on ‘model\_predictions.csv’ rather than ‘RNN Results.xlsx’. Hence, when we use this function and specify a start and end row with ‘arg\_one’ and ‘arg\_two’, we look for those rows within ‘model\_predictions.csv’, find the true and predicted overall NSAA score columns, and plot them alongside each other on the $x$- and $y$-axes. See ‘plot\_trues\_preds()’ above for further information about these graphs produced.
	\item \underline{\textit{‘plot\_model\_preds\_single\_acts()’}}: This is the third function to read in the 'model\_predictions.csv' file but, as we treat 'single-act' rows in the file differently than those that use alternative directories for assessment, it's easier to keep the functionalities separated. Hence, we first load in the file as a DataFrame object and select only the rows that have the value contained in 'arg\_one’; i.e. ‘act’ in the name of the short file: this signifies that a single-act file has been assessed on a model, rather than a full source file. From here, for each line we extract from the row's cells the percentage of acts correctly predicted, the percentage of correctly predicted D/HC label for sequences, and the difference between the true and predicted overall NSAA score. From these, we take one of the values for each of the single-act files and plot these values against the act number. This is then repeated 2 more times for the other 2 extracted values over each of the 17 single-act files. This then leads to 3 subplots where the $x$-axis is the act number (between 1 and 17) and the $y$-axis is one of percentage of acts correctly predicted, percentage of correctly predicted D/HC label for sequences, or the diff between true/predicted overall NSAA.
	\item \underline{\textit{‘plot\_rnn\_results()’}}: This is the function that analyses the 'RNN Results.xlsx' files and is responsible for the majority of graphs that show the performance of different RNN setups (e.g. sequence lengths, overlap proportions, number of features, types of raw measurements, etc.). The 'arg\_one' and 'arg\_two' arguments take the start and end experiment numbers of the file (once it has been loaded in as a DataFrame object) by looking at the 'Experiment Number' column to decide on which rows of the DataFrame object that we are concerned with. From here, for each row (which is associated with a model that has been created and tested upon) we extract the names of each measurement the model in question has used, the sequence length, and the results that it has produced. Then, based on the fourth provided argument ('xaxis\_choice'), we decide on what to plot along the $x$-axis: if it's set to 'seq\_length', then for each measurement (e.g. 'AD', 'jointAngle, etc.), we create a line and plot how well it performed at various sequence lengths with respect to different metrics (e.g. $R^2$, RMSE, etc.) based on the third provided argument 'out\_type'. If instead it's 'ft' (file type), 'seq\_over' (sequence overlap), or 'features' (number of features used), then a single line to plot is used instead over all the lines from DataFrame selected to plot the aspect of the data specified by the 'xaxis\_choice' against the metric specified by 'out\_type'. Numerous examples of these types of graphs can be seen in the sections of the report discussing specific experiment sets.
\end{itemize}


%%%%%%%%%%%%%%%%%%
\section{'data\_balancer.py'}

%%%%%%%%%
\subsection{Overview}

\quad One of the inherent problems with the dataset is the lack of 'variance' within the subjects for their overall scores. This is mainly a feature of how the NSAA assessments are conducted and the inherent variation of severity of Duchenne muscular dystrophy across the subjects. As the individual activity scores range from 0 (can't complete the activity at all) to 2 (completes it perfectly) and as there are 17 activities in total, the overall cumulative score ranges from 0 to 34. However, in reality, most patients in the study have scores ranging between 15 - 24 for moderate Duchenne. When it comes to training a network on the subjects' data and testing it on new files, this causes a problem if the subject has a particularly low overall NSAA score (e.g. 3). In other words, the lack of variation in the data we have available may slightly limit the potential of models generalisation ability to new subjects with particularly extreme cases of DMD. Thus, this is an important aspect to cover when we wish to improve generalization performance of the models to new subjects.\\

\quad A classic way in machine learning of helping to get around this is in using data balancing. This is traditionally done for classification problems rather than regression problems, as we are doing here. However, we get around this by, for the purposes of rebalancing the data set, considering overall NSAA scores as class labels rather than scores to be regressed on. There are two ways we consider here to balance our data, which are outlined below:\\

\quad Consider a dataset of 10 sequences of data (i.e. 2D structures of data of shape (sequence length, \# of features) with scores: [3, 15, 15, 15, 20, 20, 34, 34, 34, 34]. We have 2 ways of approaching this:

\begin{itemize}
	\item \underline{\textit{Downsampling}}: Counts the frequency of each number in the list and finds the lowest frequency; in the above case, it is 1 (as there is only 1 '3' in the list). Next, for each of the labels in the list above, we randomly select '1' sample of each label in the list and, more importantly, the label's corresponding $x$ value (i.e. a single sequence). Thus, we are reduced to a list of 4 sequences and with a label list ($y$ labels) of [3, 15, 20, 34] (note that there is only 1 of each sample because there was originally 1 '3' label). Hence, we now have a much smaller list, but an even spread of $y$ values for the samples we have remaining.
	\item \underline{\textit{Upsampling}}: We start off the same, with finding the frequency of each number in the list, but this time considering the highest frequency in the list. In the above case, this would be '4', as there are 4 ‘34's in the list. Next, for each label value in the list, we randomly sample a $y$ and corresponding $x$ value (being a sequence) a total of '4' times for each label. For example, for the '15' labels (i.e. 3 sequences and 3 '15' labels), we randomly pick a pair of $x$ and $y$ values from the 3 available and do this a total of 4 times. Thus, we end up with a much larger list of [3, 3, 3, 3, 15, 15, 15, 15, 20, 20, 20, 20, 34, 34, 34, 34] of $y$ values with corresponding $x$ values (sequences).
\end{itemize}

\quad Upsampling has the advantage of it being less likely to discard any of the data that has been given to us; however, it also means that many samples are repeatedly used as 'new' samples, which may lead to unpredictable training results, along with an inflated data set may being more challenging to train on. Downsampling, meanwhile, might give better generalization results than non-resampled data while being a smaller data set (thus making it quicker to train models that achieve better results), but the discarding of many data points might leave out important insights from the data out of the training process.

%%%%%%%%%
\subsection{How it works}

\quad The script contains 3 functions: 'ext\_label\_dist()', 'downsample()', and 'upsample()'. The last two functions are more-or-less identical to their respective algorithms that are outlined above, with a few implementation details differing but the overall ideas being the sample; hence, we won't repeat the more-or-less same algorithms here. Instead, it's worth considering how each of the functions are used. The script is never run directly, but rather serves simply as a storage place for several functions that are fetched by 'rnn.py'; hence, it's instead useful to consider exclusively how 'rnn.py' calls the functions. Also note that these are only run by the 'rnn.py' script if the '--balance' optional argument is provided.\\

\quad It's also worth noting the distinction between 'y\_data' and 'y\_data\_balance' when used as parameters for 'downsample' and 'upsample': 'y\_data' might be, depending on the output type that we are training towards (e.g. D/HC classification, overall NSAA score, or single act scores) a list of 1's and 0's, a list of values between 0 and 34, or a list of lists of 17 values between 0 and 2. Hence, we want a unified way of rebalancing the data that is irrespective of the form that 'y\_data' takes. Hence, 'y\_data\_balance' will always be the overall NSAA scores for the corresponding 'x\_data'; if, for 'rnn.py', the 'choice' argument is 'overall', then this will be exactly the same as 'y\_data', but for others it will contain the overall NSAA scores that are corresponding to the $x$ and $y$ samples. The ‘y\_data\_balance' is then used in the algorithms outlined above to find the indices of 'x\_data' and 'y\_data' to select to create the new lists of data.\\

\quad The functions of 'data\_balancer.py' and how they are used by 'rnn.py' are as follows:

\begin{itemize}
	\item \underline{\textit{'ext\_label\_dist()'}}: For each file that the 'rnn.py' model is training on, reads in the 'nsaa\_6mw\_info.xlsx' file, finds the relevant row in the table corresponding to the file name in question, and returns the overall NSAA score for this file name. This is then used as the label for each of the sequences that are extracted from the file in question, and the process is then repeated for every other file in the source directory, 'dir'.
	\item \underline{\textit{'downsample()'}}: If the '--balance' argument is set as 'down', then this function is called that takes in the 'x\_data' and 'y\_data' created from sequences (as 'rnn.py' would normally create) and the additional 'y\_data\_balance' that we have created additionally to use to balance the script, and from these downsamples the data and produces two new lists of 'new\_x\_data' and 'new\_y\_data' via the algorithm outlined above.
	\item \underline{\textit{'upsample()'}}: Called in the same way as 'downsample()' but via '--balance=up', while taking in the same arguments but instead using the algorithm for upsampling as described above.
\end{itemize}

\quad With the 'ext\_label\_dist()' and either 'downsample()' or 'upsample()' having been run the requisite number of times ('ext\_label\_dist()' once for every file in the source directory, 'dir', and only once for either of the other two), this data then replaces the original 'x\_data' and 'y\_data' in 'rnn.py', prints the new balanced shapes to the user, adds several output strings to be printed at the end of the script's running to show the before- and after-data-balancing for the distribution of labels, and the execution of 'rnn.py' subsequently continues as usual.



%%%%%%%%%%%%%%%%%%
\section{'file\_renamer.py'}

%%%%%%%%%
\subsection{Overview}

\quad One of the primary problems with working with ‘.mat’ files as part of this project is the lack of standardization of file names as they were collected. We have primarily been dealing with 5 source directories containing ‘.mat’ files: 'NSAA' (containing NSAA assessments of subjects), '6minwalk-matfiles' and '6MW-matFiles' (containing the 6 minute walk assessments of subjects), and 'allmatfiles' and ‘NMB’ (containing the natural movement files of subjects wearing the suit either as solely joint angles or all raw measurements, respectively). Each directory had its own primary way of labelling files but, even within directories time, it wasn't necessarily consistent throughout the directory.\\

\quad This posed a not-insignificant problem in that some of basic characteristics of the file were determined by its file name (e.g. whether it was a 'D' or 'HC' file came from reading its file name, along with what subject the file was associated with). Until development of this script, the solution was having multiple ways of processing every file name within the various scripts that need them. However, there's several flaws in this approach:\\

\begin{enumerate}
	\item It was not particularly extensible to new files with new formats being added. If new files were added to one of the source file directories with a slightly different naming format, it would require going deep into several scripts in order to change how they extracted the subject name of each new file it’s associated with, it's D/HC label, etc. This process ends up just adding more 'if...else' clauses to many already-cluttered parts of the scripts.
	\item As a result of having to change numerous things in several scripts, the process was more prone to human error. For example, as a result of a small oversight and not correctly reading the 'D' part of a file name that corresponded to subjects with 'D' in their subject name (e.g. 'D5'), the script was incorrectly interpreting the D/HC label for many files as being 'HC' rather than 'D' like it should have been; hence, the model was trained incorrectly due to labelling sequences incorrectly. In comparison, if we would have used 'file\_renamer.py' from the beginning, we would have easily spotted any files that have been renamed incorrectly and correct them before other scripts had the chance to misinterpret their labels.
\end{enumerate}

%%%%%%%%%
\subsection{How it works}

\quad The basic operation of the 'file\_renamer.py' script can be summarized as follows:

\begin{enumerate}
	\item Reads in the name of a source directory of ‘.mat’ files of which we wish to standardize the names.
	\item Gets the names of all ‘.mat’ files within the directory and divides them into one of two categories: 'files\_kept' (i.e. the vast majority of files which we don't want to remove) and 'files\_to\_delete' (files which we want to remove from the directory). Note that this is only for certain files that have been previously determined to be too large, too small, or not 'relevant' files to either training or testing models; for example, files that contain 'AllTasks' in their name in the 'allmatfiles' source directory, as these contain the same information as the other files in the directory but concatenated together for a single subject, so there's no need to use these as well as the others.
	\item Based on the source directory name, apply a set of regular expression ('regex') rules to each file name that are in 'files\_kept'. These are unique to each directory, as there are some things that we need to check for in some directories but not in other. These regular expressions are a set of substitutions: they search the file name for a certain characteristic and, if it finds it, replaces it with another before using this new string as the basis for the next regex. These regexes include: replacing non-capitalized subject names to capitalized versions (e.g. changing 'd4-003.mat' to 'D4-003.mat'), replacing 'NSA' with 'NSAA when found in a file name, changing instances of '-6MW.mat' to '-6MinWalk.mat' (as the type of activities they contain is the same whether it was sourced from '6minwalk-matfiles' or 6MW-matFiles'), and so on.
	\item With this new list of file names that we are to change 'files\_kept' to, we first remove the files within the source directory based on the file names within 'files\_to\_delete' and then, for each name in 'files\_kept' and its corresponding name in 'new\_files\_names', replace the name of the file in the former with the name in the latter. The result is that all of the files within the specified source directory are automatically changed based on the standard we predefined.
\end{enumerate}

\quad However, it's important to note that this script is not intended to be run more than once, and only at the beginning. Hence, it should be executed before any of the other scripts like 'comp\_stat\_vals.py' or 'ext\_raw\_measures.py' are used. This is because these scripts use the names of the files they are sourced from to create new files with names based on their source names; hence, for 'file\_renamer.py' to be useful, they should be used prior to other files being created that are based on the files that 'file\_renamer.py' wishes to rename. Hence, 'file\_renamer.py' is only needed to be used once. For this reason, it's also included within 'setup.cmd' as part of the setup process and is applied before any of the other scripts for the above reason.


%%%%%%%%%%%%%%%%%%
\section{'settings.py'}

%%%%%%%%%
\subsection{Overview}

\quad The purpose of this file is to hold many of the variables that are used throughout the rest of the script. In particular, there are many variable names (such as 'source\_dir') that hold the same values throughout all of the scripts. These variables contain values that include directory sources paths, paths to certain files that scripts output information to, lists of sensor names that have been given to us via the 'MVN User Manual', and so on; the common factor is that they are all referenced as being the same values across several different scripts and are thus interpreted as system constants.\\

\quad In storing these values in a separate file, we achieve three things:

\begin{enumerate}
	\item It reduces the amount of overall 'clutter' within the scripts, especially when we need to reference large variables such as those holding large lists of strings, which makes the scripts themselves both easier to debug and easier to maintain.
	\item For variables that are supposed to remain static, it reduces the possibility of accidentally changing them to suit the script they are currently being referenced in. For example, we are less likely to accidentally change the name of one of the 'raw\_measurements' when they are only accessed in other scripts and not modified and, if one is changed in 'settings.py', then this change is reflected out to all other scripts in the same way (e.g. preventing two scripts from each having their own versions of 'raw\_measurements', which could cause conflict in manipulating output files).
	\item If they are required to change for whatever reason (e.g. if a new user has their 'local\_dir' in a different location to the default value, or if the batch size to be used across numerous scripts is modified to be something else), then it's much easier to do so in a single 'settings' script rather than tracking down and modifying each respective variable in each script.
\end{enumerate}

\quad To access these values, each of the scripts calls the necessary variables from ‘settings.py’ in the 'import' section of the script. The idea of scripts only importing the variables that it needs was that it enhances clarity (i.e. if 'from settings import *' was used, we wouldn't as easily be able to see that 'local\_dir' comes from 'settings' as if we had used 'from settings import local\_dir'). Additionally, it's also recommended that any user using this project and ‘setup.cmd’ for the first time should first examine the relevant path names (such as 'local\_dir', 'results\_path', etc.) to ensure that the source ‘.mat’ files are contained in the expected location, the scripts can access the necessary output .’xlsx’ and ‘.csv’ files, and so on.




%%%%%%%%%%%%%%%%%%
\section{'predictions\_selector.py'}

%%%%%%%%%
\subsection{Overview}

\quad With so many file predictions being made and stored in 'model\_predictions.csv' as part of model predictions sets, it became necessary to have a way to sort through them all and return the rows that we are most interested in. This is why this script has been built: to filter rows of the table (each corresponding to a complete file prediction made using 'model\_predictor.py' or by extension the 'test\_altdirs.py' script) based on several arguments (e.g. the subject names we're interested in, the directory the subject was trained on, or the alt directories that the models were trained on if they are 'altdirs' rows) and, based on whether '--best' or '--worst' is provided, return the best or worst 'm' rows according to output metric 'n', where these are provided as part of '--best'/'--worst' (e.g. '--best=30,overall').\\

\quad In essence, this functions similarly to how an SQL query would operate as 'SELECT $<$a$>$ FROM model\_predictions WHERE $<$condition$>$’. However, the desire was to do this in Python so the whole pipeline would only require one language for implementation (no accounting for libraries built on top of languages like C++, e.g. for TensorFlow). Furthermore, this is easily possible via extensive use of the 'pandas' library to load in 'model\_predictions.csv' as a DataFrame object, which is excellent for the filtering of rows based on cell values, ordering rows by lowest/highest values in a specified column, and so on to make manipulation of the table as easy as using an SQL query. Additionally, this also means that anyone else running this system only needs to setup a single language/IDE in order to execute all of the scripts.\\

\quad The idea from building this script is having an easy way to see some of the 'most relevant' rows of the table to the user. Presently, this just takes the form of console output, though easy modification to have these lines written to file is possible. This script is especially useful for when we have many files to 'sift' through in order to get an idea of which are the best or worst performing on a given metric. For example, one particular application could be using the script to look at all the natural movement behaviour files that have been assessed on models build on NSAA and 6-minute walk files (totalling ~400 files) and selecting the best 20 of these according to which predicts the overall NSAA score of that file closest to the true value for that file. This has the potential to help us identify the types of natural behaviour files (e.g. sitting and eating, playing, sitting and moving on the floor, etc.) perform the best according to the metric. Another application could be, for a given subject name from the NSAA directory and on models trained on the same directory but left out of the training set completely, which options make the subject be predicted closest to the correct score (e.g. if the models’ data are upsampled, downsampled, trained on single-act files, etc.). We can see this script being used in particular in MPS 23.

%%%%%%%%%
\subsection{How it works}

\quad The script itself is fairly simple with no functions to call or classes to instantiate; rather, it executes a series of 'groups' of instructions that carries out the above-outlined tasks based on the script arguments. These can be summarised as follows:

\begin{enumerate}
	\item Loads in the 'model\_predictions.csv' file as a DataFrame object.
	\item Filters the rows of the table based on the 'sfn' argument, which removes all rows where the subject name doesn't match the value of 'sfn'; alternatively, if 'sfn'=’all’, keep all rows at this point.
	\item Filters the rows of the table based on the 'sd' argument, which removes all rows whose source directory column is different from the value of 'sd'.
	\item If the 'mtd' is given (i.e. if we're concerned with 'altdir' rows), filters the rows of the table based on this arg, which removes all rows whose ‘altdir’ column is different from the argument value. Note that the this argument is given as comma-separated values, which corresponds to the list values of the column in question.
	\item Based on whether the optional '--best' or '--worst' arguments are given (or both), extracts the first part of the argument (s) as the number of ‘--best’/‘--worst' lines in the table and the second part as the short name of the metric to use to determine which are the ‘--best'/‘--worst' (i.e. by deciding which of the output columns of the table to use to order the rows).
	\item For each of the remaining rows of the tables (i.e. after having been filtered by steps 1-4), we now filter the columns of the table: the first four columns are kept (the subject name, source directory, model trained directories, and measurements tested), followed by one of the output columns (the column in question is selected by the second part(s) of the ‘--best'/‘--worst' argument). These values are additionally preprocessed: e.g. if 'overall' is selected, then the absolute value of the difference between the true and predicted values in their respective columns are selected, while if we're using the 'percentage of predicted correct sequences' metric, the relevant column for 'Percentage of predicted <D, HC> sequences' is used based on the true D/HC label for the row.
	\item Creates a list of column names to create a new table of the top $n$ results that include the aforementioned 4 beginning column names from 'model\_predictions.csv', followed by column names of the output metrics with the names of the directory that the models that outputted this metric were trained using.
	\item Finally, select the top or bottom (or both) $n$ number of lines based on the selected column metric, depending on which of '--best' or '--worst' has been selected and the number of lines to extract from each of them, having reversed them if needed for percentage metrics ('pacp' and 'ppcs'), before printing out the selected rows to the console as a DataFrame object.
\end{enumerate}




%%%%%%%%%%%%%%%%%%
\section{'dis\_3d\_pos.py'}

%%%%%%%%%
\subsection{Overview}

\quad One desire for the data that we have received as '.mat' files is to be able to plot the subject portrayed within the file as a real-time 3D plot. The aim of this is to hopefully allow us to do two things:

\begin{enumerate}
	\item Visualize the subject within the data as doing certain activities in order to provide a reference (along with the console 'Plotting time...' output) as to what activities are taking place at which time; this is particularly useful as it helped in creating the Google annotations sheet.
	\item In plotting this, it easily allow for anomalies within the data file to be detected; for example, if the subject suddenly 'jumps' position or the limbs appear extremely contorted, it might indicate corrupted data which might need to be 'cut out' of the file (or have the whole file discarded).
\end{enumerate}

\quad Though this functionality also exists within the 'comp\_stat\_vals.py' script, it was felt necessary to also provide the functionality as a separate script within the system; hence, a lot of the code that was required by the '--dis\_3d\_pos' optional argument within 'comp\_stat\_vals.py' is repeated for this script.

%%%%%%%%%
\subsection{How it works}

\quad This script involves a series of basic steps that the data goes through in order to display a dynamic, 3D plot to the user. Hence, we shall explain it here as these steps which include the following:

\begin{enumerate}
	\item Loads in a '.mat' file corresponding to the 'dir' and 'fn' arguments provided to the script. This is read in as a DataFrame object and is returned from 'preprocessing()' and passed to 'display\_3d\_positions()'.
	\item Extracts the values from the 'position' column and reads this in as a 'positions' matrix (of shape (\# of samples, 69)), separates the columns of this new matrix into tuples of $x$, $y$, and $z$ axes for each segment within positions for every sample, define connected segments via tuples of pairs of values, sets the boarders of the 3D plot (i.e. the $x$$‘y$/$z$ mins/maxes), plots the 3D figure from the first sample with connections between points defined by the tuples of pairs of values, and animates it by fetching a new sample to plot every '1/sampling\_rate' sections so the figure is animated in real-time while outputting to console the current time-stamp of the figure in seconds.
	\item After ~5 seconds where the data is sourced, extracted, reconfigured to work in 3D, and animated, a new window will appear. This is the 3D plot that runs in real time. Note that one should also see as a console output the time stamp in seconds of where the plot currently is at (i.e. how far through the positions matrix it is). There is no current way to pause, slow down, or speed up the plotting, though one can change the viewing perspective by left clicking and dragging with the cursor or zoom in and out by right clicking and dragging with the cursor.
\end{enumerate}



%%%%%%%%%%%%%%%%%%
\section{'file\_mover.py'}

%%%%%%%%%
\subsection{Overview}

\quad To enable the working of certain batch scripts, it became a necessity to build into the batch files the ability to relocate files that are located anywhere on a user's PC to the proper sub directory of the local directory in order to have the data pipeline run properly. For example, if there was a source '.mat' file for 'D9V2' subject as an NSAA file (i.e. the second NSAA assessment done for subject 'D9') located somewhere on a user's PC, we wish to be able to copy it over to the $‘<$local directory$>$\textbackslash NSAA\textbackslash matfiles’ subdirectory. However, while we are able to do this potentially in a batch file via the 'move' command, we also wish to be able to change the location of where to copy the file to depend on the type of file we are working with (e.g. if the file is an NMB file, it would be placed in a different location within the local directory than if it was an NSAA file); additionally, we also wish to make use of the 'local\_dir' variable stored in 'settings.py' so one wouldn't have to modify a variable within a batch file if the local directory location was changed.\\

\quad For the above reasons, it was evident that it was simply easier to implement the 'move file' functionality in its own separate Python script. The intention, however, is to only ever use this file as part of a batch file or the ‘assess\_nsaa\_nmb\_file.py’ script as the first step in placing a file in the correct location to be used within the data pipeline.

%%%%%%%%%
\subsection{How it works}

\quad As this is a short script with a singular purpose, it's worth outlining the simple steps, as the program runs in a procedural manner:

\begin{enumerate}
	\item Takes in as arguments the name of the directory within the local directory to place the file within based on the type of file (e.g. 'NMB', 'NSAA', '6minwalk-matfiles', etc.) and the complete or local path (relative to the ‘$<$project directory$>$\textbackslash source\textbackslash batch\_files’ directory) to the file we wish to move.
	\item Checks for argument validity for the 'dir' argument and, given it is one of the allowed options, adds the strings to the 'local\_dir' variable based on the 'dir' argument so that 'local\_dir' now points to the correct 'inner' directory to store the copied source '.mat' file in.
	\item Attempts to copy the file given as the argument to the program (as a complete or relative file path) to the new value of 'local\_dir', and throws an exception if it cannot locate the file by the path given.
\end{enumerate}



%%%%%%%%%%%%%%%%%%
\section{'assess\_nsaa\_nmb\_file.py'}

%%%%%%%%%
\subsection{Overview}

\quad As part of the finished deliverables for the project, we wanted to create a 'wrapper' Python script that was able to assess a single file (either an NSAA or NMB file) wherever it was located within a user's local system on the models that we have selected as our 'final' chosen models (i.e. those that are contained within '$<$project directory$>$\textbackslash source\textbackslash rnn\_models\_final'). The idea of this script is that it would act as the primary tool that someone would use who only wants to assess a single file on the models that we have built and chosen as the best possible models for the job.\\

\quad While we could have implemented this functionality as a batch script, it was felt that it would be easier implemented as a Python script. This made things like conditional calling of other scripts, argument parsing, and so on simpler to program than if we were using a batch script. It also allowed for dynamic user interaction through inputs to the script, which meant that the script could be written in a more user-friendly way. In other words, for this script we do away with taking in arguments and instead ask for user input at points throughout the script execution. The hope is that it makes it easier to use for any user and can simply run it with only the project directory obtained and a '.mat' file somewhere on their system of which they wish to assess.\\

\quad As the script is meant to not require the local directory, this posed a potential problem to calling the other scripts (such as 'comp\_stat\_vals.py') which require the files to be located within the local directory in order to operate them. To get around this, we make use of the 'file\_mover.py' functionality within ‘assess\_nsaa\_nmb\_file.py’ where, if it doesn't see a local directory where it's expecting (based on the 'local\_dir' variable value in 'settings.py'), as would be the case where the user doesn't have the local directory, it instead creates a local directory with the same name, with the corresponding inner directories, and places the file from wherever the user specified into here. This then allows the subsequent scripts to operate upon this file as normal.

%%%%%%%%%
\subsection{How it works}

\quad The primary operation of the scripts is to take in user input and, from the various inputs, create strings that are passed in turn to the 'os.system()' function to call each script in turn with the correct arguments. Again, as this is a fairly simple script in its execution with no function calls or object creations, we can summarize the script as a series of several steps:

\begin{enumerate}
	\item Gets from the user the user-specified path to the '.mat' file which the script shall be assessing (can be either an absolute path or a path relative to the '$<$project directory$>$\textbackslash source' directory.
	\item Gets from the user whether the file is of an NSAA assessment or a natural movement behaviour file.
	\item Executes 'file\_mover.py' to move the file to the required subdirectory of the local directory (based on the NSAA\textbackslash NMB choice specified) or, if the directory doesn't exist, creates the required directory and subdirectories and then copies the file to the required subdirectory.
	\item Executes 'file\_renamer.py' to rename the now-copied file if required.
	\item Gets from the user the comma-separated measurements (raw or computed statistical values) to use to assess the file.
	\item Executes 'ext\_raw\_measures.py' if required to extract the raw measurements from the file.
	\item Executes 'comp\_stat\_vals.py' and 'ft\_sel\_red.py' if required to extract the computed statistical values and reduce the dimensionality of the file’s computed statistical values.
	\item Gets from the user whether or not they wish to use models built on 'alt\_dirs' and/or built solely on non-'V2' files.
	\item Based on the inputs given by the user regarding 'alt\_dirs' and 'V2' files, execute 'model\_predictor.py' to assess the file's measurement data on the appropriate models, display the results to console, and write the results to 'model\_predictions\_newfiles.csv'.
\end{enumerate}




%%%%%%%%%%%%%%%%%%
\section{Additional batch scripts}

\quad Along with the Python scripts that make up the system pipeline, we also make use of several batch scripts for automating some of the tasks and for setup. As these aren't particularly long or complicated, it isn't worth creating a separate section for each, but rather a single section covering all of them along with when we would use them:

\begin{itemize}
	\item \underline{\textit{'setup.cmd'}}: This script runs the necessary 'pip' package installation commands to setup all the external libraries needed for running the project. Specific versions of the packages are used to match the exact versions used as part of this project to avoid potential complications, although setting up the most recent versions of the packages would most likely work just as well. We also run the necessary system scripts on all setup source directories. This requires that the user has setup the source directories ('NSAA', '6minwalk-matfiles', etc.) in a base directory that matches the name of the 'local\_dir' global variable stored in 'settings.py'. Assuming that, the rest of 'setup' will extract the statistical values from each file in every directory, along with reducing the features of these, standardizing the names of the files, extracting all raw measurements from every 'AD' file, and dividing up files to extract single activities from 'AD' files.
	\item \underline{\textit{'models\_no\_leftout.cmd'}}: At the point where we have found the optional model parameters to assess left-out subjects, we then wished to build new models but with no subjects left-out of training. Hence, this script is essentially an extension of MPS 20 where, instead of building models with left-out subjects as done in MPS 20, we build them with all subjects included. These models were then copied over to ‘rnn\_models\_final’ within ‘$<$project directory$>$\textbackslash source’ so they could be used by the ‘assess\_nsaa\_nmb.py’ script.
\end{itemize}

%%%%%%%%%
\subsection{Model prediction set scripts}

\quad In an effort to make the execution of the model predictions sets easier (which often require numerous new models to be created with 'rnn.py' and many separate file predictions to be made with 'model\_predictor.py'), we have created batch scripts to automate this process. This also holds the additional benefit where any user can inspect what arguments we have run each script with and also enables them to run them for themselves to see if comparable results can be obtained (obviously requiring the setup of all other files via 'comp\_stat\_vals.py' and other necessary scripts via ‘setup.cmd’ beforehand).\\

\quad The idea is that, for each model predictions set that we are running, all that is needed is therefore to just run the specified '.cmd' script. This will build the requisite models, though sometimes it won't build any new models but will instead rely on models built by previous '.cmd' scripts; hence, it's recommended that each model prediction set batch file be run in numerical ascending order. Once a given model predictions set’s batch file has been run, with the necessary models built and file predictions made, the results will appear in 'model\_predictions.csv' as the final rows in the table. It's also worth noting the time discrepancies between some of the '.cmd' files: some will only be calling 'model\_predictor.py' multiple times, which is comparatively quite quick to execute. However, those that call 'rnn.py' many times will take a lot longer; for example, 'model\_predictions\_set\_3.cmd' needs to build 60 separate RNN models, each of which may take 10-15 minutes to run (assuming the user is building them using a GPU), which could take 10-15 hours in total to execute the script.\\

\quad Finally, the scripts don't take any arguments, as the Python script parameters have been decided in advance. For example, prior to executing the batch scripts for model predictions sets 3 and up, we decided to test the models on the left-out subjects D3, D9, D11, D17, and HC6 (see the experiments results discussion set for an overview as to why these subjects were chosen). Hence, any changes that would be made to these '.cmd' scripts must modify each instance of the Python script that is called by the batch script in question in order to correctly alter these chosen script parameters.


















%\begin{figure}[tb]
%\centering
%\includegraphics[width = 0.4\hsize]{./figures/imperial}
%\caption{Imperial College Logo. It's nice blue, and the font is quite stylish. But you can choose a different one if you don't like it.}
%\label{fig:logo}
%\end{figure}

%Figure~\ref{fig:logo} is an example of a figure. 



%% bibliography
\bibliographystyle{apa}


\end{document}
