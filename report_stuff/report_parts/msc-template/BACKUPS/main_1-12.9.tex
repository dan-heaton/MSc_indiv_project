\documentclass[12pt,twoside]{report}
\usepackage{graphicx}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Definitions for the title page
% Edit these to provide the correct information
% e.g. \newcommand{\reportauthor}{Timothy Kimber}

\newcommand{\reporttitle}{Recurrent Neural Networks Applied for Human Movement in Subjects with Duchenne Muscular Dystrophy}
\newcommand{\reportauthor}{Daniel Heaton}
\newcommand{\supervisor}{Aldo Faisal}
\newcommand{\degreetype}{MSc Computing (Machine Learning)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% load some definitions and default packages
\input{includes}

% load some macros
\input{notation}

\date{September 2019}

\begin{document}

% load title page
\input{titlepage}


% page numbering etc.
\pagenumbering{roman}
\clearpage{\pagestyle{empty}\cleardoublepage}
\setcounter{page}{1}
\pagestyle{fancy}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
Your abstract.
\end{abstract}

\clearpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section*{Acknowledgments}
%Comment this out if not needed.

%\clearpage{\pagestyle{empty}\cleardoublepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%--- table of contents
\fancyhead[RE,LO]{\sffamily {Table of Contents}}
\tableofcontents 


\clearpage{\pagestyle{empty}\cleardoublepage}
\pagenumbering{arabic}
\setcounter{page}{1}
\fancyhead[LE,RO]{\slshape \rightmark}
\fancyhead[LO,RE]{\slshape \leftmark}













%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\part{Background and Basis of Research}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Project Overview and Background\\}


%%%%%%%%%%%%%%%%%%
\section{Motivation for Project and Purpose of Work}

\quad Modern machine learning algorithms and methodologies has seen great success in regards to being applied to biomedical data in order to provide insights that assist specialists and help diagnose potential conditions that may afflict subjects. One example of this is DeepMind’s recent progress with AI-assisted eye scans to detect over 50 different types of diseases potentially in a subject’s eye as accurately as world-leading expert doctors [1]. Deep learning techniques have also been utilized by HeartFlow to help build 3D models of subjects’ hearts and assess the impacts of blockages on blood flow to the heart [2]. Based on these breakthroughs, among others, in recent years, it is very probable that AI-assistance will become the new norm in various clinics and hospitals around the world within the next decade [3]. Taking note of the prominent applicability of artificial intelligence to the analysis of human movement in particular, Imperial College London have undertaken a research initiative in collaboration with Great Ormond Street Hospital to investigate the applicability of various AI techniques to the analysis of subjects with Duchenne muscular dystrophy [4], a form of muscular dystrophy that predominantly affects young males under the age of 12 and severely impacts their movement ability to varying degrees. The hope is that great strides in treatments can be achieved by utilizing artificial intelligence to inform and make decisions on a subject-by-subject basis and potentially draw insights about the condition. \\

\quad With regards to this project specifically that is undertaken as part of this research initiative, we wish to investigate the applicability of recurrent neural networks (RNNs) to the features of human movement data provided by body suit measurements captured from subjects with Duchenne muscular dystrophy (DMD). This will hopefully provide not only evidence on the applicability of these models to this sort of data, but also should provide important insights into the data itself and of the subjects providing it. These insights from the project will hopefully have a direct positive impact in the ability to assess subjects via the North Star Ambulatory Assessment (NSAA) and possibly provide insights into other features of the condition. Generally, in this project we are trying to gain insights about the body suit data that is captured as ‘.mat’ files (MATLAB data files) through the different measurements that are captured by the 17 sensors of the suit (joint angles, position, accelerometer values, etc.) of NSAAs or 6-minute walks assessments of the subjects by means of sequence modelling using RNNs. This use of sequence modelling is necessary to model the dependencies through time of measurements, and we are more likely to have a robust model if measurement values are treated as NON-independent with respect to time.\\

\quad The overall goal of the project is therefore to provide a deliverable that includes a complete system that works with varying forms of suit data, learns from it, and provides insights about it, while hopefully being able to be adapted to new subjects, new NSAA assessments of existing subjects, and help inform specialists of the severity of their condition. A significant hope, therefore, is to provide a software solution that positively and directly impacts the lives of those with DMD through hopefully making their assessments easier and more accurate.

%%%%%%%%%%%%%%%%%%
\section{Aims and Objectives}

\quad With the overall aim of the project outlined and with the motivation for undertaking the work provided above, we now turn our attention to covering some of the main aims of the project. These include:\\

\begin{itemize}
	\item Building a reasonably good model (with surrounding supporting scripts that are outlined later on) that, when presented with new, unseen ‘.mat’ files of body suit data, can give a reasonably good approximation of individual NSAA activity scores and an overall NSAA score (i.e. the accumulation of all individual activity scores). A prominent limitation currently, however, is the overall lack of data files: we have no more than 50 complete ‘.mat’ files in total for each of the  6-minute walk and NSAA assessments. This is primarily due to the fact that the data collection is currently an ongoing process and a large repository of previously-collected suit data from other subjects with DMD does not appear to exist that’s publicly available. Hence, an implicit requirement of the project is to be able to make the most out of the data we have available, such as using it to train a model to predict different things, use different measurements contained within the ‘.mat’ files, look at applying statistical analysis on the raw data, and so on.
	\item Being able to use trained model(s) to gain insights into the most influential activities and measurements from the ‘.mat’ files on overall NSAA score and to identify activities that correlate highly with overall assessment. In doing so, it could possibly enable the reduction of 17 activities needed for accurate overall NSAA assessment to far fewer if only a few are needed to correctly assess the subject. The conclusions that we could possibly draw from the project, therefore, hopefully have the potential to aid specialists in the practical undertaking of the assessments through minimizing the amount of testing the subjects have to do.
	\item Investigating the impact of training models on different types of source data directly. For example, we’d like to see whether or not it’s possible to train models on natural movement behaviour data sets to the same standard as if we were using NSAA data sets when training towards overall and individual NSAA scores. If this were to be the case, then there exists a real possibility of not requiring the NSAA assessments to be completed by subjects at all, and instead simply requiring the subject to undertake natural movement instead, which may be significantly easier and/or more practical for subjects.
	\item Building models that are trained only on one ‘version’ of assessments of subjects and attempt to generalise to subsequent versions. Here, by ‘version’ we mean an assessment of a subject that takes place at a certain time, with subsequent versions being the same assessment but taken 6-months later on. For example, a subject’s initial NSAA assessment would be stored as the subject name ‘D4’, and when that subject returns 6-months later to undertake their subsequent NSAA assessment the resultant data file would be stored as ‘D4V2’. The hope is therefore to train models on non-‘V2’ files and generalise to newly presented ‘V2’ files. This should provide an advisory tool for any specialists wishing to assess how a subject’s conditioned has progressed during the time between assessments.
	\item Looking into how possible it is to build models that generalise well to new subjects and the system settings needed to achieve this; by this, we mean models that are able to assess subjects that they have never come across before during training (which differs to the previous bullet point, which looks at new data from existing subjects). Therefore, if this were to be the case then we would be able to extend the applicability of this system to not only new assessments of the existing subjects but brand new subjects to the overall research initiative. There are numerous techniques that we would have to look into if the models have a problem generalizing, and so a large amount of the model predictions sets will focus on this aim.
	\item Package all the scripts and models necessary for a specialist or any other researcher wishing to use any of the built tools in a way that is easy to use and gives intuitive output. This requires us to construct the system in a way where it is possible to be uses by others outside of the development environment in order to be practically applicable to achieve the aims outlined above.\\
\end{itemize}

\quad With our overall project aims outlined above, it’s also useful to cover some of the objectives that we intend to achieve in order to complete these aims, many of which will be investigated within their own experiment set or model predictions set. These include:

\begin{itemize}
	\item Comparing models built from different measurements (e.g. joint angles, acceleration values, computed statistical values, etc.) on their performance of evaluating unseen sequences of data to an accurate D/HC classification and overall/single-act regression of NSAA scores.
	\item Evaluating the ideal values for different sequence setups for the data going into the model with respect to the performance for various output types. This includes finding the ideal sequence length, sequence overlap, and discard proportion of frames within the sequences.
	\item Investigating the ideal number of features needed for the raw measurements and computed statistical values to train a model. This will involve a trade-off, with more features providing more of the inherent variance within the data and fewer features making it easier for the model to learn from.
	\item Looking into how well models performed when evaluated on files from a different source directory than their own. For example, we’d like to investigate the potential to models built from NSAA files and assess subject files from the natural movement behaviour data set. If this is possible, then with the finished models we could use these to assess a subject based solely on their natural movement, which might be much more practical than requiring the subject to undertake the NSAA assessment.
	\item Investigating how well models perform when they are familiar with the subject as opposed to when the model has never seen the subject before in training (even if it was trained on different data from the same subject than was used for assessing the subject).
	\item Assessing the applicability of generalisation techniques that includes downsampling the data, adding Gaussian noise to the data set, concatenation of features for multiple measurement types, and the leaving-out of anomalies within the subjects.
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Basis of Research Project\\}

%%%%%%%%%%%%%%%%%%
\section{Duchenne Muscular Dystrophy}

\quad The data that we shall be working with for this project is from subjects who have varying severities of Duchenne muscular dystrophy (DMD). DMD is a genetic disorder that is characterized by progressive muscle degeneration and weakness and is caused by the absence of dystrophic, a protein that helps keeps muscle cells intact [5]. This leads to increasing levels of disability and is a progressive condition, meaning that it gets worse over time. It’s classified as a rare disease, with around 2,500 patients in the UK and an estimated 300,000 sufferers worldwide [6]. There are currently no known cures for any form of muscular dystrophy (MD), though there are treatments available to help manage the conditions [7].\\

\quad DMD is one of the more severe forms of MD and generally affects boys in their early childhoods, and those with the condition generally only live into their 20s or 30s. The muscle weakness starts in early childhood and symptoms are usually first noticed between the ages of 2 and 5. The weakness mainly affects the muscles near the hips and shoulders, so among the first signs of the disorder are when the child has difficulty getting up off the floor, walking, or running. The weakness progresses to eventually affect all muscles used for moving and also those involved in breathing and the heart muscle. Many are confined to a wheelchair by 12 years of age, with those in their late teens generally losing the ability to move their arms and experiencing progressive problems with breathing.\\

\quad With the aim to increase the life span and movement options of sufferers of DMD, a range of treatments are available. These range from steroids to increase muscle strength, physiotherapy to assist with mobility, and surgery to correct postural deformities. And thanks to advances in cardiac and respiratory care, life expectancy for sufferers is increasing and many are able to survive into their 30s and 40s with careers and families, with there even being cases of men with the condition living into their 50s. Additionally, there is ongoing research looking into ways to repair the genetic mutations and damaged muscles associated with various forms of MD.\\

%%%%%%%%%%%%%%%%%%
\section{The North Star Ambulatory Assessment}

\quad The North Star Ambulatory Assessment (NSAA) is a 17-item rating scale that is used to measure functional motor abilities in subjects with DMD and is generally used to monitor the progression of the disease and the effects of treatments. The tests are to be completed without the use of any thoracic braces or any equipment assistance that may help the subject to complete the activities. To carry out the assessments, the assessor conducting the assessments needs a mat, a stopwatch, a box step, a size-appropriate chair, and at least 10-metres of pathway [8].\\

\quad To carry out the assessment, the assessor gets the subject to carry out 17 sequential tasks. Each task is graded as follows: ‘2’ if there is no obvious modification of activity, ‘1’ if the subject uses a modified method but achieves the goal independent of physical assistance from another, and ‘0’ if the subject is unable to complete the activity independently. The 17 activities involved in the assessment with the requests to the subject are given below:

\begin{enumerate}
	\item \textbf{Stand}: "Can you stand up tall for me for as long as you can and as still as you can?”
	\item \textbf{Walk}: “Can you walk from A to B (state to and where from) for me?”
	\item \textbf{Stand up from chair}: “Stand up from the chair, keeping your arms folded if you can”
	\item \textbf{Stand on one leg – right}: “Can you stand on your right leg for as long as you can?”
	\item \textbf{Stand on one leg – left}: “Can you stand on your left leg for as long as you can?”
	\item \textbf{Climb box step – right}: “Can you step onto the top of the box using your right leg first?”
	\item \textbf{Climb box step – left}: “Can you step onto the top of the box using your left leg first?”
	\item \textbf{Descend box step – right}: “Can you step down from the box using your right leg first?”
	\item \textbf{Descend box step – left}: “Can you step down from the box using your left leg first?”
	\item \textbf{Gets to sitting}: “Can you get from lying to sitting?”
	\item \textbf{Rise from floor}: “Get up from the floor using as little support as possible and as fast as you can (from supine).”
	\item \textbf{Lifts head}: “Lift your head to look at your toes keeping your arms folded.”
	\item \textbf{Stand on heels}: “Can you stand on your heels?”
	\item \textbf{Jump}: “How high can you jump?”
	\item \textbf{Hop right leg}: “Can you hop on your right leg?”
	\item \textbf{Hop left leg}: “Can you hop on your left leg?”
	\item \textbf{Run (10m)}: “Run as fast as you can to….(give point).”
\end{enumerate}

\quad The NSAA assessment has been shown to be a quick, reliable, and clinically relevant method to measure the functional motor ability of ambulant children with DMD, and is also considered to be suitable to be used in research. It has also been shown to have high intra-observer reliability and high inter-observer reliability [9]. This means that NSAA is generally fairly reliable so as to be used as part of research assuming adequate training is provided to assessors. Furthermore, the hierarchy of items within NSAA was shown to be supported by clinical expert opinion, with items in the NSAA assessment being listed based on their level of difficult which is agreed upon by most experts, while a questionnaire-based study shows that clinicians generally consider NSAA as clinically relevant [10].\\

%%%%%%%%%%%%%%%%%%
\section{The KineDMD Research Initiative}

\quad The project undertaken as described in this report is part of a wider research initiative known as the ‘KineDMD’ study, conducted by Imperial College London in collaboration with Great Ormond Street Hospital (GOSH). The study involves around 20 DMD subjects (‘D’) subjects and 10 healthy control (‘HC’) subjects who participate in the study for 12 months, who are assessed wearing a sensor suit on selected days during clinical assessments at GOSH, along with using fitness tracker bracelets in the form of Apple watches throughout the trial, which collect data of everyday movements while the subjects are at home or school. A broad aim of the research initiative is to make use of AI to make sense of the data patterns collected from the suit and watches for each of the subjects which, from there, would aid doctors in being able to monitor disease progression with more precision [11]. The initiative has been funded with £320,000 through the Duchenne Research Fund to develop and test the bodysuit that captures the motions of subjects suffering with DMD [4].\\

\quad The hope is that insights found would cut down the time taken to test new treatments and thus drive down the costs of future clinical trials. A further aim of the initiative is that the developed suit and associated AI techniques and research projects undertaken as part of the initiative will help determine whether any new treatment regimes are working, which would be able to help inform doctors on future treatments. This is particularly useful for specialists, as the condition can be difficult to treat due to relatively slow progression and each subject responds uniquely; furthermore, many of the assessments are done by ‘eye’ instead of using measurement and objective methods. The initiative hopes that bringing AI techniques into the assessments will take a lot of the human fallibility elements out of the assessments and give a more informed perspective that is better able to understand the progression of the condition in the subjects.\\




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Overview of Recurrent Neural Networks\\}

%%%%%%%%%%%%%%%%%%
\section{Outline}

\quad As recurrent neural networks (RNNs) will form the basis of the engineering aspect of the project through implementations using Python and supporting libraries, it’s necessary to outline some of the theory and applications prior to implementing them; in doing so, we can explore why exactly they are useful when adapted to work with the body suit data we’re concerned with and to solve the problems we’re looking to solve. We begin by exploring the shortcomings of feedforward neural networks (FFNNs) and how using RNNs in their place works to overcome them (and, in particular, why they’re applicable here). We then touch on the mathematical basis of how the hidden nodes’ states change with respect to time and the input, along with how we use long short-term memory (LSTM) units to help deal with the vanishing/exploding gradient problems. Finally, we look at how we can implement this type of network within a Python script by means of the TensorFlow framework, including how we build the model, train it, and test it on unseen data.\\

%%%%%%%%%%%%%%%%%%
\section{Motivation, Architecture, and Training}

\quad A problem with FFNNs is that they don’t handle the order of data that is fed into the network: each sample fed through and classified or regressed is independent of all other samples. For example, if a convolutional neural network is trained to determine whether images are either of a cat or of a dog, then its assessment of each image’s label is independent (rightly so) of the images its seen before. This is appropriate in many situations; however, here we’re dealing with time-series data from the suit, where each row (‘frame’) of joint angle values, position values, and so on that are contained in a ‘.mat’ file produced by the body suit is a single sample in time. We also know instinctively that these values in real-life are dependent on previous values: for example, for a person in movement, the position of their various body parts are influenced by what they were a short time ago. FFNNs don’t handle order of the values of data that are fed in. For example, if there were inputted numerous frames of data from the body suit, then it would treat each as independent entities with regard to the network’s predictions.\\

\quad This lack of memory with FFNNs is something that RNNs attempt to fix in the following way: rather than the values of the hidden nodes of the FFNN being only affected by the values that feed into it (e.g. the network inputs or the values from the previous layer), hidden layers in RNNs are also affected by their own previous values. In contrast with FFNNs, RNNs share their weight parameters across different time steps: this allows a sequence to extend and apply the model to examples of different forms and generalize across them (which allows a sentence like “I went to Nepal in 2009” and “In 2009 I went to Nepal” to recognize the year, ‘2009’, in the same way)[12]. The resultant core architectural difference between the two can be seen in the image below:\\

\begin{center}
\includegraphics[scale=1.4]{project_figures/fig3_1}
\end{center}

\quad In this sense, the RNN can be seen to contain a memory of sorts that enforces time-dependencies of data that is fed through it. If we considered a sequential model where the state of a hidden node $h^t$ is modified by not only the input $x$ but also the layer’s previous state $h^{t+1}$, we can essentially ‘unfold’ this dependency with respect to time to get:\\

\begin{center}
\includegraphics[scale=0.8]{project_figures/fig3_2}
\end{center}

\quad In other words, the output of a hidden node at time $t$ is given as $h^{t}$ and is a function $f$ of the input at this time from the previous layer $x^t$ and the output of the same layer at the previous time step $h^{t-1}$. This can therefore be seen as the ‘memory’ aspect of an RNN: values from previous parts of a sequence carry over to influence the subsequent parts. It should be noted, however, that this is only within sequences and subsequent sequences are considered to be independent of each other (in the same way that images fed through a convolutional neural network are independent of each other); hence, the choosing of the correct time-dependency in the form of the sequence length is a key aspect of experimentation which we further look into in our experimentation. This idea of unfolding in time can be extended to apply to multiple layers and multiple nodes per layer and can be seen in the general equation that the hidden nodes in an RNN use to calculate their output:\\

\begin{center}
\includegraphics[scale=0.4]{project_figures/fig3_3}
\end{center}

\quad Note that the above function $\phi_h$ is equivalent to $f$ in the above diagram, as in both cases they represent the activation function for layer $h$. We interpret this as the output of a hidden layer at time step $t$ being a combination of the previous hidden layer output and the current input to the hidden layer, with both being modified by their respective weight matrices. It’s also important to note that, not only is there a weight matrix $W_{xh}$ that is learned through training to map from the previous layer’s values to the current one, but there is an additional weight matrix $W_{hh}$ that is learned to control how much of the same layer’s previous values impact the current layer. Alternatively, a way to look at it is the $W_{hh}$ matrix controls the influence each left-to-right arrow in the unfolded sequence image has on the state it points to, while the $W_{xh}$ controls how much influence up down-to-up arrow in the unfolded sequence image has on the state it points to.\\

\quad This requirement of using the additional weight matrix for the states of the hidden layers is also reflected in the backpropagation through time (BPTT) equation for the updating of the $W_{hh}$ matrix via gradient descent:\\

\begin{center}
\includegraphics[scale=0.27]{project_figures/fig3_4}
\end{center}

\quad The idea is that the overall loss $L$ is the sum of all the loss functions from times $t=1$ to $t=T$, and since the loss at time $t$ is dependent on the hidden units at all previous time steps, the gradient is as seen above. Note that its chain rule structure is still very similar to standard backpropagation used in FFNNs, with a primary difference coming from the impact of all previous values of the hidden layer prior to time $t$ on the overall derivative of loss with respect to $W_{hh}$. Below, we can also see how these derivative values propagate backwards through time:\\

\begin{center}
\includegraphics[scale=1.0]{project_figures/fig3_5}
\end{center}

\quad However, a large problem arises from the $\frac{\partial h^t}{\partial h^k}$  terms having $t-k$ multiplications in the above equation, which therefore multiplies the weight matrix $W_{hh}$ as many times. If this weight matrix is less than 1, this factor becomes very small, which results in a vanishing gradient, severely impacting the ability of the network to train on data and thus learn anything useful (the opposite happens if the weight matrix is greater than 1, which results in the network having an exploding gradient and never converging). To counteract this, a common way to implement sequence models is by using gated RNNs and, for this project, we chose to use LSTM units.\\



%%%%%%%%%%%%%%%%%%
\section{The Long Short-Term Memory RNN Architecture}

\quad The idea of using gated RNNs, which includes the LSTM architecture, is that we are able to create paths through time that have derivatives that neither vanish nor explode and involve connection weights that may change at each time step. Gated RNNs are also automatically able to decide when to clear a hidden state (i.e. set it to 0), and a core idea of LSTMs is to introduce loops within themselves to produce paths where the gradient can flow for long durations of time, while the weight on this internal path loop is conditioned on context, rather than fixed as in the standard RNN. The weight of this path is controlled by another unit and thus the time scale can be changed based on the input sequence [12]. A diagram of a single LSTM network cell and how it interacts with the wider RNN can be seen in the image below, with the LSTM unit itself being the central part of the three green boxes below:\\

\begin{center}
\includegraphics[scale=0.7]{project_figures/fig3_7}
\end{center}

\quad The central idea of this input is the horizontal line running through the top of the unit which allows the data to run along the unit relatively unchanged if required. The gates, represented in the above small yellow boxes within the central green box, allow the unit to let information in (we shall denote these as gates 1 to 4, where gate 1 is the leftmost yellow box and gate 4 is the rightmost box). These gates are there to protect and control the cell state [13].\\

\quad Gate 1 is the ‘forget gate’, which decides which information to ‘throw away’ from the cell state and is a combination of $h_{t-1}$ and $x_t$ and outputs a number via the sigmoid function $\sigma$ between 0 (which signifies to ‘get rid of this completely’) and 1 (‘keep this completely’). This could see applicability with a word sequence (i.e. a sentence) where we wish forget older parts of a sequence in order to make a more accurate assessment of the next word of the sequence based on more recently occurred words. The output of this gate $f^t$ is given as:\\

\begin{center}
\includegraphics[scale=0.7]{project_figures/fig3_8}
\end{center}

\quad Gate 2 is known as the ‘input gate’, which decides the values we’ll update within the cell in order to store new information in the cell state. This is used in conjunction with Gate 3, which is a ‘tanh’ gate that creates a vector of new candidate values that can be added to the state. The equations governing the outputs of each of these two parts are given as:\\

\begin{center}
\includegraphics[scale=0.7]{project_figures/fig3_9}
\end{center}

\quad These two are multiplied together to get the new information that we wish to store in the cell, which replaces the old information that we have lost via the ‘forget gate’.\\

\quad With the information we wish to discard having been forgotten and the new information that we wish to replace it with having been calculated, we then turn to modifying the old cell state $C_{t-1}$ into the new cell state $C_t$. This is done by multiplying the previous cell state by the forget gate output to forget the things we decided earlier to forget, followed by adding the new proposed candidate values scaled by how much we wish to update each value, and is given by the equation that governs the new cell states information:\\

\begin{center}
\includegraphics[scale=0.7]{project_figures/fig3_10}
\end{center}

\quad Now that we have the cell state obtained, we finally decide how much of this information to output from the cell (i.e. a filtered version of the cell). We then use Gate 4 (the ‘output gate’) to decide which parts of the cell we shall output, which we multiply by the ‘tanh’ of the new cell state $C_t$ (which makes sure the cell state outputs between -1 and 1). This ensures that we only output the parts we decided to output (e.g. in the case of the language model it allows one to only output information pertinent to verbs if that is what comes next in the sequence). The output of the output gate and of the cell itself is thus given as:\\

\begin{center}
\includegraphics[scale=0.7]{project_figures/fig3_11}
\end{center}

\quad In using LSTMs as part of our architecture, we enable the models to condition themselves on sequences with some parts forgotten within the cell and also being able to chose which parts of hidden states it wishes to output to the next layer and the next state of the same hidden layer. As a result, this architecture of the hidden units is much more conducive to modelling long-term relationships within a sequence and also being able to train via backpropagation with much reduced effects from the vanishing and exploding gradient problems.\\



%%%%%%%%%%%%%%%%%%
\section{Implementing an RNN using TensorFlow}

\quad With all that said, there still must be a practical way of implementing RNNs using LSTM units as part of the project for the RNN architecture to actually be useful for us. Fortunately, there exists numerous open source APIs and Python libraries that handles much of the underlying details of a neural network that simply needs the user to design the architecture. For this project, TensorFlow was chosen to be the API of choice due to previous experience in using it for implementing RNNs in time-series data in other work, along with excellent supporting documentation being available and the ability to easily utilize a GPU to help with training the model. Further justification can be found in the ‘System Choices’ chapter of the report.\\

\quad A detailed guide on building an RNN using TensorFlow is beyond the scope of this report; however, it was felt worthwhile to outline some of the central elements of the models that are built in ‘rnn.py’ and how they relate to the concepts outlined above. While we shall give a detailed breakdown of the script itself in the ‘Script Ecosystem Overview’ chapter of the report, we now turn our attention to specific sections of code within ‘rnn.py’ that are particularly significant to the architectural makeup of the models:\\

\begin{itemize}
	\item The input shape of the model is setup with a placeholder variable that sets the input size equal to (x, y, z), where $x$ is the batch size (i.e. number of sequences per training batch), $y$ is the sequence length (i.e. the number of frames of data that is ‘pushed through’ the model per sequence; generally either 60 for raw measurement data or 10 for computed statistical values), and $z$ is the dimensionality of the frames itself (e.g. 66 for joint angle data).
\begin{center}
\includegraphics[scale=0.8]{project_figures/fig3_12}
\end{center}
	The equivalent is also done for the $y$ data, depending on the output type the model is training towards.
	\item We define the LSTM cells, with their size and number of cells (i.e. equivalent to the number of nodes per hidden layer and number of hidden layers, respectively, if we were using a ‘traditional’ RNN) in a single line of code: we define multiple \textit{BasicLSTMCell} objects with a size set as \textit{self.lstm\_size} (a hyperparameter that we can tune), a dropout percentage given as \textit{tf\_keepprob}, and create multiple of these in a loop, with the number of these given as \textit{self.num\_layers}. These multiple cells (i.e. hidden layers of the model) are then used to create a \textit{MultiRNNCell} object, which acts as a wrapper for all the hidden layers of the model:
\begin{center}
\includegraphics[scale=0.6]{project_figures/fig3_13}
\end{center}
	\item Finally, we set up the model architecture so that the input $x$ held in the placeholder \textit{tf\_x} feeds into the ‘cells’ (i.e. the hidden layers), which modifies the initial state of the model throughout the application of the input sequence $x$ to the hidden layers to give us an output \textit{lstm\_outputs} and a final state of the layers, \textit{self.final\_state}:
\begin{center}
\includegraphics[scale=0.6]{project_figures/fig3_14}
\end{center}
	\item These steps are defined within the \textit{build()} method for the ‘RNN’ class which is called upon by the constructor of the object at object creation. Hence, when we create an RNN object as…
\begin{center}
\includegraphics[scale=0.5]{project_figures/fig3_15}
\end{center}
…it results in setting the attributes of the RNN object, including most of the hyperparameters that influence the architecture of the object, and calling the \textit{build()} method of the object that uses many of these hyperparameters. Thus, the above statement sets up the computational graph that defines our RNN model which is now ready to have data inputted through it for the training process.
	\item To train the model, the method \textit{train()} is called by the ‘rnn’ object, which results in splitting the training data components \textit{x\_train} and \textit{y\_train} into batches, whereupon each batch-pair (i.e. a batch of \textit{x\_train} with a batch of \textit{y\_train}) is placed into a dictionary that matches train components to batches (i.e. it would match a batch of \textit{x\_train} to the \textit{tf\_x} placeholder variable described above, which ensures that the \textit{x\_train} batch is used as input to the model). Each of these dictionaries are then fed through via the \textit{session.run()} method that specifies we are training the model (which hence calls upon the optimizer within \textit{build()} to train the model) and takes in the dictionary to train the model on each batch:
\begin{center}
\includegraphics[scale=0.6]{project_figures/fig3_16}
\end{center}
	\item A similar process is then used when we wish to test the model via the \textit{predict()} method called by the ‘rnn’ object. Much like \textit{train()}, it splits the \textit{x\_test} data into batches, which it adds to the ‘feed’ dictionary (setting the dropout probability to 0\% this time via \textit{tf\_keepprob:0: 1.0} and the session to use the \textit{test\_state} of the model) and calls the \textit{sess.run()} method to push this dictionary through the model to get the predictions made on the batch. We retrieve the predictions based on the output type we are working towards and adds the predictions obtained to the list of predicted values for the \textit{x\_test} input:
\begin{center}
\includegraphics[scale=0.6]{project_figures/fig3_17}
\end{center}
\end{itemize}

\quad While there are, however, many other steps in the process of using the ‘rnn.py’ to build the models, these are some of the crucial steps where we applied knowledge of RNNs and their usefulness to sequence modelling to create a deep learning solution in TensorFlow. And with easy access to other libraries that make reading in data from ‘.csv’ and ‘.xlsx’ files and manipulating it as matrix data easy (e.g. via ‘pandas’ and ‘numpy’) and general-purpose machine learning libraries such as ‘sk-learn’ to help with other tasks (such as the splitting and shuffling of sequences for training/testing, the evaluating of various metrics like mean squared error, and so on), we have all the resources needed to build RNN models using LSTM units using the applicable preprocessing steps to tailor it towards working with our data pipeline and produce results that can be observed and compared within experiment sets and model predictions sets.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\part{System Preparation: Choices and Setup}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{System Choices: Language, IDE, and Libraries\\}


%%%%%%%%%%%%%%%%%%
\section{Python}

\quad As with most software-related projects, one of the primary choices that must be made is what programming language to implement the components of the system in, along with what development environment it is to be built using. Both of these have a large impact in the time and ease it will take to develop the system, as well as how optimal it will be running in its final variation. For the choice of programming language, we chose to use \textbf{Python (3.6)} to build all scripts from for the following reasons:


\begin{itemize}
	\item It takes very few lines to implement many things when compared with other languages like Java; for example, the code below shows how a neural network can be implemented in Python in only 9 lines using the Keras library (a wrapper for TesorFlow):
\begin{center}
\includegraphics[scale=0.8]{project_figures/fig4_1}
\end{center}
	This enables faster development and easier testing of new ideas and concepts than other languages, as we can afford to care less about problems of tricky syntax (e.g. with using C++) and can instead move towards development ‘at the speed of thought’.
	\item The open-source nature of Python's community means that there are very often libraries that others have built that fit the profile of what we need, so we don't need to 'reinvent the wheel' by creating our own version of it; this can be seen in the system’s reliance on external functions from ‘scikit-learn’ to compute metrics, along with ‘pandas’ to handle the reading and writing to and from ‘.csv’ files, as opposed to writing our own functions to carry out this functionality.
	\item Python has seen extensive use for building and testing machine learning and deep learning models by research and business communities; thus, it is easily the most well-developed with regards to open-source libraries such as TensorFlow, with TensorFlow's Python API being most complete of its various language implementations [14].
	\item Many of the frameworks and libraries that power the system we have built (e.g. NumPy and SciPy) build upon lower-layer Fortran and C implementations for fast and vectorized operations for multidimensional arrays, which helps overcome the inferior speed of a scripting language like Python when compared to these lower-level languages [15].
	\item Further development of the system is easier, as being written in a language like Python (which is close to English) lends itself to the easier understanding of what is going on within each script. Coupled with variables having intuitive names and commenting where necessary, this helps with anyone undertaking edits or rewrites to one or more of the scripts in the future.
\end{itemize}

%%%%%%%%%%%%%%%%%%
\section{Integrated Development Environment}

\begin{center}
\includegraphics[scale=1]{project_figures/fig4_2}
\end{center}
\quad With regards to where we shall develop the Python programs for this project, we have chosen to use the \textbf{PyCharm Community Edition 2016} integrated development environment. We can see above a screenshot of how the IDE is setup for this project. This has been chosen for the following reasons over a text editor or another IDE:

\begin{itemize}
	\item \underline{\textit{Multiple program tabs}}: This enables the easy transitions between different scripts. This is particularly useful when we are making changes to multiple scripts simultaneously (e.g. adding the same optional argument to both ‘rnn.py’ and ‘mode\_predictor.py’ that ensures certain models built by ‘rnn.py’ are then retrieved correctly by ‘model\_predictor.py’).
	\item \underline{\textit{In-built terminal}}: This allows us to run programs from command line within the IDE itself without having to open a separate terminal window and navigating to the script directory every time it’s reopened. This is a major quality of life improvement, as most of the scripts are run from the command line with required and optional arguments.
	\item \underline{\textit{Good compatibility with git}}: This has two main benefits. The first is that changes made to scripts and their specific lines of change from a previous commit are highlighted in blue, which helps with accounting for modifications made when writing commit messages and keeping track of all recent changes made. The other  benefit is the GUI approach to committing to the GitHub repo (as can be seen below) which is a much easier way to make regular commits and also highlights easier more subtle changes made (such as writing lines to output files).
\begin{center}
\includegraphics[scale=1]{project_figures/fig4_3}
\end{center}
	\item \underline{\textit{Previous experience}}: We’ve used it before for many previous Python projects, including in two professional roles, for a final-year undergraduate individual project, and for many pieces of coursework involving the use of several machine learning and deep learning libraries such as ‘scikit-learn’ and ‘TensorFlow’. Hence, this previous experience and the resultant familiarity with the environment helps make development of this project a more expedient and easier experience.
	\item \underline{\text{Debugging and error handling}}: The layout of PyCharm makes for writing and immediate testing and modifying of code very simple, with debug options showing locations of compilation errors very easily and with clarity. This minimizes the time lost in development due to basic syntax errors and other basic software-engineering-related issues.
	\item \underline{\text{Package implementation}}: It’s easy to add additional packages via 'Available Packages' in the 'Project Interpreter', which is useful as we need to add many additional libraries, from TensorFlow to simple libraries like ‘pyexcel’.
\end{itemize}


%%%%%%%%%%%%%%%%%%
\section{TensorFlow}

\quad For programming in Python, there are numerous options for which library we can use to implement our central RNN models. One option is the ‘Keras’ wrapper that wraps the TensorFlow framework. Although this is a lot simpler to use and has fewer aspects to manually code, there are numerous advantages that TensorFlow has over this that includes the following:

\begin{itemize}
	\item More extensive and highly detailed documentation and examples for TensorFlow.
	\item Higher amount of direct control over the RNN models with things such as weights and optimizers.
	\item Better performance with TensorFlow through threading and queues to speed up the training process.
	\item Availability of the TensorBoard visualization tool to help understand our models.
\end{itemize}

\quad Thus, using TensorFlow will hopefully lead to more successful RNN models that can better learn from raw measurements and computed statistical values that can thus better operate on newly-presented subject data. Preparatory work for using this framework includes prior use as the engine for an undergraduate individual project, along with reading Chapter 14 and 16 of [15] where we learned:

\begin{itemize}
	\item The benefits of using TensorFlow for neural network training performance in utilizing GPU cores, where using a high-end GPU resulting in ~15 times more floating-point calculations per second than using an equivalently-priced CPU.
	\item Concepts of graphs, sessions, ranks, tensors and operations, which gave clarity to the concepts of the computational graph structure used by TensorFlow.
	\item The 'placeholder' concept of  TensorFlow where a variable is an 'empty' variable that expects data input (in our case, these 'placeholders' will be implemented for \textit{x\_train}/\textit{x\_test}).
	\item How aspects specific to an RNN works in TensorFlow, such as implementing layers as LSTM cells and initial/final states for the variables.
\end{itemize}

\quad With this obtained knowledge from the above textbook, along with examination of other examples found primarily on GitHub or the TensorFlow documentation website, we felt confident enough in our knowledge of the TensorFlow library that, coupled with prepared input data, our RNN models could now be created.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{System Setup: Software and Data Preparation\\}

%%%%%%%%%%%%%%%%%%
\section{Overview}

\quad A necessity to either continue further work on this system, validate the results we obtained in experimentation and elaborate upon in this report, or produce one’s own results through system use is to setup the required components to run the system. Before being able to modify or run parts of the system, there are several things that must be setup beforehand. This includes:

\begin{itemize}
	\item Obtaining relevant system resources.
	\item Downloading the requisite data sets and setting them up in the correct directories.
	\item Setting up Python and ‘pip’, along with the IDE and necessary packages.
	\item Running all the setup scripts that are run via the‘setup.cmd’ script.
	\item Setup of TensorFlow to use a GPU.
\end{itemize}

\quad Once these steps are all completed, the editing of scripts, building of models, testing of files, and so on can be done by the user. In this part of the report, we shall be going through all the necessary steps to run the system on a different workstation; the hope is that, with the steps completed in this section, any user with the necessary computational resources can build models, reproduce results, and carry out additional experiments using the suit data captured that is used as part of the project.

%%%%%%%%%%%%%%%%%%
\section{Necessary System Resources}

\quad As this system works with large amounts of data and requires a heavy computational workload in order to build and test models, among other tasks, anyone running parts of this system requires a workstation setup with the necessary resources to execute many of the scripts, store the data, and so on. The vast majority of the work done on this project has been undertaken on a ‘Dell XPS 15 (9570)’ laptop with the following specs:

\begin{itemize}
	\item \underline{\textit{CPU}}: 'Intel Core i7-8750H’
	\item \underline{\textit{RAM}}: ‘16GB DDR4, 2666MHz’
	\item \underline{\textit{GPU}}: ‘Nvidia GeForce GTX 1050Ti with 4GB GDDR5’
	\item \underline{\textit{Storage}}: ‘512GB SSD’
\end{itemize}

A system with similar specs should be adequate to run the system; however, the following is ideal:

\begin{itemize}
	\item \underline{\textit{CPU}}: At least a 7th gen Intel i5 or i7 (or AMD equivalent). A lot of the data preparation, computing of statistical values, reading from and writing to ‘.csv’s, and so on are done using the CPU (i.e. anything the system does that’s not including the training, testing, and assessing of models); hence, a good CPU will enable the user to run these scripts in reasonable time.
	\item \underline{\textit{RAM}}: Minimum of 10GB needed; some of the larger datasets we look at (e.g. when multiple raw measurements’ data are combined into one data set for a data shape of (~16000, 60, ~180)) require in excess of 8GB of memory just for the data, not including resources required for the IDE and other parts of the script being run. Any less than 10GB, therefore, may risk system instability or limit the user from carrying out certain parts of the experimentation outlined in this report. Additionally, DDR4 is recommended so as to increase the speed at which data is able to be written and read from memory.
	\item \underline{\textit{GPU}}: We make heavy use of TensorFlow running using the inbuilt GPU; the alternative would be to use the CPU, which by our estimation is approximately 10x slower to train models than using the GPU. Hence, to realistically build models in this system we need a good GPU. Ideally, the user would use an ‘Nvidia’ card as that is the easiest to setup with TensorFlow and, preferably, the card would have many CUDA cores (the 1050ti has 768) to enable faster parallel processing when training models.
	\item \underline{\textit{Storage}}: As of writing, the total storage required for all data sets contained within the ‘local directory’ (including ‘.mat’ files for NSAA assessments, 6-minute walks, natural movement behaviour, and the intermediate data extracted from all files via the data pipeline) amounts to over 170GB, while the system itself contained within the ‘project directory’ requires approximately 725MB of space; hence at least this much storage is required, ideally on an SSD to enable fast read speed from storage (which will happen a lot during the setup scripts). See the chapter on the ‘Project and Local Directories’ for more information about what these specific directories contain.
\end{itemize}


%%%%%%%%%%%%%%%%%%
\section{Data Sets Setup}

\quad With a workstation obtained with the requisite resources, the next step is obtaining the data sets required by the system. There are two approaches that can be taken: the first involves being able to access the link shown below (which should be possible for anyone with Imperial College London credentials), which contains all the data used as part of this project (i.e. that also contains all the intermediate data constructed via the scripts that make up the data pipeline). Hence, a user that downloads all the data from this link would not need to run the Python scripts contained within the ‘setup.cmd’ script, only the pip installation commands. The second approach should be taken if the user either doesn’t have Imperial College London credentials or otherwise can’t access the files, or if one wishes to run the pipeline ‘from scratch’ (i.e. computing ones own intermediate data files via the data pipeline); this does require the user to fully run the ‘setup.cmd’ script. Note that the below explanations assume the user already has access to the complete ‘project directory’ if one is reading this report; however if not, consult the chapter on ‘Project and Local Directories’ for guidance on how to access this.\\

The first approach is as follows:

\begin{enumerate}
	\item Setup a base directory in the user’s storage directory; the default name for this that has been used thus far has been ‘C:\textbackslash msc\_project\_files\textbackslash ’; however, a more intuitive name can be chosen by the user if desired. This becomes the user’s ‘local directory’ for the project.
	\item Download each of the directories contained within the OneDrive link: \url{https://imperiallondon-my.sharepoint.com/:f:/g/personal/djh18\_ic\_ac\_uk/Euymu00dXG1Cmmeoz3xxq24BekH57ZuDmU9uZtoSr60xfg?e=L5C62Z} and place each directory in the ‘msc\_project\_files’ directory (e.g. ‘left-out’, ‘allmatfiles’, etc.) within the user’s created ‘local directory’.
	\item Modify the requisite line within ‘$<$project directory$>$\textbackslash source\textbackslash settings.py’ to point to this new location. For example, if the user has setup the ‘local directory’ as ‘example’ in ‘C:\textbackslash’, then they should modify the ‘local\_dir’ variable (line 7) to now contain: ‘local\_dir=”C:\textbackslash example\textbackslash”. In doing this, it ensures that all other scripts that need to access the data files in ‘example’ are correctly pointed to it.
	\item Once the steps outlined in the section below on Python, pip, PyCharm, and the necessary packages have been undertaken, open ‘$<$project directory$>$\textbackslash source\textbackslash\\ batch\textbackslash setup.cmd’ for editing in a text editor, comment out line 19 and onward (as these create the intermediate data from the pipeline which we now already have), save the file, and run it to setup the Python packages.
\end{enumerate}

The second approach is as follows:

\begin{enumerate}
	\item Setup a base directory in the user’s storage directory; the default name for this that has been used thus far has been ‘C:\textbackslash msc\_project\_files\textbackslash’; however, a more intuitive name can be chosen by the user if desired. This becomes the user’s ‘local directory’ for the project.
	\item Obtain permission from the owners of the data sets used as part of the KineDMD research initiative to access and download the directories.
	\item Create another directory within the ‘local directory’ called ‘output\_files’. This shall contain a number of things produced by the scripts and by the models, including the ‘.csv’ files of computed statistical values, the constructed models themselves, among other parts of the system.
	\item The user should have links to the following data sets (though if they don’t the requisite permission for each must be obtained by the relevant parties): ‘NSAA’, ‘NMB’, ‘allmatfiles’, ‘6MW-matFiles’, and ‘6minutewalk-matfiles’. Each of these directories should then be downloaded and directly placed within ‘local\_dir’.
	\item Once the steps outlined in the section below on Python, pip, PyCharm, and the necessary packages have been undertaken, run the ‘setup.cmd’ in full to obtain the necessary Python packages for the project and compute the intermediate data used as part of the project.
\end{enumerate}

\quad With these steps done, the data should now be in a form in which all the scripts that form the system should be able to access with the necessary directories constructed.


%%%%%%%%%%%%%%%%%%
\section{Setup of Python, Pip, Necessary Packages, and PyCharm}

\quad We now turn our attention to the setting up of the language, the ‘pip’ tool for package installation, and the libraries required to run the scripts. The first step is the installation of Python; the version this system runs on is ‘3.6.0’ and, while installing any subsequent versions should be acceptable, we shall install this version to avoid any possible complications to do with the language further down the line. Version ‘3.6.0’ can be downloaded from \url{https://www.python.org/downloads/release/python-360/} and by selecting the relevant installer from ‘Files’. With the installation window open, ensure that the ‘Add Python 3.6 to PATH’ box is checked; this shall ensure that the user is able to run Python commands from the command line or terminal:

\begin{center}
\includegraphics[scale=0.8]{project_figures/fig5_1}
\end{center}

\quad With this installed, we can ensure that Python is setup correctly with the required version by entering ‘python --version’ at the command line:

\begin{center}
\includegraphics[scale=0.8]{project_figures/fig5_2}
\end{center}

\quad It should be noted that the installation window for the user should also say that it is installing ‘pip’. This can be asserted to have been installed for the user by entering ‘pip’ at the command line, which should show output of something like this:

\begin{center}
\includegraphics[scale=0.8]{project_figures/fig5_3}
\end{center}

\quad If, for whatever reason, ‘pip’ has not been installed, follow the steps outlined at \url{https://pip.pypa.io/en/stable/installing/}, which includes the modification of the PATH environment variable to ensure that we are able to use ‘pip’ correctly.\\

\quad Once ‘pip’ has been installed and/or asserted to be setup ready to use, we are able to install the required packages. Navigate to the ‘$<$project directory$>$\textbackslash source\textbackslash batch\_files’ directory and execute the ‘setup.cmd’ script. This will setup the required packages to run the project. Not only that, but it will ensure that the versions of each of the packages installed for this system will also be setup by the user; this should minimize the chances that the user will encounter dependency issues between packages (different versions of ‘tensorflow’ have been shown to interact badly with certain versions of ‘numpy’, for example). It should be noted that ‘setup.cmd’ will also run all the Python scripts within the ‘source’ directory that are used to setup the files for other scripts such as ‘rnn.py’ and ‘model\_predictor.py’, so if the user doesn’t wish to run these at this time, the recommended way is to comment out each of these lines. This can be done by commenting out each of these setup lines (line 19 and onward) by inserting ‘REM ‘ at the beginning of each line. Note that this means that ‘setup.cmd’ must be run later when the user wishes to prepare the data for the system and if the user doesn’t already have the intermediate data.


%%%%%%%%%%%%%%%%%%
\section{Installation of PyCharm (IDE)}

\quad If the user is intending to do any long-term modifications to the system, or if they simply want a more convenient place to launch the scripts from, then it is recommended that they install an IDE for the project; specifically, PyCharm Community Edition. Using this provides several advantages to the user:

\begin{itemize}
	\item An in-built terminal to run the scripts of the system with the necessary arguments.
	\item Easy interaction with Git to work from the project code in GitHub/GitLab.
	\item Syntax assistance when editing any files.
	\item Multiple tabs to help with editing multiple files simultaneously along with the script variable explorer.
\end{itemize}

To setup the IDE, the following steps should be undertaken:

\begin{enumerate}
	\item Download the community edition of PyCharm, the link for which can be found at: \url{https://www.jetbrains.com/pycharm/download/#section=windows}.
	\item In the ‘Installation Options’ window, it’s recommended that the user selects the ‘Add “Open Folder as Project”’ option (to allow opening the ‘project directory’ as a PyCharm project) and the ‘.py’ association (so Python files open in PyCharm as default).
	\item Launch PyCharm and select ‘Do not import settings’.
	\item From the ‘Customize PyCharm’ window, click ‘Skip Remaining and Set Defaults’.
	\item Prior to continuing with PyCharm setup, we first must setup Git if the user doesn’t have it already. The following section applies to Windows, but the equivalent can easily be done for iOS or Linux.
	\begin{enumerate}
		\item Download Git from \url{https://git-scm.com/download/win}.
		\item Run the installation, making note of where Git is installed, with the default settings.
	\end{enumerate}
	With this now completed (or not, depending on whether Git is already installed), the user should launch PyCharm and in the ‘Welcome to PyCharm’ window, click ‘Configure’ and ‘Settings’, followed by navigating to ‘Version Control’ and Git. In the ‘Path to Git executable’, enter the location of the ‘git.exe’ program that was just installed (or previously installed). This can be found within the ‘Git\textbackslash bin\textbackslash’ directory within the location where Git was setup. Click ‘Apply’ and ‘OK’.
	\item In the ‘Welcome to PyCharm’ window, select ‘Check out from Version Control’ and Git. This is because we will be directly installing the ‘project directory’ directly from ‘GitHub’. Note that it’s recommended doing this even if the source directory has already been downloaded and setup elsewhere, as doing it this way ensures that it is easily to modify and commit to git if any changes to the scripts are to be made. Within this new window, enter the URL: \url{https://github.com/dan-heaton/MSc_indiv_project} within the URL window and click ‘Clone’ to clone the repository. Alternatively, if one wishes to clone from GitLab instead, swap the URL above with: \url{https://gitlab.doc.ic.ac.uk/djh18/MSc_indiv_project}.
	\item The final step is to associate PyCharm with the Python interpreter we have previously installed. To do so, with the project opened in PyCharm, navigate to ‘File’ and ‘Settings’. Under ‘Project: MSc\_indiv\_project’ and ‘Project Interpreter’, click the settings icon and ‘Add...’. Under ‘System Interpreter’ the Python executable should already be detected within ‘Python36\textbackslash’ as ‘python.exe’. However, modify this path to the ‘python.exe’ file if has not done so already. Click ‘OK’, ‘Apply’, and ‘OK’. In the main PyCharm window, it may take a few seconds to configure this interpreter but, once done, the user will be able to edit and run the scripts of the system within PyCharm.
\end{enumerate}


%%%%%%%%%%%%%%%%%%
\section{Configuring of TensorFlow to Use the GPU}

\quad The following section works on the assumption that the user is working on a workstation containing a GPU. This is more or less a necessity to build models using ‘rnn.py’: while a reasonable GPU with $>$700 CUDA cores builds a typical model in 5-15 minutes, building these using an equivalently-priced CPU would take 1-3 hours. While the necessary steps to undertake the complete setup of a GPU are somewhat arduous, there exists a useful guide to doing so at \url{https://www.tensorflow.org/install/gpu}, along with a CUDA installation guide at \url{https://docs.nvidia.com/cuda/} for multiple OS’s. This includes details on setting up the CUDA toolkit and the CuDNN (CUDA Deep neural Network library), which are required to run TensorFlow on the GPU. With this setup, TensorFlow with subsequently run as default in all scripts using the GPU to create models as opposed to using the CPU.











%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\part{System Overview and Explanation} 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Project and Local Directories: Overview and Explanations\\}


%%%%%%%%%%%%%%%%%%
\section{Background}

\quad Prior to undertaking an in-depth discussion of the individual scripts, their uses, and the various experiment and model prediction sets we undertook as part of this project, it’s worth giving a brief explanation of the two types of directories that we are concerned with for this project. There are two directories that we are concerned with: the ‘project directory’ (containing the source files, the documentation used and written to, among other things) and the ‘local directory’ (containing the source ‘.mat’ files that we use as inputs to scripts and some of the outputs of the models, including the models themselves). Below, we cover each of the two in turn, how to access and/or set them up, and so on. The aim is thus to educate any users on how the project is laid out and how each part of the system connects to each other.


%%%%%%%%%%%%%%%%%%
\section{The Project Directory}

\quad The project directory is the directory containing all of the source code, the information required about the NSAA subjects (‘nsaa\_6mw\_info.xlsx’), the documents containing the results of the experiment sets and model prediction sets, and other reports and presentations constructed throughout the duration of the project. Hence, this is where the majority of the deliverables of the project lie, with the exception of the majority of the models that were created throughout the project and the data that is required to train the models. The reason we keep these apart (i.e. why project directory and local directory are not synonymous) is as follows:

\begin{itemize}
	\item The local directory requires $>$170GB of storage for all the data sets that is used in the project. However, we’ve been making extensive use of storing the project directory within DropBox and as a Git repository and, as it would be not possible and very impractical, respectively, to store the local directory on both DropBox and within GitHub or GitLab, we chose to keep these separate.
	\item When we run several of the data pipeline scripts (e.g. ‘comp\_stat\_vals.py’ or ‘ext\_raw\_measures.py’) we end up producing a lot of new files within the local directory). Therefore, to avoid constant DropBox synchronizations or many new or deleted files to appear in each Git commit, it was felt that it would be easier to simply keep the data aspects apart from the source code and other documentation.
	\item Permission may be required to deal with certain data directories contained within the local directory, as this contains data about ongoing subjects of a research initiative that is not freely available. Hence, it was the desire to keep the project directory available to whoever wished to access it, without predicating access on also being able to access the data required to populate the local directory (e.g. if one wished simply to browse or borrow ideas from the scripts within the project directory); this also enables and encourages an easier transition to applying the project to other data sets possibly for other domains.
\end{itemize}

\quad To access the complete project, the advisable way to obtain it would be to clone it via GitHub. The repo can be accessed at \url{https://github.com/dan-heaton/MSc\_indiv\_project} and cloned via \url{https://github.com/dan-heaton/MSc\_indiv\_project.git}. Alternatively, one can access it via GitLab at \url{https://gitlab.doc.ic.ac.uk/djh18 /MSc\_indiv\_project} and clone with \url{https://gitlab.doc.ic.ac.uk/djh18/MSc\_indiv\_project.git}.  The current name for the project directory as used in its local form for development has been ‘indiv\_proj; however, one could rename this freely without requiring any other changes to the scripts. However, it’s recommended that the directories without the project directory should not be changed (e.g. ‘source’ or ‘documentation’), as doing so would require rewriting of several of the scripts that hardcoded paths to access things outside of its own directory.

Broadly speaking, the directories can be broken down as the following:

\begin{itemize}
	\item \textbf{background\_research}: this folder contains some preparatory programs that were written (with help from sources cited in the scripts themselves) to familiarize oneself with the building of RNNs with the chosen libraries in order to make using them for the actual project later that much easier; hence, these aren't used by the rest of the project at all and are included for historical documentation reasons only.
	\item \textbf{documentation}: contains a number of files pertaining to the outputs of the data pipeline for the project. This includes the following:
	\begin{itemize}
		\item \textit{'RNN Results.xlsx'}, which covers the performances of various model setups on test data (i.e. a large proportion of the experimentation that covers different types of raw measurements, sequence lengths, overlaps, etc., source their results from here). These are what form the basis of our later discussion on the experiment sets.
		\item \textit{'model\_predictions.csv'}, which (unlike 'RNN Results.xlsx') shows the performance of using 'model\_predictor.py' to assess the performance of pretrained models on whole data files. These provide the information needed for the model predictions sets, as discussed later. 
		\item \textit{'model\_shapes.xlsx'}, which is just to be used by 'model\_predictor.py' to set the sequence length to the correct value (and is not particularly relevant to the user).
		\item \textit{'nsaa\_6mw\_info.xlsx'}, which contains a table of the subject names and their corresponding single-act and overall NSAA scores (this provides the necessary \textit{y\_labels} for many RNN models).
		\item \textit{'nsaa\_17subtasks\_matfiles.csv'}, which is the Google annotations sheet that was collaboratively created by others within the wider research initiative that contains the file names and times within said files where the 17 NSAA activities are performed by each of subjects. This is determined by watching the source videos of the ‘.mov’ files of the subjects performing the activities and recording at what times in the video these activities occur, along with making use of the ‘dis\_3d\_pos.py’ script; this sheet is then used by ‘mat\_act\_div.py’ to create the single-act files used in later model predictions sets.\\
	We also have several other subdirectories in ‘documentation’:
		\item \textbf{Graphs}: all graphs created by 'graph\_creator.py' are placed in here. These source from 'RNN Results.xlsx' and 'model\_predictions.csv' to create graphs that are easier to display the results of groups of experiments done than it would be to display the same information using a table. We see many of these graphs later on within the discussion of the experiment sets.
		\item \textbf{Script Explanations}: collection of 'READMEs' for each of the scripts within 'project\_files\textbackslash source'. The idea is that, if one wishes to find out what each script does, why it was written, etc., then reading its relevant 'README' should provide sufficient detail. Much of these READMEs form the basis of our script overview later on.
	\end{itemize}
	\item \textbf{paper\_reviews}: contains paper reviews done of research papers that are believed to be relevant to the project. These predominantly focus on the use of RNNs when applied to real-world human movement data, and each paper consists of a slightly-shortened bullet-pointed version of the paper and then a section of the most significant points from these bullet points. Hence, these papers are useful in justifying decisions taken with respect to model choices, experimentation directions, etc.
	\item \textbf{presentations}: contains a collection of presentations that have been created to display to group members about the project's progress thus far (which were kept in order to aid in final report writings).
	\item \textbf{report\_stuff}: contains several initial reports and other documentations of project progress thus far, and also 'MSc Project Plan.ods', which is where the already-completed and upcoming task lists are stored; this is particularly useful if one wishes to see what is currently being worked on or has recently been completed. The vast majority of the contents of this directory, however, are contained within this report.
	\item \textbf{source}: contains all of the scripts that are needed by the project pipeline to run. This includes the core Python scripts (such as 'comp\_stat\_vals.py' and 'rnn.py'), along with some 'supporting' scripts, such as 'settings.py' (to contain global variables that are used across several scripts). For information about how to run each of these scripts, run the script of interest through the command line/terminal with the ‘--help' optional argument set (e.g. ‘python comp\_stat\_vals.py --help'). This will display each of the arguments that are available to be set, the significance of each, how they interact with other arguments (if relevant), etc.
	\begin{itemize}
		\item \textbf{Batch files}: within this, we contain the batch scripts that are used to automate some of the running of the scripts. For further info about the significance of any or all of the scripts, consult the 'README(s)' for the relevant scripts in '$<$project directory$>$\textbackslash documentation\textbackslash Script Explanations\textbackslash', the script ecosystem overview in 'plans\_and\_presentations', or the diagram of the scripts and their connections to each other found at the beginning of the ‘Script Ecosystem Overview’ chapter. Along with some of the simpler automation of the tasks, we also run each of the model predictions sets from their respective batch files, as many of them require building many models and testing many different combinations of files, which require many runs of ‘rnn.py’ and ‘model\_predictor.py’; hence, the automation of this makes the process of replication hopefully much easier for the user.
	\end{itemize}
\end{itemize}


%%%%%%%%%%%%%%%%%%
\section{The Local Directory}

\quad The local directory is considered to have two purposes: to store the data sets that we make use of in this project, and to store the direct outputs of the ‘rnn.py’ scripts that include the models themselves and the ‘.csv’ output predictions that are written directly on a sequence by sequence basis (e.g. for a model created with ‘rnn.py’ using a test set of 1000 sequences, there will exist within this ‘.csv’ each 1000 predicted value and true value, depending on the output type set by the user). There are three reasons why we include this ‘rnn.py’ output within the local directory:

\begin{itemize}
	\item As we create many models as part of this project, the size of this directory has become an issue and thus we would prefer to keep it separate from the project directory for space reasons. We therefore want to keep a lot of this data ‘clutter’ apart from the what is considered the ‘core’ of the project with the project directory.
	\item We also want to keep a consistent philosophy with which we consider to be ‘intermediate data’, which is data that exists as a product of one script and that is used by other scripts: in this case, the models produced are intermediate data in that they are created by ‘rnn.py’ and used by ‘model\_predictor.py’. This holds for other forms of intermediate data such as data produced by ‘comp\_stat\_vals.py’ and ‘ext\_raw\_measures.py’, and so we wish to do the same for the models and ‘rnn.py’ predictions output.
	\item The majority of these models are only used once as part of one particular experiment set or model predictions set, and therefore they don’t form a part of the ‘complete’ system (with the exception of the final selected models that are contained within the ‘source’ directory, though this is only a small number of the overall number of created models). Thus, we keep these models separate from the ones constituting the completed system at the end of all experimentation; in other words, the models that are intended to be used by a user to do assessments with specific subjects are contained within the project directory, while the models created during all experimentation are contained within the local directory.
\end{itemize}

\quad To access the complete local directory of files that we have been used for this project, use the OneDrive link given as \url{https://imperiallondon-my.sharepoint.com/:f:/g/personal/djh18\_ic\_ac\_uk/Euymu00dXG1Cmmeoz3xxq24BekH57ZuDmU9uZtoSr60xfg?e=L5C62Z} where one can find a directory given as ‘msc\_project\_files’. This is the local directory as used during the development of the project. Download and store it while modifying the local variable in ‘settings.py’ (as directed in the ‘System Setup’ chapter). Note that the total directory is in excess of 170GB, so sufficient space may need to be made for it beforehand.


%%%%%%%%%
\subsection{The Local Directory: 'rnn.py' Outputs}

\quad We first look at the two sub-directories within the local directory containing the outputs of ‘rnn.py’:

\begin{itemize}
	\item \textbf{output\_files\textbackslash rnn\_models}: this contains all models that have been produced by ‘rnn.py’ throughout the course of the project, including the final models used that are contained within the project directory. Each model’s contents are the product of the TensorFlow library working through ‘rnn.py’ and each model consists of a directory that looks something like this:
\begin{center}
\includegraphics[scale=0.3]{project_figures/fig6_1}
\end{center}

\quad These files constitute a single model in the eyes of ‘model\_predictor.py’, which is the only script that makes used of these models. For instance, within these files contains the model input and output shapes, the numbers of hidden layers, other hyperparameter settings, and the weights of each of the neuron connections. In other words, it’s a fully trained model that is ready to be used by ‘model\_predictor.py’ to be used on a whole subject’s data file.\\\\
\quad The names of the models may seem unnecessarily long and complex, but they are in fact simply the exact arguments used to invoke the instance of ‘rnn.py’ that creates this specific model, excluding the always-necessary ‘python rnn.py’ parts of the argument sequence. There are two reasons why we do this:
	\begin{itemize}
		\item This creates the names of the directories automatically, which takes much of the human-element of labelling each directory out of the process based on what experiment set or model prediction set it is used for.
		\item In having them written by name based on the ‘rnn.py’ arguments, we can ensure that ‘model\_predictor.py’ will always use the correct models during experimentation by writing a simple set of rules within ‘model\_predictor.py’. For instance, if we want ‘model\_predictor.py’ to use models that have been created with added Gaussian noise, we can ensure that it uses only model directories that contain ‘--noise' within their names. Hence, it’s an easier way to determine the correct models to use rather than analysing the contents of the model directory (e.g. which would mean looking into the ‘model.ckpt.meta’ file) to determine if it’s a model we need to use.
	\end{itemize}
	\item \textbf{output\_files\textbackslash RNN\_outputs}: each model that is built by ‘rnn.py’ that is also tested on a test partition of data (i.e. if the ‘--no\_testset’ argument is not set) writes a separate ‘.csv’ file to this directory. Each file contains, for a given created model, the arguments used to invoke it (in the form of the ‘.csv’ file name), the ‘overall’ results of the test set on the model (e.g. MAE of data for the ‘overall’ output type, accuracy of data for the ‘dhc’ output type, etc.), the individual predictions made for each test sequence versus their true values, and the model hyperparameter settings. An example of this can be seen below:
\begin{center}
\includegraphics[scale=0.2]{project_figures/fig6_2}
\end{center}
\end{itemize}

\quad The contents of this file are entirely optional to use, however, as all the requisite information about the test set performance (that is to be inputted into ‘RNN Results.xlsx’ and then used as part of experiment sets) are also produced as console output, which is easier to copy over for the user. However, this files serve as a log of model performance through time as we continue to create more models if we wish to use them as a reference at any point.



%%%%%%%%%
\subsection{The Local Directory: Data Sets}

\quad With the directories containing model outputs having been covered, we shall now look at the directories containing the raw data sets, what is contained within each directory, and what ‘type’ of data these directories contain. It should be noted that the ‘source’ scripts assume that each of these directories are a constant (i.e. that any other users don’t modify the names of the directories); any changes made to the names here require modifications to the necessary variables within ‘settings.py’ (e.g. the ‘sub\_dirs’ variable).\\

\quad Prior to looking at each data set in turn, it’s preferable to briefly discuss the general locations of raw source data (e.g. as source ‘.mat’ files) and intermediate data. This becomes particularly relevant when we shall shortly be discussing the contents of each data directory, and knowing what produced intermediate data and where is conducive towards understanding the data pipeline. Below, we can see how each script within the data pipeline produces data and where the data is stored:

\begin{enumerate}
	\item \textbf{'comp\_stat\_vals.py'}: This script takes the data stored as source ‘.mat’ files from within the data directories within the local directory and outputs data to the corresponding path within 'output'. For example, if ‘comp\_stat\_vals.py’ intends to operate on ‘NSAA’ (based on the ‘dir’ argument passed to it), it sources its data from '$<$local directory$>$\textbackslash NSAA\textbackslash matfiles' as ‘.mat’ files and produce the computed statistical values in '$<$local directory$>$\textbackslash output\_files\textbackslash NSAA\textbackslash AD' as ‘.csv’ files. This functions in the same way for other source data sets (e.g.' 6minwalk-matfiles') where the path to the computed statistical value files is more-or-less the same as the source directory path with 'output\_files' appended after the path to the local directory.
	\item \textbf{'ext\_raw\_measures.py'}: Unlike computed statistical values, the produced raw measurement files are instead stored within subdirectories of the source data set directory that they are sourced from, as opposed to a subdirectory of 'output\_files'. For example, if ‘ext\_raw\_measures.py’ intends to extract the raw measurements from the '6minwalk-matfiles' directory, it retrieves files from '$<$local directory$>$\textbackslash 6minwalk-matfiles\textbackslash all\_data\_mat\_files' and writes each measurement extracted per file to a sub-directory with a name matching the measurement name (e.g. ‘D4’s joint angle data is written to '$<$local directory$>$\textbackslash 6minwalk-matfiles\textbackslash all\_data\_mat\_files\textbackslash jointAngle' as ‘D4-6MinWalk-jointAngle.csv’ while its position data is written to '$<$local directory$>$\textbackslash 6minwalk-matfiles\textbackslash all\_data\_mat\_files\textbackslash position' as ‘D4-6MinWalk-position.csv’, and so on).
	\item \textbf{'mat\_act\_div.py'}: It should be noted first that, as this script extracts single-act files from complete-act files, it therefore only expects to be used on the 'NSAA' directory, as only files from 'NSAA' contain NSAA activities. When activities are divided, they are placed either within 'act\_files' or 'act\_files\_concat' (depending on whether ‘--single\_act\_concat’ was set for the script) within the 'NSAA' directory itself, much like ‘ext\_raw\_measures.py’. For example, ‘mat\_act\_div.py’ would pull source ‘.mat’ files from '$<$local directory$>$\textbackslash NSAA\textbackslash matfiles' and write single-act files (non-concatenated) to '$<$local directory$>$\textbackslash NSAA\textbackslash matfiles\textbackslash act\_files' as source ‘.mat’ files but only containing single-activities within each. Note that if ‘ext\_raw\_measures.py’ then operates on these single-act files, they get written to a subdirectory within here, much like how it would operate on complete-act files; hence, drawing the joint angle files from the above case would write the files to '$<$local directory$>$\textbackslash NSAA\textbackslash matfiles\textbackslash act\_files\textbackslash jointAngle'. Similarly, when we compute the statistical values of single-act files, they get written to the 'output\_files' directory; in the above case, the computed statistical values of the single-act NSAA files would be written to '$<$local dir$>$\textbackslash output\_files\textbackslash NSAA\textbackslash AD\textbackslash act\_files'.
	\item \textbf{'ft\_sel\_red.py'}: To simplify the process of storing the feature-reduced variants of the ‘.csv’ outputs of ‘comp\_stat\_vals.py’, we decided to store the files in the exact same directory as the files they operate on. For example, if we wish to reduce the dimensions of the computed statistical value files for the 'NMB' source data directory, we would write the feature-reduced files to '$<$local directory$>$\textbackslash output\_files\textbackslash NMB\textbackslash AD', where the computed statistical values are already stored. The difference here, however, is that the feature-reduced equivalents will have ‘FR\_’ appended to the front of each of the files. For example, the computed statistical values for the ‘D4’ subject whose data is from 'NMB' will be stored within the above path as ‘AD\_D4\_stats\_features.csv’, and when ‘ft\_sel\_red.py’ operates on this subject, it will take the data from this file and write to a file stored in the above path as ‘FR\_AD\_D4\_stats\_features.csv’ (alternatively, if the feature-reduced-concatenation option is set within ‘ft\_sel\_red.py’ it will instead be stored as ‘FRC\_AD\_D4\_stats\_features.csv’). This naming convention ensures that ‘rnn.py’ fetches the feature reduced variants of files to ensure that models of input nodes size >4000 isn’t required.
	\item \textbf{'rnn.py'}: Although this is considered part of the data pipeline, this simply fetches the data from all of the above locations dependent on the arguments given, while we have already discussed the output locations of ‘rnn.py’ in the previous section.
\end{enumerate}

\quad With the relationship between the data pipeline scripts and the data set directories having been established, we now move on to examining the data that’s contained within each of these directories. The data directories are as follows:

\begin{itemize}
	\item \textbf{6minwalk-matfiles}: This contains the 6-minute walk data of many (but not all) of the subjects within two sub-directories within this directory: 'all\_data\_mat\_files' and 'joint\_angles\_only\_matfiles'. The former contains the source ‘.mat’ files for all available measurements for the subjects’ walking assessments, while the latter contains source ‘.mat’ files with only joint angles. Therefore, what we consider ‘JA’ and ‘DC’ files (‘Joint Angle’ and ‘Data Cube’, respectively) that are referenced in experiment set 1 comes from 'joint\_angles\_only\_matfiles', while in most other cases when we use the data of 6-minute walk assessments, we use the 'all\_data\_mat\_files' directory.
	\item \textbf{6MW-matFiles}: This directory also contains some of the 6-minute walk assessment files for some of the subjects and, while some of the files overlap with '6minwalk-matfiles', it also contains assessment data that is not found in the other directory. Additionally, we don’t see any ‘joint angle only’ data within this data set, and so all source ‘.mat’ files are stored directly within this directory.
	\item \textbf{allmatfiles}: This contains the natural movement behaviour as source ‘.mat’ files that contains only the joint angle data (much like '6minwalk-matfiles\textbackslash joint\_angles\_only\_matfiles' files). As we only had the natural movement in ‘joint angle only’ form for quite a while (until we were given the 'NMB' data set), we had to make use of this for several of the later model predictions sets, though we later received the natural movement behaviour in true ‘AD’ form (i.e. containing all the measurements data for each subject) as 'NMB'.
	\item \textbf{left-out}: This is a small sample of data files that have been excluded from the main data sets that can act as data that is left-out of any of the data sets used to train models. It should be noted the difference between assessing using ‘model\_predictor.py’ on files from left-out as opposed to 'left-out' subjects (as used in many model predictions sets): assessing a file from the 'left-out' directory will assess a model that is familiar with the subject (through having been trained on other files of the same subject), while assessing a complete left-out subject will assess a model that is not familiar with \underline{any} files for the specific subject.
	\item \textbf{NMB}: This is the complete ‘AD’ data for the natural movement behaviour, as opposed to 'allmatfiles' which only contains the joint angle data in source ‘.mat’ file form. As a result of receiving this data late in the project lifecycle, we are only able to use this data set in later model predictions sets. It should be noted the sheer number of files per subject: many of the subjects have up to 30 files captured from each of them that will have captured a variety of ‘natural’ activities, such as sitting, playing, eating, and so on. The data contained within these files, therefore, is much more ‘unstructured’ than either the 6-minute walk or NSAA assessment data.
	\item \textbf{NSAA}: This contains the data for each of the subjects’ full NSAA assessments, and the source ‘.mat’ files specifically are contained within the 'NSAA\textbackslash matfiles' subdirectory. It should be noted that, for some subjects, their assessments are split over several files. We account for this when extracting raw measurements and computing statistical values by simply concatenating these source files with respect to time, while we select the correct file to use to get the single-act files via ‘mat\_act\_div.py’ by referencing the file name columns within the Google annotations sheet.
\end{itemize}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Reference Documents Explanation\\}

%%%%%%%%%%%%%%%%%%
\section{Background}

\quad As we can see from the system overview diagram shown at the beginning of the ‘Script Ecosystem Overview’ chapter, as well as having many Python and batch scripts that play integral roles within the system we also make heavy use of several documents that provide much needed information for training models and serve as places to store the results of experiments. These include the Google sheet of single-act annotations, the reference document containing the overall and individual scores of each subject used in the project, the ‘model\_predictions.csv’ file, and the ‘RNN Results.xlsx’ file. Each of these files in turn can be found within the project source directory within the ‘$<$project directory$>$\textbackslash documentation’ directory. In the section below, we will discuss file each in turn, from the information they contain to how and where they are used within the system (i.e. what other scripts depend on them and what scripts feed into the documents). The aim here, therefore, is to give a more concrete understanding of what these documents contain prior to seeing them referenced in the results discussion and general system overview sections.

%%%%%%%%%%%%%%%%%%
\section{Google Annotations Sheet ('nsaa\_17subtasks\_matfiles.csv')}

\begin{center}
\includegraphics[scale=0.4]{project_figures/fig7_1}
\end{center}

\quad As part of the wider research initiative, we collaboratively undertook to analyse and record the times of each activity undertaken by the subjects as part of the initiative. The information that was collected into this document, therefore, was intended to be used as part of several projects that relied on the start and end times of each activity undertaken by the subjects. As a result, the subjects’ corresponding activity videos were divided into three parts and each of us determined the activity times for our given subjects.\\

\quad The process to undertake these annotations was as follows:

\begin{enumerate}
	\item Load either the source video that corresponds to the ‘.mat’ file (which is provided in the data sets as ‘.mov’ files) OR use the corresponding ‘.mat’ file with the ‘dis\_3d\_pos.py’ script to load a basic 3D dynamically updating image through the ‘matplotlib’ library (more on this in the ‘Script Ecosystem Overview’ chapter).
	\item For each activity of the NSAA assessment set (e.g. ‘raise from floor’, ‘run’, ‘step up using right foot’, etc.), observe the time in seconds (or frames) the activity starts and finishes in the file. Note that we only count the \textbf{first completed} activity within the source file and we try to be slightly accommodating of the start time (for example, if the activity started between 3s and 4s in the file, we record it as having started at 3s to ensure we capture the complete activity with a bit of ‘slack’).
	\item For the subject in question and for the activity in question, record the start and end times in \textbf{frames} in the ‘start’ and ‘end’ columns for the activity (note that if the time was observed in seconds, simply multiply this by 60 as the suit samples at 60Hz), along with recording the name of the file that the activity occurred in (as this will be the same name as the corresponding ‘.mat’ file the suit data for this video will be in, just with a ‘.mat’ file extension instead of ‘.mov’).
\end{enumerate}

\quad In the image above, we can see a snapshot of several activities that have been recorded for some of the subjects, including the file names containing the activity in question for the subjects, along with the start and end times within the respective files. It should be noted that not every activity could be drawn from each of the subjects’ files. This could be due to the subject simply not performing the activity they were told to perform or were otherwise unable to perform the activity. In these cases, the start and end times will be marked with either a ‘-1’ or ‘0’, with a ‘0’ sometimes signifying an incomplete activity annotation done by the annotator.\\

\quad For our project, the main use of this document is as a tool for the ‘mat\_act\_div.py’ script. For each of the source files that the script wishes to divide up, it will look in the table for the name of the subject in question for that source file, find the row corresponding to that file and, for each activity, get the start and end times for that activity and extract the corresponding frames from the source file (making sure its name matches that of the activity’s ‘filename’ column entry). So while these single-act ‘.mat’ files will be further processed by other scripts (such as ‘comp\_stat\_vals.py’ and ‘ext\_raw\_measures.py’), ‘mat\_act\_div.py’ is the only file that directly relies on the Google annotations sheet, and no file within the system modifies it in any way.


%%%%%%%%%%%%%%%%%%
\section{NSAA Scores Reference Document (‘nsaa\_6mw\_info.xlsx’)}

\begin{center}
\includegraphics[scale=0.4]{project_figures/fig7_2}
\end{center}

\quad The next script in the system is the reference file which we use to create the $y$ labels used by every model that is built in the system. As a result of it being the only place that contains the $y$ label information for each of the subjects, it’s referenced by several scripts including ‘ft\_sel\_red.py’, ‘rnn.py’, and ‘model\_predictor.py’. The script itself was a collaboration of several other scripts found within the source data directories for ‘NSAA’ and ‘6minwalk-matfiles’, including ‘KineDMD data updates Feb 2019.xlsx’ and ‘nsaa\_matfiles.xlsx’. Some of these scripts contain information about certain subjects that others don’t; hence, rather than having the scripts checking each of the files in turn, it was felt that it would be easier to combine the information from all of them into one file.\\

\quad Each of the models that are built by the ‘rnn.py’ script are built to target one output type, which will be either ‘dhc’ (the D/HC label), ‘overall’ (the overall NSAA score), ‘acts’ (the 17 individual activity scores), or ‘indiv’ (the score of the individual activity assuming the input are single-act files) for each of the sequences the model is training and subsequently assessing on (see the ‘Data Forms and Types’ chapter for more information on each of these output types). For the ‘dhc’ output type, we don’t need to reference this file, as this can be determined simply by the file name. For example, in preprocessing the data into $x$ and $y$ components within ‘rnn.py’, if the given data that is currently being preprocessed for a given file is from a file called ‘D4\_position.csv’, then we know that the ‘dhc’ label would be ‘D’ (or 1 when being fed into the network), while if the data came from the ‘HC6\_position.csv’ file the label would be ‘HC’ (or 0).\\

\quad However, for the other types of $y$ labels that we use to train our ‘rnn.py’ to target the other output types, it’s not as simple as observing the name of the file. This is where the NSAA score reference file comes in. As we can see in the above image, it contains the information for every subject we have (that was collected by the initial assessors of the subjects) that includes their individual activity scores and their overall NSAA score. Hence, by finding the relevant row within the document, we get all the information we need for the other output types. For example, let’s say the preprocessing function of ‘rnn.py’ is extracting data from the file corresponding to subject ‘D16’.\\

\quad Once the data file has been separated into their sequences with necessary sequence overlap and an optional proportion of the sequence dropped (more on these later in the discussion of experiment sets), we then need to get the corresponding label for this sequence. If the model is being trained for the ‘overall’ output type, then we check the table for the value in the ‘D16’ subject’s ‘NSAA’ column, which corresponds to ‘23’. This is the $y$ label that each sequence extracted from the ‘D16’ source file would get if we are training for the ‘overall’ output type. Alternatively, if the model output type was instead ‘acts’, the $y$ label would be a list of values ‘[2, 2, 1, 2, 2, 1, 1, 1, 1, 2, 0, 2, 1, 2, 1, 1, 1]’ or, if the source file was a specific single act file for the subject, e.g. ‘D16\_position\_act4.csv’, then it would get the label ‘2’ (both of which are determined by the above table. This process of label extraction is identical across multiple scripts, be it for training models in ‘rnn.py’ or getting the true values of assessing subjects in ‘model\_predictor.py’, and so on.


%%%%%%%%%%%%%%%%%%
\section{Results for Experiment Sets (‘RNN Results.xlsx’)}

\begin{center}
\includegraphics[scale=0.35]{project_figures/fig7_3}
\end{center}
\begin{center}
(continued)
\end{center}
\begin{center}
\includegraphics[scale=0.35]{project_figures/fig7_4}
\end{center}

\quad With the two main reference files used by the project covered, we now move onto the first of the two documents containing the results of the various experiment sets and model predictions sets that we run. The first of those, ‘RNN Results.xlsx’, contains the information of building various models within the ‘Experiment Set’s section of the results discussion which we cover later on. These include testing different measurements with which to build models, examining different sequence lengths and sequence overlap proportions, and so on. What separates this document from the ‘model\_predictions.csv’ document that we shall discuss shortly is that ‘RNN Results.xlsx’ contains the results obtained from just analysing the testing data sets supplied to the ‘rnn.py’: this is generally 20\% of the source data set that is fed into the ‘rnn.py’ script. Hence, the results contained in each cell of the ‘Results’ column for this document covers the performance of various model setups on this testing data. In contrast, for most of the ‘model\_predictions.csv’ sets these are the results of feeding in complete files of subjects to be assessed on prebuilt models via the ‘model\_predictor.py’ script. Therefore, ‘RNN Results.xlsx’ gets its results from the direct console output of building models in ‘rnn.py’, while ‘model\_predictions.csv’ gets its results from running ‘model\_predictor.py’ on models already built in ‘rnn.py’ and assessing on complete files.

\quad Another difference from ‘model\_predictions.csv’ is that ‘RNN Results.xlsx’ has its information manually inserted into the table, as opposed to having it automatically done in ‘model\_predictions.csv’ via the ‘DataFrame.writecsv()’ method called in ‘model\_predictor.py’. The reason this is manually done is that, for each model that is created that we wish to write to a row of in ‘RNN Results.xlsx’, there is additional information that ‘rnn.py’ has no knowledge about and therefore cannot write to the table. This includes a short description of the model that has been created and what prior scripts were needed to have been run in order to preprocess the data that is required for this model (such as ‘comp\_stat\_vals.py’ or ‘ext\_raw\_measures.py’). Therefore, when we wish to write a row of data to ‘RNN Results.xlsx’, we manually fill in parts of the table ourselves and copy/paste in other parts of the console output (here, into the ‘Results’ column). Additionally, along with the description, necessary prior scripts with arguments to have been run, and the results of the model, we also output the general model configuration that includes the shape of the data and several of the more significant hyperparameters. The idea is that anyone examining our results in the results discussion section can refer back to this table fairly easily and see exactly how the models were built.

\quad The way these results from ‘RNN Results.xlsx’ are generally assessed is via the ‘graph\_creator.py’ script. This script reads directly from the file, grabs the requested rows, and plots one or more of the columns against each other depending on the arguments (for example, one configuration of ‘graph\_creator.py’ could plot the results of the MAE of the overall NSAA score against the sequence lengths of the corresponding rows, which we do so in experiment set 8). This is therefore the primary way with which we use the ‘RNN Results.xlsx’ file in this report, though it also serves the purpose of providing results which can be compared to via other users using the system.


%%%%%%%%%%%%%%%%%%
\section{Results for Model Predictions Sets (‘model\_predictions.csv’)}

\begin{center}
\includegraphics[scale=0.4]{project_figures/fig7_5}
\end{center}
\begin{center}
(continued)
\end{center}
\begin{center}
\includegraphics[scale=0.4]{project_figures/fig7_6}
\end{center}

\quad The final document that we make heavy use of in the system is the ‘model\_predictions.csv’ document. This is where any assessments made by the ‘model\_predictor.py’ script is stored (and by extension ‘test\_altdirs.py’ which calls ‘model\_predictor.py’ numerous times) and as such corresponds to the model predictions sets that are discussed later in the ‘Experiments and Results Discussion’ chapter. As all the information that we wish to write to the file is known by the ‘model\_predictor.py’ at run time, this processed is automated via the script itself and does not require the user to enter information into the table manually. As the primary differences between the two documents have already been outlined, it just remains to outline the types of information that is recorded in the table along with how we make use of this when assessing various model setups and investigating performance of different variations of subject files in the model predictions sets.\\

\quad Each assessment made by ‘model\_predictor.py’ writes one row of information to this table. This contains the name of the subject we are assessing on, along with extra strings that signify different types of models that subject was assessed on or any other preprocessing steps that were taken for the subject file(s) (see the relevant model prediction set descriptions to see to what each of these additional strings correspond). The name of the directory that the subject file(s) are sourced from is also included, along with any other directories that act as alternative directories from which to source files (see model prediction set 1 for an example of this). We also include the different measurements extracted from this subject’s file(s) that were used to act as input to the necessary models (the exact models that are chosen therefore depend on the types of measurement that are extracted from the subject file in question).\\

\quad The rest of the columns contain the assessed results of the subject for all the output types we are testing the subject on (which is generally the ‘overall’, ‘acts’, and ‘dhc’ output types). For example, the ‘Predicted ‘D/HC Label’’ reflects the results of the assessing the subject on models built for the ‘dhc’ output type, while the ‘True ‘Overall NSAA Score’’ and ‘Predicted ‘Overall NSAA Score’’ reflects the results of assessing on models built for the ‘overall’ output type. Therefore, we use all the information from subject assessments on various types of models to create tables comparing different subjects and setups (as shown in many model predictions sets) or to be used via ‘graph\_creator.py’, which can access ‘model\_predictions.csv’ in a similar way to ‘RNN Results.xlsx’ in order to create graphs over various rows of the table, based on the arguments passed to ‘graph\_creator.py’.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Data Forms and Types\\}

%%%%%%%%%%%%%%%%%%
\section{Background}

\quad Before moving onto how the data is actually used to train and test an RNN model, it’s worth outlining the forms the data can take, what it actually represents, and how we treat these forms differently. This is necessary so that the RNN model itself is able to treat these data files in the same way (though giving noticeably different results depending on the nature of the data file); this avoids the problem of having to create a unique RNN script for every different type of measurement (‘input type’) and model type we are targeting (‘output type’).

%%%%%%%%%%%%%%%%%%
\section{The Source '.mat' Files}

\quad All the data that we are concerned with is captured by subjects wearing the Xsens MVN inertial motion capture system (i.e. the body suit). Further information on how this suit works can be found in the “MVN User Manual.pdf” file in the base ‘project directory’. Each single file have is captured by one instance of a \textit{single} subject wearing the suit (i.e. one subject’s visit to the hospital). The files themselves are stored as ‘.mat’ files within a tree structure containing the data measurements themselves, among other metadata including segment, sensor, and joint names, the time and date the data was captured, and other relevant metadata. To access this information we are concerned with (i.e. non-metadata), we can first open the ‘.mat’ file in MATLAB before navigating to ‘tree.subject.frames.frame’. An example of what the data values within a ‘.mat’ file look like at this point can be seen below as the aforementioned ‘data values’ within the ‘.mat’ file:

\begin{center}
\includegraphics[scale=0.25]{project_figures/fig8_1}
\end{center}
\begin{center}
\includegraphics[scale=0.25]{project_figures/fig8_2}
\end{center}

\quad Note that each row is a single sample (what we call a ‘frame’) captured by the bodysuit and 60 of these rows correspond to the data captured over 1 second (as the sensors sample at 60 Hz). The columns, meanwhile, primarily consist of measurements that are captured by the suit’s 17 inbuild sensors. Note that there are different aspects that are measured by the suit depending on the measurement; for example, for ‘position’ the sensors measure the positions of 23 segments on the body suit in 3D space which, given that it’s measuring in 3 dimensions, correspond to 69 values for a given time instance (i.e. a single ‘row’ of data), while ‘jointAngle’ measures using the 22 joints of the suit, which gives 66 total values. The result is that, over a single time instance of 1/60th of a second, approximately 739 distinct values are captured by the suit. This is what we mean when we refer to an ‘all data’ file (or ‘AD’ file for short): it contains all the possible data captured by the suit for an instance of the subject wearing the suit.\\

\quad In contrast to this, we are also provided with ‘.mat’ files that correspond to the same subject’s instance of wearing the suit but that only contain the joint angle measurements. The idea behind this is that, as it’s believed that joint angles will be an important measurement for model training in several scenarios (classification of file type, regression of overall NSAA score, etc.), the ‘joint angle’ files (or ‘JA’ files for short) provide a simple jumping-off point to train some preliminary models (the results of which we shall see in the ‘Results’ section. The form of a JA file that corresponds to that of the AD file seen above is the following:

\begin{center}
\includegraphics[scale=0.25]{project_figures/fig8_3}
\end{center}

\quad As can be seen above, the table now looks very similar to how a corresponding ‘.csv’ file would also look, which lends itself to simplicity in reading the data into Python scripts and using it to train a model. Note that the dimensions are (21809, 66), with ‘21809’ corresponding to ~363 seconds worth of data over 66 dimensions (i.e. 3 dimensions of 22 joint angles).\\

\quad These joint angles files are also contained within what we call a ‘data cube’ (or ‘DC’ for short). This single ‘.mat’ file contains the joint angle data for 25 subjects and a table that contains information about them, as seen below:

\begin{center}
\includegraphics[scale=0.25]{project_figures/fig8_4}
\end{center}

\quad The image above shows how the data cube contains cells which each contains a whole joint angle file’s worth of data within them (that look similar to that seen above), while the table below shows the table contained in the data cube that contains useful information (metadata) about the respective joint angle files. Hence, due to the useful structure and provided table, when we look to train an RNN model on raw joint angle data, we use the data cube as standard for the first several experiment sets (though we later extract the joint angle data from ‘AD’ files as is done for other raw measurements in later experiment sets and model predictions sets).

\begin{center}
\includegraphics[scale=0.25]{project_figures/fig8_5}
\end{center}

\quad Finally, an important distinction to make is what sort of real-world activity each ‘.mat’ file actually shows. The first type is ‘6minwalk’, which as the name suggests is 6 minutes’ worth of data (so approximately 21600 frames) of the subject walking around a given area more-or-less continuously. This is provided to us as ‘AD’, ‘JA’ and ‘DC’ files, so for a given subject we can chose how we wish to interpret their data. The second type we are currently concerned with is ‘NSAA’, which are files usually between a length of 3 and 10 minutes that contain the subject carrying out the 17 activities that are set as part of the North Star Ambulatory Assessment. These files, however, are only provided to us as ‘AD’ files and do not come in ‘JA’ or ‘DC’ form (though we can still extract the raw joint angle measurements from an NSAA file via the ‘ext\_raw\_measurements.py’ script; more on that in the ‘Script Ecosystem Overview’ chapter). The final type is the natural movement behaviour data set provided to us in the ‘allmatfiles’ directory (which contains the joint angle data for the natural movement behaviour data of the subjects) and in the ‘NMB’ (which contains all the measurements from the natural movement behaviour files and which we received late in the project, hence why we used ‘allmatfiles’ for a long time). These directory contains numerous files from the same subjects as outlined previously performing various natural movement behaviours, such as sitting and talking, eating lunch, playing, and so on; while this isn’t used in most of the experiment sets to begin with, we do use this later in some of the model predictions sets.\\

\quad With the forms that the data can take now summarized, we can move onto what we actually do with this data prior to it being used to train an RNN.


%%%%%%%%%%%%%%%%%%
\section{The Data Pipeline}

\quad What is referred to as the ‘data pipeline’ is shorthand for four Python scripts (‘comp\_stat\_vals.py’, ‘mat\_act\_div.py’, ‘ext\_raw\_measures.py’, and ‘ft\_sel\_red.py’) that read from data files, manipulate data, and write to new files in the form of ‘intermediate data’. The aim of the pipeline is to convert the data that is specified by the user of the script (via arguments passed to the Python scripts) into a format that is ultimately usable by the RNN model. The specifics of what each script does are not covered here for the sake of brevity (see ‘Script Ecosystem Overview’ for this), so we instead focus on the different shapes and forms the data goes through depending on whether it came in as an ‘AD’ or ‘JA’/’DC’ file (as the ‘DC’ simply contains multiple ‘JA’ files, we treat both ‘DC’ and ‘JA’ files the same way). Refer back to the subsection ‘The Local Directory: Data Sets’ within the ‘Project and Local Directories’ chapter for information about the source and destination folders used for the variations outlined below.

%%%%%%%%%
\subsection{Variation 1: Joint angles from ‘JA’ and ‘DC’}

\quad Below, we can see the pipeline with respect to ‘JA’ or ‘DC’ file(s) as input to the RNN.

\begin{center}
\includegraphics[scale=0.2]{project_figures/fig8_6}
\end{center}

\quad This is what is done when we are dealing with raw joint angle data. Let’s say we wish to feed in the ‘JA’ file for subject ‘D4’, as seen in the diagram above. The data comes in the form of a 22K x 66 matrix, as can be seen in a previous image, which is loaded by the ‘comp\_stat\_vals.py’ Python script and simply written back out in exactly the same way to a ‘.csv’ format. In essence, we are removing some of the metadata from the ‘.mat’ file and then transforming the data values into ‘.csv’ format. The reason we do this is because the ‘RNN’ is expecting to load data from a ‘.csv’ format when dealing with all files regardless of its setup, so this essentially standardises the process so the RNN works in the same way for each different original data file type. An important aspect to note, though, is that this is also what is done with the ‘JA’ or ‘DC’ files: in the case of ‘AD’ files, to get the raw measurements we instead use ‘ext\_raw\_measures.py’.\\

\quad In the ‘JA’/’DC’ case, this data is then loaded by the RNN from ‘JA\_D4.csv’ and has sequences made out of it. In this context, we refer to a sequence as a 2-dimensional grouping of data that the RNN will treat as a single sample of data. In the above case, the sequence is a (60, 66) matrix of values: each row of 66 values are fed into the RNN one after another, repeating for a total of 60 times before the output of the RNN is observed; the RNN is essentially ‘reset’ before the next matrix of values is considered, hence the sequences are essentially independent of each other. In this case, the data is time dependent of each other only in 1 second intervals (given a 60 Hz sampling rate of the body suit), while outside of these sequences the data is treated independently of each other (in the same way as each image being classified by a convolutional neural network is independent of each other).

%%%%%%%%%
\subsection{Variation 2: Computed statistical values from ‘AD’ files}

\quad Next, we can see below the pipeline with respect to ‘AD’ files(s) as input to the RNN:

\begin{center}
\includegraphics[scale=0.2]{project_figures/fig8_7}
\end{center}

\quad The first point to notice is that the data starts with far more dimensions than ‘JA’ or ‘DC’ files. This is because the ‘AD’ files consider ALL the measurements and not just one measurement (the joint angles). With using them as input to ‘comp\_stat\_vals.py’, we process them differently; namely, we don’t just write them directly to a .csv, but rather calculate statistical values. These operations (which including finding the mean, variance, first/second eigenvalues of covariance matrices, fast Fourier transform values, mean sum absolute values, and so on) operate on only a limited number of rows of the data at a time. For example, in the above case, we select a given number of rows of the 22K frames at a time (in this case, 60 to correspond to 1 second’s worth of data), calculate statistical features over every single one of its 620 dimensions (along with between some of the 620 dimensions), and compute this as a row of approximately 4000 statistical values. This is then repeated for every other 60-frame part of the original 22K to produce a total of approximately 360 rows, each containing 4000 statistical values.\\

\quad Much like the sequence length that the RNN processes, the length of time to compute the statistical features over is a parameter that is set as an argument to the script (i.e. a hyperparameter that we set ourselves); hence, the size of ‘60’ is a value that has been determined to produce enough data while calculating over enough rows. A thing to bear in mind, though, is that if we increase this parameter so the statistical values are calculated over a longer time period, we will reduce the number of rows that are outputted from this script, hence reducing the amount of data available at the next stage of the pipeline (i.e. the data going into the feature reduction script outlined below).\\

\quad With this data having its statistical values computed and outputted to a new file (in this case of the above image, ‘AD\_D11\_stats\_values.csv’), we then load this .csv into a new Python script called ‘ft\_sel\_red()’. The purpose of this is to simply reduce the dimensionality of the data while preserving as much useful information as possible and to then rewrite this to a new file. While there are several options in feature reduction and feature selection (including principal component analysis, random forest feature selection, feature agglomeration, among others), we use PCA as standard based on its prevalence in other studies. Hence, when run on the input data and with a target reduction size set to 30 dimensions (again, this is a hyperparameter set as a script argument and is subject to experimentation in experiment set 7), the data is transformed from a dimensionality of (360, 4000) to (360, 30), which is far more conducive to being used as training data for the RNN in the next stage, as this reshaping helps minimize the effects of the ‘curse of dimensionality’.\\

\quad Finally, this is fed into the RNN and sliced up into sequences of length that we defined in the same way as in ‘JA’/’DC’ as previously described. A choice of ‘10’ is used here as the sequence length due to the limited size of the data (360, 30) in comparison to the case of reading from raw joint angle files (22000, 60) Also, it’s worth noting that even though the sequence length is only ‘10’ here, as each of the rows here have a full 60 rows worth of data from the original ‘AD’ file (which corresponds to 1 second’s worth of data embedded within them), each sequence from an ‘AD’ file to the RNN has 10 seconds worth of data encoded into it as statistical values. In comparison, the raw joint angle data has a sequence length of 60 and encodes only 1 second’s worth of data. This evidently plays a role in the difference in their respective results, which we shall discuss shortly.

%%%%%%%%%
\subsection{Variation 3: Extracted raw measurements from ‘AD’ files}

\quad Below we can see the pipeline with respect to using ‘AD’ files as a source of raw measurements:

\begin{center}
\includegraphics[scale=0.35]{project_figures/fig8_8}
\end{center}

\quad Unlike the previous two variations of the data pipeline, here we don’t make use of either ‘comp\_stat\_vals.py’ or ‘ft\_sel\_red.py’. This is because in this setup we neither compute statistical values, nor extract exclusively joint angles, nor do we need to do any dimensionality reduction. Rather, we wish to extract any raw measurements that we need from a given file. This is done by the ‘comp\_stat\_vals.py’ script: it takes the name of a file to extract the raw measurements from (or multiple names to carry out this process on) and the names of the raw measurements we wish to extract. From here, the script will seek out the columns of relevance within the tree structure of the corresponding source ‘.mat’ file for the given subject(s), as each vector of a certain raw measurement at each time instance is stored in its own column of the ‘tree.subject.frames.frame’ table which, when expanded, becomes a matrix of values for the raw measurement. This matrix of values (in the case of the ‘position’ measurement having a rough shape of (22000, 69)) is then written to a new ‘.csv’ file with a name reflecting the subject name the measurements are from along with the measurement name (e.g. ‘D4\_NSAA\_position.csv’).\\

\quad From here, the ‘rnn.py’ script is able to build models from these other measurement types in the same way it has done so for the joint angle data and the computed statistical values from the first two variations of the data pipeline by providing a different raw measurement name (e.g. ‘sensorMagneticField’ or ‘acceleration’) as opposed to just either ‘AD’ for computed statistical values (note here that ‘AD’ references to the computed statistical values; see the Glossary for further clarification about the two ways in which ‘AD’ is referenced) or ‘jointAngle’. After this, the following steps are identical, from the creating of sequences to the training and testing of models. It should also be noted that, like the ‘JA’ and ‘DC’ files, the extracted raw measurement values from ‘AD’ each represent 1/60th of a second’s worth of data; hence, when they are used to create sequences of 60 rows of raw measurement data, this represents 1 second’s worth of data in the same way it does for the ‘JA’/’DC’ files even though it comes from the ‘AD’ files: it’s only when computed statistical values are outputted that the data originating from ‘AD’ files that it comes to represent 1 second for each row rather than only 1/60th of a second.



%%%%%%%%%%%%%%%%%%
\section{Output Types}

\quad Now that the data has been prepared as a series of sequences of data either from a single ‘AD’, ‘JA’, or ‘DC’ file, or multiple of each, we now look at what the recurrent neural network models we have built actually do. It’s important to note that each of the variations works with every type of input file, be they from raw joint angle files or ‘AD’ files capturing either 6-minute walk or NSAA data; that is, after all, a primary purpose of using the pipeline. It’s also necessary to note that the trained model in every variation operates on a sequence-by-sequence basis even for testing; that is to say, it doesn’t classify or provide a regression score for a complete file but rather for a sequence of a pre-specified length within said file. The classifications or regression values of each of the sequences of the file being assessed can then be aggregated together to provide a classification or regression score for the whole file (how this is exactly carried out is subject to further research). Finally, it’s worth noting that each variant is implemented within one Python script called ‘rnn.py’, and the necessary architectural differences are setup based on arguments passed into the script.\\

\quad The three main variants (or ‘output types’) that we have developed are described as follows:

\begin{itemize}
	\item \textit{'dhc' - Classification of sequences of being from ‘DMD’ or ‘HC’ subjects}: The purpose of this variation is to classify sequences as being from a file that is from either a ‘DMD’ or ‘HC’ subject. Consider the previously outlined case where ‘FR\_AD\_D11\_stats\_values.csv’ is fed in from the pipeline to ‘rnn.py’ at the end. When it is loaded into the ‘RNN’ script, it’s divided into ‘36’ sequences of length ‘10’ to give a data shape of (36, 10, 30): this is the \textit{x} data in the sense of a neural network. For the \textit{y} data, we simply look at the title of the file this data originated from to provide a label of either ‘1’ or ‘0’ depending on the nature of the file name: since it’s a ‘D’ file due to it being about patient ‘D11’ it gets a label of 1. This is then repeated for each sequence to obtain a list of ‘1’s of length 36. We then repeat this process for all other files pushed through the data pipeline, some of which will be from ‘D’ subjects and others from ‘HC’ subjects. The result, in the case of doing this over all ‘AD’ files for files corresponding to NSAA subject assessments, is an input shape of (742, 10, 30) for ‘x’ and (742, ) for ‘y’, which contains a mixture of 1’s and 0’s for each sequence. There is also only a single neuron output for the network that contains categorical value of either 0 or 1 for a given sequence: this output will go to 0 if the sequence inputted is predicted to be from a ‘HC’ subject or 1 if predicted to be from a ‘D’ subject.
	\item \textit{'overall' - Overall NSAA regression score}: Here, the RNN is tasked with taking a sequence and trying to predict the overall NSAA score of the subject that it comes from. This score corresponds to the accumulation of the scores of the 17 individual activities done in the subject’s assessment. As each activity is scored either a 0, 1 or 2 (with 2 being a perfect score), the overall NSAA score will range from a 0 for a subject with severe DMD and a 34 for a subject that shows no symptoms (i.e. a ‘HC’ subject). Using the above example, we start with a shape of (742, 10, 30) over all the NSAA AD input files. For each of these 742 sequences, we then check a table that contains the subject information (‘nsaa\_6mw\_info.xlsx’, as described in the previous chapter): this table provides a list of the subjects, their testing details, and crucially their individual NSAA scores (17 of these between 0 and 2) and overall NSAA score (1 of these between 0 and 34). This overall score is then used for every sequence from a given file that is inputted to the ‘RNN’. Using this, we then obtain 742 values between 0 and 34, with each value being the overall NSAA score of the source file of its corresponding \textit{x} component of shape (10, 30). From here, we again have in the RNN architecture a single output neuron, but this time it outputs a regression value (rather than a classification value as in the previous case) between 0 and 34; hence, each sequence that passes through the RNN will result in it making an estimate of the overall NSAA score of the subject that the sequence originates from.
	\item \textit{'acts' - Classification of NSAA single activity scores for all 17 actions}: In a similar vein to predicting the overall NSAA scores, the RNN is also able to train towards predicting individual activity scores; that is to say, given a single sequence, the RNN will output an array of 17 values, each being either a 0, 1, or 2, that corresponds to its prediction of the individual activity scores of the subject that the sequence originates from. Again, given the same \textit{x} data fed through the pipeline, the corresponding \textit{y} values to train and test on are obtained from the ‘nsaa\_6mw\_info.xlsx’ table to obtain the necessary array of 17 values for each sequence. Hence the data fed into the RNN now has a shape (in the case of all NSAA AD files being used) of (742, 10, 30) for ‘x’ and (742, 17) for ‘y’. To account for this, the RNN is modified to predict 17 individual classification labels of either 0, 1, or 2 for 17 output neurons.
\end{itemize}

\quad Currently, all 3 variants have many of the same hyperparameters, including the train-test ratio of data (0.2; excluding the ‘--no\_testset’ optional argument being set which sets it to 0), the number of units in each LSTM cell (128), the number of hidden layers (2), and the learning ratio of the ‘adam’ optimisation algorithm (0.001). The reasoning behind this was that differences in experiment results will hopefully be down to differences in source data file types (e.g. raw joint angle files of 6-minute walks vs ‘AD’ files of 6-minute walks) rather than potentially different architectures (e.g. if one had more hidden layers, increased performance may be resulting from this rather than the source of the data going into the RNN, which is what we want to be able to compare). There are some differences though: a policy decided upon early was that the RNN should train until its loss more-or-less converges. This required different numbers of epochs dependent on the source data type. For example, if the data going into the RNN came from raw joint angles, it only needed approximately 20 training epochs to converge, whereas data from ‘AD’ computed statistical value files needed between 200-300 to converge. Finally, it’s also worth pointing out the main difference in training for classification (either for single or multiple output nodes) and regression was the different loss functions used, in that classification used binary cross entropy and regression used mean-squared error.\\

\quad It should also be noted that there is one more output type that is slightly unlike the others and that is the ‘indiv’ output type. This setups the models to only have one output neuron that computes a score of between 0 and 2, much like the ‘overall’ output type (though models trained for the ‘overall’ output type can range from between 0 and 34 for an output value). However, for the ‘indiv’ output type, we are only concerned with ‘single-act’ input file data, i.e. files that contain data of a certain raw measurement (or computed statistical values) for a single activity for a certain subject that are produced by the ‘mat\_act\_div.py’ script. Hence, the aim of this output type is to compute, for a given subject’s single-act file that has a corresponding true value associated with it (that is contained within the ‘nsaa\_6mw\_info.xlsx’ table), the predicted value between 0 and 2. Note that this is a regression task, and so the model can predict anywhere between 0 and 2, with the final prediction made by ‘model\_predictor.py’ for a given subject based on the average of these predictions for a sequence and then rounding this average to the closest integer to get the predicted value for that complete subject. We should also note that the reason we don’t consider this output type with the others is that it operates on a different file type: while the other three output types can be obtained for any input file, the 'indiv’ output type can only be done for single-act files. This makes intuitive sense, as it’s somewhat pointless to ask a model to predict a single act score corresponding to a specific activity for a sequence from a subject which could have come from anywhere within the source file (i.e. any one of the 17 activities or the ‘in-between’ activities data).


%%%%%%%%%%%%%%%%%%
\section{Additional Data Preprocessing Work and Tools Used}

\quad One of the disadvantages of the NSAA files provided is that all the activities for each subject are provided in the same ‘.mat’ file and the table that provides information of these subjects’ trials with the body suit (‘nsaa\_6mw\_info.xlsx’) contain information about the overall cumulative NSAA score as well as the individual activity scores (among other meta data), but not the specific times of occurrence of each activity. These activity times are needed in order to ‘divide up’ the original ‘AD’ files into ‘AD’ files that contain only the timeframe of a specific activity via the ‘mat\_act\_div.py’ script, which would be useful to have for this project as well as other research initiative projects. Hence, we were tasked as a group to work together to create a table of each subject’s predicted times for each activity. A portion of the results, contained in a shared Google sheet called ‘DMD Start/End Frames’, can be seen below:

\begin{center}
\includegraphics[scale=0.25]{project_figures/fig8_9}
\end{center}

\quad While we have touched on the above file within the ‘Google Annotations Sheet’ section of the ‘Reference Documents’ Chapter, it’s worth at this point clarifying a few things about how it was put together. The above image is only a portion of the table, and there are many more columns that aren’t displayed, but the above can be interpreted fairly simply. For example, for the ‘D11’ subject’s NSAA ‘AD’ file, the file is observed via the ‘dis\_3d\_pos.py’ script (elaborated upon in the ‘Script Ecosystem Overview’ chapter) to show the subject do the ‘standing’ activity between frames 300 and 1380 (corresponding to between 5s to 23s) and the ‘stand from chair’ activity between frames 3300 and 4920. There were several rules that were followed in order to extract these frame times by human observation:

\begin{itemize}
	\item As activities were often repeated by the subject, we consider only the first \underline{completed} activity as the start and end points in the sheet; for example, if the subject tried to do the activity ‘hop on right food’ several times without success before being able to do so, we only count the last successful attempt in the table.
	\item Ideally, we try to give a small amount of ‘leniency’ on the start/end times of the activity; the idea behind this is that no part of the activity is therefore ‘missed’ when it’s divided up into smaller 'AD' files.
	\item Some of the activities were either seemingly not performed or performed alongside another activity, or performed very subtly; for example, the ‘nod head activity’ was particularly tricky to detect in many subjects. In these cases, a best guess on the time of occurrence of the activity was made; the checking of how we divided up these files is further continuation work that can be done for the project and is covered later.
\end{itemize}

\quad So given that we have the ‘rules of thumb’ for our annotation work, it was next necessary to actually ‘visualize’ these ‘AD’ files. As there was no easy way of ‘running’ one of these ‘AD’ files in ‘.mat’ format and as we (at that point) had no access to the ‘.mov’ video files that corresponded to each source ‘.mat’ file, the project necessitated a script that could use the thousands of ‘position’ values of the AD file (‘position’ being one of the measurements in an ‘AD’ file along with ‘jointAngle’, among others) to feed into a function that animated a stick figure as it moved around three-dimensional space. This was the work of the ‘dis\_3d\_pos.py’ Python script and, when the script is run with the required arguments, e.g. for the ‘D11’ subject , the script does the following:

\begin{enumerate}
	\item Loads the position values from the ‘AD’ file for subject ‘D11’ into a massive array.
	\item Group each of the columns corresponding to \textit{x}, \textit{y}, and \textit{z} dimensions of all segments together.
	\item Send these to the ‘animate’ function that animated it at a rate of 60Hz (so the animation would appear as real-time) and closed upon completing the plotting of all values for that file.
\end{enumerate}

\quad The result of this was 3D data in a new window that looked like the following:

\begin{center}
\includegraphics[scale=0.4]{project_figures/fig8_10}
\end{center}

\quad Additionally, the current running time of the animation function was also printing to the console; this enabled us, by observing the movements of the stick figure and what activities it seemed to perform, to get a reasonably accurate idea of the times of occurrence of each activity.\\

\quad With the Google annotations sheet now being complete with the help of the above plotting script, we could now use the sheet as a reference tool for a script that could ‘divide up’ a given source ‘AD’ file; namely, the ‘mat\_act\_div.py’ script. The rough functionality of the script can be summarised in the diagram below:

\begin{center}
\includegraphics[scale=0.23]{project_figures/fig8_11}
\end{center}

\quad When the ‘mat\_act\_div.py’ script is then run with an argument given to be the name(s) of the file(s) to ‘divide up’ based on the annotated Google sheet, it does two things:

\begin{enumerate}
	\item Reads in the Google sheet that we had previously annotated, locates the relevant row given the provided arguments, and extracts a list of the activity times (both start and end times) as pairs of integers and the names of the files that contains the activities.
	\item Loads in the relevant ‘AD’ file for the given argument(s) and, using the pairs of integers in the above list, slices up the file into 17 non-overlapping parts and writes these to a subdirectory of the file(s)’s source data directory with names specific to the activity and subject source file (see the names in the bottom-right image above).
\end{enumerate}

\quad While these divided-up ‘AD’ files are not used to tune the models for the general cases of using the system, these are used instead in later model prediction sets to train RNN models to predict single-activity NSAA scores and evaluate the significance of particular activities with respect to predicting overall subject assessment.






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Script Ecosystem Overview}

%%%%%%%%%%%%%%%%%%
\section{Overview and Script Diagram}

\quad In this chapter, we shall be covering all of the scripts that are used as part of the system. By ‘system’ here, we mean the complete set of scripts and supporting documents that are used to carry out all the experiment sets and model predictions sets in an effort to realise the aims of the project, and that encompass the primary body of work done for the project. Each section covers one Python script in turn (with a small section covering all batch scripts at the end), and can each be divided into two parts. The first, ‘Overview’, describes why we felt it necessary to add the script to the system, what problem it’s supposed to solve, and it’s broad operations. The second, ‘How it works’, describes the script generally as either a sequence of steps or describes its different functions in more detail, depending on which is deemed more appropriate. In conjunction with extensive commenting found in each script, the hope is that by using this chapter, any user could understand in detail the purpose and behaviour of each script of the system.\\

\quad Below, we can see a diagram of all the scripts involved in the system for the project. It covers all the inputs that are needed for the project, how they are processed by the various scripts, and what types of outputs are produced by the system. Note that this is only a vague overview, without any details of how the scripts do work (this is discussed further below) nor details of the names and locations of the source files (which is covered extensively in ‘The Local Directory’ section of the ‘Project and Local Directories’ chapter), with the aim more to show the order things should be run in either by the user or by the batch scripts (not included below) and how the outputs relate to each other. Additionally, it is hoped that it will serve as a useful reference point to understanding the complete system and how each script fits into it.

\begin{center}
\includegraphics[scale=0.8]{project_figures/fig9_1}
\end{center}

%%%%%%%%%%%%%%%%%%
\section{'comp\_stat\_vals.py'}

%%%%%%%%%
\subsection{Overview}

\quad In an effort to experiment with both raw measurement values used in RNN models and also pre-computed features, we needed a separate script that takes in the body of raw measurements from source ‘.mat’ files and computes various statistical values taken over the entire file and the majority of useful measurements. This is the primary purpose of this script: to take in a source ‘.mat’ file of suit data for one or more subjects and, for each of them, produce an output ‘.csv’ file containing rows of statistical value data of the source ‘.mat’ file.\\

\quad Each of the statistical analyses that are used are implemented by a distinct function that performs a bit of syntactical help (e.g. the function to calculate mean includes type changing and rounding of numbers). The statistical features that are computed include: ‘mean’, ‘variance’, ‘mean absolute diff values’, ‘FFT’, ‘covariance components between axes’, ‘mean sum of values across axes’, among others. The bulk of these features have been extracted 'intra-columns'. This is meant by the following: consider a 'JA' (joint angle) file; it's columns correspond to only 1 'measurement', the 'feature' itself ('feature' in this context meaning one of the 17 sensor labels, 22 joint labels, or 23 segment labels, the choice of which depends on which measurement we are referring to), and the 'dimension' of this feature (as we are dealing with 3D position data, this is 3D with each representing the \textit{x}, \textit{y} or \textit{z} dimension). Many of the statistical features are thus computed on the values within each individual column; for example, the mean for a specific column is computed by averaging all the values for a specific measurement's specific feature's specific dimension (e.g. measurement 'joint angle's feature 'jRightWrist's dimension \textit{x}-dimension'), of which there are ~22k total values for this corresponding measurement/feature/dimension for the ~22k frames (or rows) in a file. However, there are also several statistical functions that are applied one-layer up; that is, rather than calculating over a single column representing a single dimension of a feature of a measurement, it calculates over 3 adjacent problems for ALL dimensions of a feature of a measurement. These mainly include operations that calculate features over a 2-dimensional array of data.\\

\quad The final variation of running statistical functions are those that operate on single columns are then reapplied to the calculate the same statistical function over all newly-calculated values. For example, if we are concerned with the variance values for the 'position' measurement over 23 feature names over the \textit{x}-dimension, then we take the variance of these calculated values to form a new value representing the 'position' measurement, the '(over all features)' feature, and the \textit{x}-dimension. This process is repeated with statistical functions that operate over the other 2 axis dimensions.\\

\quad The statistical features that are calculated per column (i.e. over a single axis) include:

\begin{itemize}
	\item Mean
	\item Variance
	\item Absolute mean sample difference
	\item Fast Fourier transform's (1-dimension) largest value
\end{itemize}

\quad The statistical features that are calculated per set of 3 columns (i.e. over all 3 axes of a feature for a given measurement) include:
\begin{itemize}
	\item Mean sum of the values of each dimension.
	\item Mean sum of the absolute values of each dimension.
	\item First eigenvalue of the covariance matrix of the 3 columns.
	\item Second eigenvalue of the covariance matrix of the 3 columns.
	\item \textit{x}- to \textit{y}-axis covariance (i.e. row 1 col 2 value of the 3x3 covariance matrix).
	\item \textit{x}- to \textit{z}-axis covariance (i.e. row 1 col 3 value of the 3x3 covariance matrix).
	\item \textit{y}- to \textit{z}-axis covariance (i.e. row 2 col 3 value of the 3x3 covariance matrix).
	\item Fast Fourier transform (2-dimension) largest 3 values (as 3 separate calculations).
	\item Proportion of samples outside the mean zone in every dimension.
\end{itemize}

\quad Each of these calculations done for a specific measurement, specific feature, and a single dimension are written as a single value as part of a row with the column title:
\begin{center}
\textit{($<$measurement name$>$) : ($<$feature name$>$) : ($<$axis$>$-axis) : (statistical function)}\\
\end{center}

...while, when it is subsequently called to repeat the process over all feature names, the column has the title:
\begin{center}
\textit{($<$measurement name$>$) : (over all features) : ($<$axis$>$-axis) : (statistical function)}\\
\end{center}

\quad For the calculations done for a specific measurement, specific feature, and over all 3 dimensions, they are again written as a single value as part of a row with the column title:
\begin{center}
\textit{($<$measurement name$>$) : ($<$feature name$>$) : ((\textit{x},\textit{y},\textit{z})-axis) : (statistical function)}\\
\end{center}

...while, when it is subsequently called to repeat the process over all feature names, the column has the title:
\begin{center}
\textit{($<$measurement name$>$) : (over all features) : ((\textit{x},\textit{y},\textit{z})-axis) : (statistical function)}\\
\end{center}

\quad The result is then a single row of all of these values for a whole file with \textit{n} columns in the row, with each column corresponding to an above label with associated value computed over the whole file. As we may have many measurements over which to calculate (e.g. 'position', 'velocity', 'angular acceleration', etc.), many features (e.g. 23, 22, or 17 depending on the measurement), 3 dimensions (or 1 dependent on which statistical function we are using), and ~15 statistical functions to compute, a single row for an 'AD' (all data) file can be several thousand columns long. Note that for a ‘JA’ file this is significantly less as we are only concerned with 1 measurement (the 'jointAngle' measurement as this is the only one in the file). Again, it’s important to note that these values are calculated across each of the samples (e.g. 22k) for each of the single columns or collection of 3 columns (depending on the statistical function in question).

%%%%%
\subsubsection{The split files functionality}

\quad An obvious problem from the method described above is that, while we might have plenty of statistical information computed over the single file, it’s only contained within a single row. To work around this, we added in the ‘--split\_size’ functionality. This essentially divides up the files into sections along time (i.e. a certain number of rows) before computing the statistical values over each of these sections rather than the whole file. For example, let’s say that we originally have 22000 rows of a data in a source ‘.mat’ file for a subject. If we provide ‘--split\_size=1’ to the ‘comp\_stat\_vals.py’ script, we interpret this as ‘compute the statistical values over 1 second increments’. This involves dividing up the file into sections of 60 rows (as 60 rows of data correspond to 1 second’s worth of data due to the sampling rate of the suit being 60Hz), followed by computing the statistical values over each of these blocks of rows, and finally vertically concatenating these lines to produce the output as a ‘.csv’.\\

\quad Thus, rather than having an output of shape (1, ~4000), we instead have an output of shape (366, ~4000); the downside of course is that each of these rows now contain computed statistical values calculated only over blocks of 60 rows, as opposed to the whole file. However, it was felt necessary to undertake this process in order to produce a somewhat comparable amount of data to be used alongside raw measurements. It should also be noted that a split size of ‘1’ isn’t set in stone, and is something we shall be experimenting with in later experimentation.

%%%%%%%%%
\subsection{How it works}

\quad The basic operation of the 'comp\_stat\_vals.py' script can be summarized as follows:

\begin{enumerate}
	\item Read in a certain .mat file (either a ‘JA’, ‘AD’, or ‘DC’ file) into Python as an object of either the ‘JointAngleFile’, ‘AllDataFile’, or ‘DataCubeFile’ class. Alternatively, if provided with the ‘all’ name in place of a file’s name, complete this process over all available files in the specified data set.
	\item Apply statistical analysis on each file’s various measurements, features of measurements, and dimension of the features; alternatively, if the ‘--split\_size’ functionality is set, divides the file into sections before computing the statistical values.  Note that this is optional if a joint angle is selected and called with the 'write\_direct\_csv' method, which just translates a joint angle .mat file to .csv format.
	\item Write the computed statistical value  to a .csv file with a name corresponding to the read in file; the aim with this is for it to be an easy-to-digest format for the next stage in the analytics pipeline (e.g. the ‘ft\_sel\_red.py’ script to reduce the dimensionality of the computed statistical value files).
\end{enumerate}



%%%%%%%%%%%%%%%%%%
\section{'ft\_sel\_red.py'}

%%%%%%%%%
\subsection{Overview}

\quad One of the consequences of using the 'comp\_stat\_vals.py' script is that the number of features produced as columns of data for a single subject's ‘AD’ file balloons several fold: for a single subject with ~620 columns (with each being one feature, of ~56,  of one measurement, of ~11) and ~22K rows (360s at 60Hz suit sampling rate), this then becomes ~360 rows (given '--split\_size'=1, i.e. 1 row for every 60 source rows) of approximately 4000 columns. Hence our data shape has been transformed from (22000, 620) to (360,4000) for a single file. This is completely impractical to use as training data for a given model for several reasons:

\begin{enumerate}
	\item The curse of dimensionality means that the models struggle to train at all when dimensionality is this large for the amount of data samples ('360') that we have available.
	\item Many of these computed statistical features may hold not that much useful information in them, or at least less useful information compared to other useful statistical features.
	\item Even if we were to use all these features, it would take a much longer time to train models for most likely very little gain (with it most likely being worse off than smaller dimensioned data), making it even worse from a practical standpoint.
\end{enumerate}

\quad Hence, for the '\_stat\_features.csv' files that are created by the 'comp\_stat\_vals.py' script, its more-or-less necessary to reduce the dimensionality to something a lot smaller prior to using this as training data. Note that this isn't done for raw measurement data for three reasons:

\begin{enumerate}
	\item The dimensionality of these data files is already at a level that is feasible for training (ranging from 51 from sensor measurements to 69 for segment measurements).
	\item There are far more rows of data within each of these files; this is due to the fact that, with using 'comp\_stat\_vals.py' with '--split\_size'=1, we computed stat values over each block of 60 rows and hence reduce the number of actual 'numbers of data' (i.e. numbers that appear in our data set) by 60-fold. This 60-fold comparable increase in data when using raw measurements makes using this data of column size 51-69 a lot more feasible in training models.
	\item Even though we may be computing many redundant features in 'comp\_stat\_vals.py', we are much less likely to have features that are as redundant as these in the raw measurements data. This is because every feature corresponds to a single dimension for a sensor, angle, or segment, which is much more likely to hold important information that many of the computed statistical values, and thus there is more of a motivation to keep all of these.
\end{enumerate}


%%%%%%%%%
\subsection{How it works}

\quad Given a user-specified 'dir' for the directory that we wish to source the stat feature files from, the file type we're interested in (usually set to 'AD'), the 'fn' of the file(s) of which we wish to reduce the dimensions of (set to 'all' to do so over all files in 'dir'), and 'choice' (which is the feature selection/reduction technique to use), the following is undertaken by the script:

\begin{enumerate}
	\item For a given file name in 'dir', read in the file (e.g. 'AD\_D4\_stat\_features.csv') as a DataFrame and divide it into its \textit{x} and \textit{y} components.
	\item Normalize each dimension of the data if the relevant optional argument is set.
	\item Set the number of features to extract from the data if the relevant optional argument is set. As standard, we use '30', as this generally encompasses a vast amount of the variance inherent to each data file while also being a feasible data width for our RNN models.
	\item Based on the 'choice' argument given by the user, use a technique to reduce the dimensionality of the data. This can be done in an unsupervised feature dimensionality reduction manner (e.g. using principal component analysis or Gaussian random projection), unsupervised feature selection manner (e.g. variance thresholding or feature agglomeration), or in a supervised feature selection manner (e.g. by using a random forest for feature selection). 'PCA' has been used up until this point, though further experimentation with other feature selection/reduction techniques is a promising direction to take the project in.
	\item With the newly-reduced data, call the 'add\_nsaa\_scores()' function to add the overall and single-act NSAA scores to each of the rows of reduced-dimensionality data, which is necessary for getting the relevant \textit{y} labels by the 'rnn.py' script, which the output of this script feeds into. The information for these scores comes from the 'nsaa\_6mw\_info.xlsx' file, which contains the scores for every subject that has undertaken the NSAA assessment; hence, all that is required is to select the row in this .xlsx file that corresponds to the subject we are currently dealing with.
	\item The newly-reduced data, with the NSAA scores appended at the beginning of each row, is then written to the same directory as it was sourced, with the exception that an "FR\_" ("feature reduced") prefix is appended to each newly-written file name to differentiate it from the file from which it was sourced.
	\item Repeat this process for every other file name in 'dir' that is required which, if 'fn'=’all’, results in all files in 'dir' having their dimensions reduced.
\end{enumerate}




%%%%%%%%%%%%%%%%%%
\section{'mat\_act\_div.py'}

%%%%%%%%%
\subsection{Overview}

\quad Along with using the full data files of the suit as computed statistical values used in various models and with varying target outputs (e.g. ‘dhc’, ‘overall’, etc.), we also wish to extract the single activities of source ‘.mat’ files from the NSAA directory. As standard, each source ‘.mat’ file in the NSAA directory contains the suit data of one full assessment for a single subject (though on occasion this is divided into two files if the ‘walk or ‘run’ activities happened at a later point). This means that each file usually contains the subject performing all 17 activities within the same file which are separated in time, sometimes by only a second or two in the case of the 'climb/descend box' activities and sometimes by up to a minute in the case of the 'get off the floor' activities. Hence, it would be advantageous for us to extract the data of the individual activities from within each file in order to use them for training for several different model predictions sets that are explored later on.\\

\quad As the data is contained within a very large table and each row is a single time instance of data (collected at 60Hz from the suit, therefore each frame is 1/60th of a second's worth of suit data), to create new single-activity files, all we need to do is the following:

\begin{enumerate}
	\item Determine the start and end rows within the overall file of the activity in question (e.g. if we 	wished to extract the second activity data that we know starts at 13s and ends at 15s in the subject's assessment, we would need to extract rows 780 to 900 of the source ‘.mat’ file).
	\item Slice the relevant rows from the table and create a new '.mat'-friendly tree structure within the script.
	\item Write this data to an 'act\_files' subdirectory of the source directory as a new ‘.mat’ file with a file name reflecting which activity it represents.
\end{enumerate}

\quad From here, we can process these single-activity ‘.mat’ files in the same way as the standard ‘.mat’ files through the data pipeline, including the extracting of raw measurements, computing of statistical values, and training of RNN models.


%%%%%%%%%
\subsection{How it works}

\quad The key requirement for this script to work is the use of the relevant Google annotations sheet contained within the ‘documentation’ directory. This contains the manually assessed activity times of each subject, which was done by several members of the research initiative that analysed each of the videos that corresponds to each subject's ‘.mat’ files and observed roughly at what times these activities started and ended for each subject. Note that these aren't going to be perfect, which is one flaw of using this sheet, as we can't give the exact start and end times of each activity and so tend to overestimate the amount of time the activity takes (i.e. note down a start time that's most likely before the real time and an end time that's most likely after the true end time) so as to ensure the complete capture of the activity. Also, this process is not immune to human error, and therefore it's not impossible to misinterpret what constitutes a 'complete' activity, which will impact how much use these 'single\_act' files are for us when we come to use them later.\\

\quad This Google annotations sheet can either be found within the ‘documentation’ directory. From here, once this is read in by the script, there are two functions that are executed:

\begin{itemize}
	\item \underline{\textit{'extract\_act\_times()'}}: As the name suggests, this function analyses the Google sheet and creates two lists: the first list, 'act\_times', is a list of start and end times (in suit frames, i.e. seconds x 60) in a nested structure (e.g. if there are 10 subjects, each performing the 17 activities, and each have a start and end time, then 'act\_times' has a shape (10, 17, 2)); the second list, 'ids', contain a list of subject names (e.g. 'D4'), each entry of which corresponds to an entry in 'act\_times'.
	\item \underline{\textit{'divide\_mat\_file()'}}: This function then takes the above two lists and, depending on what 'fn' argument the user has selected for the subject (‘all’ if one wishes to divide up all the subjects in the ‘NSAA’ directory), the relevant row within the 'act\_times' list is retrieved. From here, for each activity the subject has completed, each activity ‘pair’ (i.e. two numbers that are the start and end times in the table for each activity) is retrieved along with the name of the file that contains that activity for the subject (generally the same for all activities for a given subject, though there are exceptions). The complete ‘.mat’ file is then then loaded, the table of data within the ‘.mat’ file is extracted, and the table is sliced for that activity pair. These rows then 'replace' the rows of the 'whole' ‘.mat’ file and the ‘.mat’ file is then rewritten to a different file with a name reflecting the activity it is currently concerned with in the 'for' loop. This then repeats for each of the 17 activities for the given 'fn' subject(s).
\end{itemize}



%%%%%%%%%%%%%%%%%%
\section{'ext\_raw\_measures.py'}

%%%%%%%%%
\subsection{Overview}

\quad While the extraction of computed statistical values is an important tool for the data pipeline as an input to the ‘rnn.py’ script, it's also necessary to be able to use different types of raw measurement values; in other words, the values that are recorded by the sensors of the body suit and are within the corresponding source ‘.mat’ files. For a given subject's suit data, each measurement (e.g. 'position', 'jointAngle', 'sensorMagneticField', etc.) is inserted into the .mat file's table of values as a column, with the height of the column equal to the number of frames that were taken of the subject (corresponding to the length of time the suit was recording x 60 samples per second). Within this single column, there are vectors of either 51, 66, or 69 values (depending on whether the suit was recording raw sensor values, anglular values, or segment values, respectively).\\

\quad The idea of this script is fairly simple. For a given subject name in a directory (or all the subject names found in that directory) and for a given measurement (or all raw measurements available), the relevant source ‘.mat’ file is opened, and the relevant column is expanded for the given measurement name so that it becomes a matrix of single values rather than a column of vectors (with a matrix of shape (\# of frames, \# of vector values)). This matrix of data is then to be written to a separate ‘.csv’ file within a directory that reflects the source directory 'dir' and the measurement name that the matrix contains.\\

\quad From here, we can then use this data to train an RNN on these raw measurement values with \textit{y}-labels (i.e. target values) that are determined by the classification of file this ‘.csv’ of data corresponds to (i.e. a 'D' or 'HC' subject), or the overall or single-act NSAA scores that correspond to the subject name of this .csv (e.g. 'D4') that can be found with 'nsaa\_6mw\_info.xlsx'. In doing this, we provide an alternative to the production of RNN-ready data by 'comp\_stat\_vals.py' and ‘ft\_sel\_red.py’ and are able to compare how manually extracted features differ in RNN performance versus raw data (where the RNN does its own feature extraction). This is explored further within the discussion of results.

%%%%%%%%%
\subsection{How it works}

\quad The script runs in a fairly simple way without the necessity of classes or functions and thus just goes through a sequence of steps, which are as follows:

\begin{enumerate}
	\item Takes in arguments for the data set directory from which to retrieve the file(s) for raw measurement(s) extraction and checks them for validity (e.g. makes sure 'dir' is one of the allowed types such as ‘NMB’ or ‘NSAA’).
	\item Retrieves the full file name(s) of the files within 'dir' from which we shall extract the measurements from. If 'fn'='all', retrieves all full file names in 'dir' as a list.
	\item Parse the list of measurements that we wish to extract based on the 'measurements' argument that are comma-separated. If 'measurements'=‘all’, then return a list of all extractable measurements available as a list.
	\item Creates a directory for each raw measurement within 'dir' to store these raw measurements extracted.
	\item For each file in 'dir', load the ‘.mat’ file, extract the table of values within its tree structure, removes any 'wrappers' around these values within the table and, for each measurement to extract, select the column from the ‘.mat’ table that corresponds to the measurement, expand it out as 'measure\_data', and write it to a '.csv' file that reflects the file name and measurement we are currently concerned with.
\end{enumerate}



%%%%%%%%%%%%%%%%%%
\section{'rnn.py'}

%%%%%%%%%
\subsection{Overview}

\quad As the central element of the system insofar as it encompasses the learning and prediction models that are relied upon to produce the results, the importance of this script should be self-evident as it contains the class that defines the RNN's architecture (the ‘RNN’ class), how it trains, predicts, and the instantiation and running of said class. Hence, rather than going through the motivation of writing this script or going through the basics of RNNs and their operation (which has been covered previously in the ‘Overview of Recurrent Neural Networks’ chapter), we instead shall highlight a few important points about the structure of the script that builds these models:

\begin{itemize}
	\item We chose to use LSTM units instead of traditional neurons mainly due to their ability to learn better and the fact that they don't suffer the vanishing or exploding gradient problems.
	\item Other hyperparameters within the RNN itself (number of layers, size of LSTM units, learning rate, etc.) are kept as a constant throughout the experiments. These were found based on prior 'best practices' through prior research projects undertaken by others as well as rudimentary tuning to find 'good enough' parameters. However, further experimentation to find optimal settings could still be looked into; see the chapter on ‘Further Improvements’ for a discussion on this.
	\item The final layer can be either a single node for classification, a single node for regression, or 17 total nodes for single-act classifications; hence, the building of the RNN model depends on the arguments passed to the script.
	\item The performance of the models that are built here are generally viewed by two means: the console output at the end of the running of the 'rnn.py' script (which provides the info we need to fill in the 'RNN Results.xlsx', provided ‘--no\_testset’ is not set) or the 'model\_predictor.py' script (which provides info for 'model\_predictions.csv'). See the later section within this chapter for more information of how 'model\_predictor.py’ works.
\end{itemize}


%%%%%%%%%
\subsection{How it works}

\quad The structure of the script is fairly complicated and slightly convoluted, with numerous conditional statements needed to handle various data processing edge cases and many possible optional argument combinations that sometimes interact with each other in strange ways that must be handled; hence rather that explaining the structure of the script in detail, it's instead worth going through how exactly the script works upon being instantiated from the command line with arguments. This should give the user the a good grasp of what's going on upon script instantiation:

\begin{enumerate}
	\item Reads in all required arguments (e.g. source directory, file name(s), output type, etc.) and optional arguments (e.g. sequence length, sequence overlap, leave out file choice, etc.) and checks each for validity.
	\item Preprocesses the data from the source directory and file name(s) chosen; this includes reading in all source '.csv' files, fetching the relevant \textit{y} labels for the \textit{x} data from the files, splitting the data into sequences, discarding a proportion of the sequences if necessary, splits into train/test components, etc.
	\item Builds the ‘rnn’ object (instantiated from the 'RNN' class) with the necessary feature length, sequence length, size of LSTM units, number of hidden layers, and so on.
	\item Train the RNN on the 'x\_train' and 'y\_train' components and tests on the 'x\_test and 'y\_test' components.
	\item Prints out the performance on the test set to the console.
	\item Write to a ‘.csv’ unique to this model the results of the predictions, the arguments used to run the script, and the results that were printed to the console output. See ‘The Local Directory: ‘rnn.py’ Outputs’ section in the ‘Project and Local Directories’ chapter for more information.
\end{enumerate}

\quad It's also worth touching on a few of the optional arguments. The required arguments should be self-explanatory and in no further need of elaboration. Note that there are several other significant optional arguments that can be set that are not covered below (e.g. ‘--seq\_len’, ‘--seq\_overlap', and ‘--discard\_prop’), though the descriptions of what these do can be found where they are used within either the experiment set or model predictions set which specifically utilizes it. However, some of these optional arguments aren’t covered in any further details in any experiment sets or model predictions sets and so are covered below:

\begin{itemize}
	\item \underline{\textit{'--write\_settings'}}: This gives the user the option to store the results of the RNN that are printed to the output to the 'RNN Results.xlsx' file, rather than the user having to manually copy-paste console results to the file in a new row. This is generally used when new experiments sets with different RNNs are being carried out to save time and minimize the chances of human error; however, we generally don’t set this when a model predictions set is being carried out, as we wish for the outputs to instead be written to ‘model\_predictions.csv’ by ‘model\_predictor.py’, and thus it serves no purpose to write the results on the test set of ‘rnn.py’ to ‘RNN Results.xlsx’ when we don’t reference them.
	\item \underline{\textit{'--create\_graph'}}: This will create a graph of the true values against the predicted values; as these are done in the continuous numerical domain, this is only really useful for the overall NSAA score output type and is generally written to a new file within the 'Graphs' directory to be used in the results discussions.
	\item \underline{\textit{'--epochs'}}: A quick way to modify the number of epochs needed to train a model; this only varies based on the type of file being trained; for example, computed stat values (i.e. the ‘AD’ measurement) files generally need only about 20 epochs to converge, while we generally use >100 epochs for raw measurements. The epoch value therefore isn't kept as a constant like the other hyperparameters but rather fluctuates as necessary to help achieve model convergence.
	\item \underline{\textit{'--other\_dir'}}: This argument is set with the name of another source directory in order to also include files from another directory (or directories) in order to train and test the model; it simply loads in additional files into the preprocessing function. The motivation behind this is further explored in the results discussion of 'model\_predictions.csv'.
	\item \underline{\textit{'--leave\_out'}}: This is the standard way to leave out a specific subject short name (e.g. 'D4') when training the model. This is the workaround instead of removing a subject from the source directory so the model is not exposed to the subject in the training process. This is primarily used in conjunction with 'model\_predictor.py' to test on the left-out subject in question for those particular models in various model predictions sets. See 'model\_predictions.csv' and its section in the results discussions for more information on using this argument.
	\item \underline{\textit{'--balance'}}: This is the way that we can either upsample or downsample the data set loaded in by calling the relevant functions within 'data\_balancer.py'. The motivation for rebalancing the data set and how it works is covered extensively in the section for that script and thus is not worth repeating here.

\end{itemize}




%%%%%%%%%%%%%%%%%%
\section{'model\_predictor.py'}

%%%%%%%%%
\subsection{Overview}

\quad While gaining insights into various types of model parameters, source data types, data preprocessing options, and so on are an important and useful output of the project, one of the primary aims is to be able to assess complete files on models built by ‘rnn.py’; for example, we may wish to see how the model performs when tested with a subject it has never seen before and record the results in 'model\_predictions.csv'. Alternatively, we may want to be using models in their 'production' form to help inform specialists about subjects based solely on model results. To do this, we need a separate script that not only preprocesses a single subject's file(s) for testing, but also loads the relevant models from a specified source.\\

\quad The 'model\_predictor.py' script was written with this in mind. While it may work with predictions and the preprocessing of data, it's unlike the 'rnn.py' script in that it does not create any models; rather, it uses the models that have been created by 'rnn.py' already. Hence, the script is only useable after 'rnn.py' has created the required models. The arguments to 'model\_predictor.py' primarily serve three purposes: to load the data from the relevant source directories based on the file types (e.g. the ‘AD’ and ‘jointAngle’ measurements) in '.csv' format (created by either the 'comp\_stat\_vals.py' and 'ft\_sel\_red.py' scripts or the 'ext\_raw\_measures.py' script), to load the models that have been created that have been trained on the directory the file in question is sourced from and with the relevant file types and for all output types, and finally to assess the '.csv' data files on the models that have been loaded and aggregate the results to make assessments.


%%%%%%%%%
\subsection{How it works}

\quad The execution of 'model\_predictor.py' runs in a fairly procedural manner; hence, it's more intuitive to describe the program as a sequence of steps that call functions when necessary rather than a series of functions that are connected together as needed (e.g. 'comp\_stat\_vals.py'). The execution is as follows:

\begin{enumerate}
	\item Checks the validity of each passed in argument.
	\item For a given file name, loads in the '.csv' files for each of the file types provided; for example, if ‘fn’='D4' and ‘ft’='AD,jointAngle,sensorMagneticField', then the 'FR\_AD\_D4\_stat\_features.csv', 'D4\_jointAngle.csv' and 'D4\_sensorMagneticField.csv' files are loaded in (the names of which might slightly vary in practice due to naming conventions).
	\item Identify the directories that contain the models that we require to use for the files' assessment; note that these are all contained within the '$<$local directory$>$\textbackslash output\_files\textbackslash rnn\_models' directory (unless the ‘--final\_models’ optional argument is used which uses the ‘$<$project directory$>$\textbackslash source\textbackslash rnn\_models\_final’ directory instead), and have names that reflect how the models were built and on what data. This is done for all three output types as well. For example, if ‘dir’='NSAA' and ‘ft’='AD' are used, then 'NSAA\_AD\_all\_dhc\_--seq\_len=10\_--seq\_overlap=0.9\_--epochs=300', 'NSAA\_AD\_all\_acts\_--seq\_len=10\_--seq\_overlap=0.9\_--epochs=300' and 'NSAA\_position\_all\_dhc\_--seq\_len=600\_--seq\_overlap=0.9\_--discard\_prop=0.9' are loaded as the model directory names containing the models.
	\item Preprocesses the data from the '.csv' files so that they will fit into the pre-trained models (e.g. by having the expected batch size and sequence length) along with fetching the requisite \textit{y} labels for the data in the same way as is done for 'rnn.py'.
	\item For each output type and for each of the '.csv' files of the data for the subject in question, put all the data through the model that corresponds to the '.csv's file type (e.g. ‘jointAngle’ or ‘AD’) and its output type in prediction mode and have the predictions collected.
	\item For a given output type, average together all predictions made over every sequence prediction for every file type to get a prediction for that output type for the whole file. For example, for the NSAA overall score output type, we average the scores for every sequence from a given file input type's predictions, repeat this for the other input types, and finally average these scores to get a prediction of the overall score that takes into account all predictions made for every sequence of all the input types (i.e. measurements) we are assessing on.
	\item Outputs these scores to the user and appends these results to a new line within the 'model\_predictions.csv' file, along with the name of the subject in question as well as the file types used, the source directory, etc.
\end{enumerate}

\quad Special attention should be paid to some of the optional arguments. Some are used exclusively by other calling scripts (e.g. '--handle\_dash' and '--file\_num' are exclusively used by the 'test\_altdirs.py' script) and others are fairly simple and self-explanatory (e.g. '--show\_graph' shows the true and predicted overall NSAA scores made for the subject, while '--single\_act' is used when the input to the models are single-act files); however, there are a few others that each require a brief explanation:

\begin{itemize}
	\item \underline{\textit{'--alt\_dirs'}}: Provide this with a name of a directory that is not the same as 'dir' to test files on models that haven't been trained on the same directory; for example, if dir='allmatfiles' and 'alt\_dirs'='NSAA' then subject files will be loaded from the 'allmatfiles' directory but tested on models trained on files originating from the 'NSAA' directory. The motivation and results of this are explored in more depth in the results discussion and can be seen in MPS 1 and MPS 22.
	\item \underline{\textit{'--use\_seen'}}: For a given file name (e.g. matching or deriving from a subject short name like 'D2-009'), the default behaviour of the script is to seek out model directories where the subject has been completely left out of the training and testing process; in other words, the subject who we're assessing is completely new to the models assessing it. This is done by specifically seeking model directories with names containing '--leave\_out=<file name>' (along with the other required directory and file type arguments). This is extensively used in model predictions sets involving the assessing of models’ generalization performance to new subjects. Sometimes, we may not want to do this specifically: for example, when we want to compare a subject being tested on a model familiar with the subject to one that isn't; see MPS 6 for an example of this.
	\item \underline{\textit{'--use\_balanced'}}: In a similar way that '--use\_seen' seeks out model directories that haven't got something in their names, this optional argument specifically seeks model directories to use that have got '--balanced'='<up/down>' in the name (depending on the value given to '--use\_balanced') and therefore have been created with upsampled or downsampled data sets. Hence, this allows us to test complete files on models that have trained on an upsampled or downsampled data set. For more information on the data balancing process, consult the README for 'data\_balancer.py' or, for more info on how well this performed on complete files, see MPS 10.
\end{itemize}



%%%%%%%%%%%%%%%%%%
\section{'test\_altdirs.py'}

%%%%%%%%%
\subsection{Overview}

\quad A key motivation of this project is investigating how well, if at all, models that are built on one type of data can be adapted to be used to assess other types of data; for example, models trained on the NSAA data set assessing on files from the NMB data set. Furthermore, to get a good idea of how well this is done, it's necessary to test numerous files on pre-trained models. In the case of testing natural movement files on models that are trained on NSAA and 6-minute walk files, this would require running 'model\_predictor.py' manually over 400 times and each time with a different file name from within 'allmatfiles' or ‘NMB’. To get around this, 'test\_altdirs.py' was created to automate this process.\\

\quad Crucially, this script only allows 'model\_predictor.py' to work on assessing models' performances on unseen files that also have aren't trained on the same type of data. This allows us to see the strength of the correlation between different types of assessment for subjects wearing the suits and also whether or not predicting the assessment scores by models trained on one type of assessment can be used to infer assessment scores of data in a form that the models haven't been trained on. In other words attempting to answer the question: “Can we have subjects just do natural movement activities and then use the models that have been trained on NSAA and/or 6-minute walk assessments to determine their D/HC classification, NSAA overall scores, etc., just as well as if they had instead done the NSAA and 6-minute walk assessments instead?” The results of this are explored later in the relevant results discussions in MPS 1 and 22.


%%%%%%%%%
\subsection{How it works}

\quad The 'test\_altdirs.py' script is executed as a series of steps does the following when run:

\begin{enumerate}
	\item Reads in the name of a directory from which we wish to source the files that we wish to use for assessment, and also the names of the directories that will have been used to train certain models (for example, supplying 'NSAA\_6minwalk-matfiles' here will ensure that each time 'model\_predictor.py' is then called it retrieves the models that are trained on NSAA and 6-minute walk files).
	\item Retrieves a list of ‘.mat’ file names from within the source directory (i.e. if 'allmatfiles' was passed as the 'dir' argument then the names of all ‘.mat’ files from within 'allmatfiles' are retrieved and stored in a list).
	\item For every file name within this list of file names, create a unique string that corresponds to the input string to run the 'model\_predictor.py' script with the required arguments. This string includes the short file name of the file in question, the file types that the models will have been trained on (for example, if 'allmatfiles' was chosen as 'dir', then this must be 'jointAngle' as this is the only type of information that can be extracted from this type of data), the assessment file directory, and the source file directories that were used to train the models.
	\item From here, all functionality is passed on to 'model\_predictor.py' for the give file, which runs once for every file with the source directory as specified by the 'dir' argument. For further information on how this runs and what it produces, refer to section on ‘model\_predictor.py’.
\end{enumerate}



%%%%%%%%%%%%%%%%%%
\section{'graph\_creator.py'}

%%%%%%%%%
\subsection{Overview}

\quad There are several different ways that the outputs of experiments can be stored as 'results', along with appearing in several locations. For example, the results of different RNN setups and its tests on the test sets will appear in the 'RNN Results.xlsx' file. Additionally, for each model run (i.e. each row in 'RNN Results.xlsx'), there is a whole file of true and predicted values over the test set stored in a single '.csv' file with a name that corresponds to the predictions. Meanwhile, the results of whole file predictions (i.e. through the use of 'model\_predictor.py' and it's wrapper script 'test\_altdirs.py') are written as a row per file prediction into the 'model\_predictions.csv'.\\

\quad However, none of the scripts that write to these files do any sort of plotting or graphing of the data. This is for two reasons:

\begin{enumerate}
	\item Many times where we are running the scripts, we don't want to see the immediate plotting results or, rather, we can't. For example, when we run the 'model\_predictor.py' script once, it's only concerned with writing a single line to 'model\_predictions.csv', in the same way that 'rnn.py' only writes one line to 'RNN Results.xlsx', so for these to plot any results over several lines, the scripts would need additional user arguments to tell the script which lines it wishes to use for plotting, which adds to the already-high complexity of the scripts. Additionally, we often run 'rnn.py' via a batch script with many slight differences (to easily create several models to test on) and 'model\_predictor.py' via 'test\_altdirs.py', so stopping to produce a graph for every line that is written to an output file would be very inconvenient and would slow down the process.
	\item In separating the functionality, we keep a large degree of modularity amongst the scripts. In other words, the scripts that write the output to the output files ('model\_predictions.csv', 'RNN Results.xlsx, etc.) have nothing to do with the actual plotting of results in graphs. This helps in debugging (i.e. a problem in displaying the data will usually be isolated to 'graph\_creator.py') and also allows us to choose when we wish to do the plotting (i.e. after the data that we determine we need has been collected, not after a predetermined point in the running of each 'rnn.py' or 'model\_predictor.py' run). Furthermore, this sort of setup opens up the possibility for an easier collaborative effort: if others were to contribute to the output files (e.g. by adding experiment results done on other types of data that is still written to the output files in the same format), then it's possible to use 'graph\_creator.py' as a standalone script without the need to have previously run any of the other scripts.
\end{enumerate}

%%%%%%%%%
\subsection{How it works}

\quad The direction that 'graph\_creator.py' takes in terms of running entirely depends on the first argument as ‘choice’. Based on this, the script calls one of five functions that processes the other given arguments in a certain way. Note that, as each function operates on the arguments given differently, some of them are given generic names such as 'arg\_one' and 'arg\_two'. Also note that, as each function requires different numbers of arguments, every argument other than the first one ('choice') is optional; hence, when 'choice' is set to 'model\_preds\_single\_acts', it won't throw an error when we only give it values for 'arg\_one' and not the other three positional arguments.\\

\quad Rather than going over things sequentially, we instead go over below each of the functions that are called by their associative 'choice' argument value:

\begin{itemize}
	\item \underline{\textit{‘plot\_trues\_preds()’}}: This is a very simple function insofar as it just takes in the name of the '.csv' output that is produced by every run of 'rnn.py' that contains the test true and predicted values and are contained within the '$<$local directory$>$\textbackslash outputs\textbackslash RNN\_outputs' directory. Hence, the only argument needed is 'arg\_one' and this is to be the full name (not including directories and the file extension) of the file we wish to use. This is then read in, the predicted and true values are read in, and these are plotted against each other in 2 dimensions, with a $y=x$ line going through them to signify their 'ideal' positions.
	\item \underline{\textit{‘plot\_model\_preds\_altdirs()’}}: Reads in the 'model\_predictions.csv' as a DataFrame object; from here, we then wish to determine which rows in the DataFrame object that we wish to use. This is then based on rows that have their 'Source dir' column set to the value of 'arg\_one' and the 'Model trained dir(s)' column set to the value of 'arg\_two'. For example, if we wish to plot the rows in 'model\_predictions.csv' where a complete file from a specific source directory (e.g. 'allmatfiles') is then assessed on models trained on 'NSAA' and '6minwalk-matfiles' files, we set ‘arg\_one’='allmatfiles' and ‘arg\_two’='NSAA,6minwalk-matfiles'. This then selects the lines from the DataFrame object that we are concerned with. From here, with these lines we extract the true and predicted overall NSAA values from both the model trained on NSAA directory files and the model trained on ‘6minwalk-matfiles’ data set files. These values are then plotted with the true values along the $x$-axis and the predicted values along the $y$-axis and is done for both models. We also extract the 'percentage of correctly predicted D/HC label for sequences' for each file and model this percentage distribution as both cumulative and non-cumulative distributions for both source directories. We then repeat the same process but for the columns representing the percentage of individual acts correctly determined, and finally plot some useful statistical values computed over the lines.
	\item \underline{\textit{‘plot\_model\_preds\_trues\_preds()’}}: This is essentially the same as ‘plot\_trues\_preds()’ but operates on ‘model\_predictions.csv’ rather than ‘RNN Results.xlsx’. Hence, when we use this function and specify a start and end row with ‘arg\_one’ and ‘arg\_two’, we look for those rows within ‘model\_predictions.csv’, find the true and predicted overall NSAA score columns, and plot them alongside each other on the $x$- and $y$-axes. See ‘plot\_trues\_preds()’ above for further information about these graphs produced.
	\item \underline{\textit{‘plot\_model\_preds\_single\_acts()’}}: This is the third function to read in the 'model\_predictions.csv' file but, as we treat 'single-act' rows in the file differently than those that use alternative directories for assessment, it's easier to keep the functionalities separated. Hence, we first load in the file as a DataFrame object and select only the rows that have the value contained in 'arg\_one’; i.e. ‘act’ in the name of the short file: this signifies that a single-act file has been assessed on a model, rather than a full source file. From here, for each line we extract from the row's cells the percentage of acts correctly predicted, the percentage of correctly predicted D/HC label for sequences, and the difference between the true and predicted overall NSAA score. From these, we take one of the values for each of the single-act files and plot these values against the act number. This is then repeated 2 more times for the other 2 extracted values over each of the 17 single-act files. This then leads to 3 subplots where the $x$-axis is the act number (between 1 and 17) and the $y$-axis is one of percentage of acts correctly predicted, percentage of correctly predicted D/HC label for sequences, or the diff between true/predicted overall NSAA.
	\item \underline{\textit{‘plot\_rnn\_results()’}}: This is the function that analyses the 'RNN Results.xlsx' files and is responsible for the majority of graphs that show the performance of different RNN setups (e.g. sequence lengths, overlap proportions, number of features, types of raw measurements, etc.). The 'arg\_one' and 'arg\_two' arguments take the start and end experiment numbers of the file (once it has been loaded in as a DataFrame object) by looking at the 'Experiment Number' column to decide on which rows of the DataFrame object that we are concerned with. From here, for each row (which is associated with a model that has been created and tested upon) we extract the names of each measurement the model in question has used, the sequence length, and the results that it has produced. Then, based on the fourth provided argument ('xaxis\_choice'), we decide on what to plot along the $x$-axis: if it's set to 'seq\_length', then for each measurement (e.g. 'AD', 'jointAngle, etc.), we create a line and plot how well it performed at various sequence lengths with respect to different metrics (e.g. $R^2$, RMSE, etc.) based on the third provided argument 'out\_type'. If instead it's 'ft' (file type), 'seq\_over' (sequence overlap), or 'features' (number of features used), then a single line to plot is used instead over all the lines from DataFrame selected to plot the aspect of the data specified by the 'xaxis\_choice' against the metric specified by 'out\_type'. Numerous examples of these types of graphs can be seen in the sections of the report discussing specific experiment sets.
\end{itemize}


%%%%%%%%%%%%%%%%%%
\section{'data\_balancer.py'}

%%%%%%%%%
\subsection{Overview}

\quad One of the inherent problems with the dataset is the lack of 'variance' within the subjects for their overall scores. This is mainly a feature of how the NSAA assessments are conducted and the inherent variation of severity of Duchenne muscular dystrophy across the subjects. As the individual activity scores range from 0 (can't complete the activity at all) to 2 (completes it perfectly) and as there are 17 activities in total, the overall cumulative score ranges from 0 to 34. However, in reality, most patients in the study have scores ranging between 15 - 24 for moderate Duchenne. When it comes to training a network on the subjects' data and testing it on new files, this causes a problem if the subject has a particularly low overall NSAA score (e.g. 3). In other words, the lack of variation in the data we have available may slightly limit the potential of models generalisation ability to new subjects with particularly extreme cases of DMD. Thus, this is an important aspect to cover when we wish to improve generalization performance of the models to new subjects.\\

\quad A classic way in machine learning of helping to get around this is in using data balancing. This is traditionally done for classification problems rather than regression problems, as we are doing here. However, we get around this by, for the purposes of rebalancing the data set, considering overall NSAA scores as class labels rather than scores to be regressed on. There are two ways we consider here to balance our data, which are outlined below:\\

\quad Consider a dataset of 10 sequences of data (i.e. 2D structures of data of shape (sequence length, \# of features) with scores: [3, 15, 15, 15, 20, 20, 34, 34, 34, 34]. We have 2 ways of approaching this:

\begin{itemize}
	\item \underline{\textit{Downsampling}}: Counts the frequency of each number in the list and finds the lowest frequency; in the above case, it is 1 (as there is only 1 '3' in the list). Next, for each of the labels in the list above, we randomly select '1' sample of each label in the list and, more importantly, the label's corresponding $x$ value (i.e. a single sequence). Thus, we are reduced to a list of 4 sequences and with a label list ($y$ labels) of [3, 15, 20, 34] (note that there is only 1 of each sample because there was originally 1 '3' label). Hence, we now have a much smaller list, but an even spread of $y$ values for the samples we have remaining.
	\item \underline{\textit{Upsampling}}: We start off the same, with finding the frequency of each number in the list, but this time considering the highest frequency in the list. In the above case, this would be '4', as there are 4 ‘34's in the list. Next, for each label value in the list, we randomly sample a $y$ and corresponding $x$ value (being a sequence) a total of '4' times for each label. For example, for the '15' labels (i.e. 3 sequences and 3 '15' labels), we randomly pick a pair of $x$ and $y$ values from the 3 available and do this a total of 4 times. Thus, we end up with a much larger list of [3, 3, 3, 3, 15, 15, 15, 15, 20, 20, 20, 20, 34, 34, 34, 34] of $y$ values with corresponding $x$ values (sequences).
\end{itemize}

\quad Upsampling has the advantage of it being less likely to discard any of the data that has been given to us; however, it also means that many samples are repeatedly used as 'new' samples, which may lead to unpredictable training results, along with an inflated data set may being more challenging to train on. Downsampling, meanwhile, might give better generalization results than non-resampled data while being a smaller data set (thus making it quicker to train models that achieve better results), but the discarding of many data points might leave out important insights from the data out of the training process.

%%%%%%%%%
\subsection{How it works}

\quad The script contains 3 functions: 'ext\_label\_dist()', 'downsample()', and 'upsample()'. The last two functions are more-or-less identical to their respective algorithms that are outlined above, with a few implementation details differing but the overall ideas being the sample; hence, we won't repeat the more-or-less same algorithms here. Instead, it's worth considering how each of the functions are used. The script is never run directly, but rather serves simply as a storage place for several functions that are fetched by 'rnn.py'; hence, it's instead useful to consider exclusively how 'rnn.py' calls the functions. Also note that these are only run by the 'rnn.py' script if the '--balance' optional argument is provided.\\

\quad It's also worth noting the distinction between 'y\_data' and 'y\_data\_balance' when used as parameters for 'downsample' and 'upsample': 'y\_data' might be, depending on the output type that we are training towards (e.g. D/HC classification, overall NSAA score, or single act scores) a list of 1's and 0's, a list of values between 0 and 34, or a list of lists of 17 values between 0 and 2. Hence, we want a unified way of rebalancing the data that is irrespective of the form that 'y\_data' takes. Hence, 'y\_data\_balance' will always be the overall NSAA scores for the corresponding 'x\_data'; if, for 'rnn.py', the 'choice' argument is 'overall', then this will be exactly the same as 'y\_data', but for others it will contain the overall NSAA scores that are corresponding to the $x$ and $y$ samples. The ‘y\_data\_balance' is then used in the algorithms outlined above to find the indices of 'x\_data' and 'y\_data' to select to create the new lists of data.\\

\quad The functions of 'data\_balancer.py' and how they are used by 'rnn.py' are as follows:

\begin{itemize}
	\item \underline{\textit{'ext\_label\_dist()'}}: For each file that the 'rnn.py' model is training on, reads in the 'nsaa\_6mw\_info.xlsx' file, finds the relevant row in the table corresponding to the file name in question, and returns the overall NSAA score for this file name. This is then used as the label for each of the sequences that are extracted from the file in question, and the process is then repeated for every other file in the source directory, 'dir'.
	\item \underline{\textit{'downsample()'}}: If the '--balance' argument is set as 'down', then this function is called that takes in the 'x\_data' and 'y\_data' created from sequences (as 'rnn.py' would normally create) and the additional 'y\_data\_balance' that we have created additionally to use to balance the script, and from these downsamples the data and produces two new lists of 'new\_x\_data' and 'new\_y\_data' via the algorithm outlined above.
	\item \underline{\textit{'upsample()'}}: Called in the same way as 'downsample()' but via '--balance=up', while taking in the same arguments but instead using the algorithm for upsampling as described above.
\end{itemize}

\quad With the 'ext\_label\_dist()' and either 'downsample()' or 'upsample()' having been run the requisite number of times ('ext\_label\_dist()' once for every file in the source directory, 'dir', and only once for either of the other two), this data then replaces the original 'x\_data' and 'y\_data' in 'rnn.py', prints the new balanced shapes to the user, adds several output strings to be printed at the end of the script's running to show the before- and after-data-balancing for the distribution of labels, and the execution of 'rnn.py' subsequently continues as usual.



%%%%%%%%%%%%%%%%%%
\section{'file\_renamer.py'}

%%%%%%%%%
\subsection{Overview}

\quad One of the primary problems with working with ‘.mat’ files as part of this project is the lack of standardization of file names as they were collected. We have primarily been dealing with 5 source directories containing ‘.mat’ files: 'NSAA' (containing NSAA assessments of subjects), '6minwalk-matfiles' and '6MW-matFiles' (containing the 6 minute walk assessments of subjects), and 'allmatfiles' and ‘NMB’ (containing the natural movement files of subjects wearing the suit either as solely joint angles or all raw measurements, respectively). Each directory had its own primary way of labelling files but, even within directories time, it wasn't necessarily consistent throughout the directory.\\

\quad This posed a not-insignificant problem in that some of basic characteristics of the file were determined by its file name (e.g. whether it was a 'D' or 'HC' file came from reading its file name, along with what subject the file was associated with). Until development of this script, the solution was having multiple ways of processing every file name within the various scripts that need them. However, there's several flaws in this approach:\\

\begin{enumerate}
	\item It was not particularly extensible to new files with new formats being added. If new files were added to one of the source file directories with a slightly different naming format, it would require going deep into several scripts in order to change how they extracted the subject name of each new file it’s associated with, it's D/HC label, etc. This process ends up just adding more 'if...else' clauses to many already-cluttered parts of the scripts.
	\item As a result of having to change numerous things in several scripts, the process was more prone to human error. For example, as a result of a small oversight and not correctly reading the 'D' part of a file name that corresponded to subjects with 'D' in their subject name (e.g. 'D5'), the script was incorrectly interpreting the D/HC label for many files as being 'HC' rather than 'D' like it should have been; hence, the model was trained incorrectly due to labelling sequences incorrectly. In comparison, if we would have used 'file\_renamer.py' from the beginning, we would have easily spotted any files that have been renamed incorrectly and correct them before other scripts had the chance to misinterpret their labels.
\end{enumerate}

%%%%%%%%%
\subsection{How it works}

\quad The basic operation of the 'file\_renamer.py' script can be summarized as follows:

\begin{enumerate}
	\item Reads in the name of a source directory of ‘.mat’ files of which we wish to standardize the names.
	\item Gets the names of all ‘.mat’ files within the directory and divides them into one of two categories: 'files\_kept' (i.e. the vast majority of files which we don't want to remove) and 'files\_to\_delete' (files which we want to remove from the directory). Note that this is only for certain files that have been previously determined to be too large, too small, or not 'relevant' files to either training or testing models; for example, files that contain 'AllTasks' in their name in the 'allmatfiles' source directory, as these contain the same information as the other files in the directory but concatenated together for a single subject, so there's no need to use these as well as the others.
	\item Based on the source directory name, apply a set of regular expression ('regex') rules to each file name that are in 'files\_kept'. These are unique to each directory, as there are some things that we need to check for in some directories but not in other. These regular expressions are a set of substitutions: they search the file name for a certain characteristic and, if it finds it, replaces it with another before using this new string as the basis for the next regex. These regexes include: replacing non-capitalized subject names to capitalized versions (e.g. changing 'd4-003.mat' to 'D4-003.mat'), replacing 'NSA' with 'NSAA when found in a file name, changing instances of '-6MW.mat' to '-6MinWalk.mat' (as the type of activities they contain is the same whether it was sourced from '6minwalk-matfiles' or 6MW-matFiles'), and so on.
	\item With this new list of file names that we are to change 'files\_kept' to, we first remove the files within the source directory based on the file names within 'files\_to\_delete' and then, for each name in 'files\_kept' and its corresponding name in 'new\_files\_names', replace the name of the file in the former with the name in the latter. The result is that all of the files within the specified source directory are automatically changed based on the standard we predefined.
\end{enumerate}

\quad However, it's important to note that this script is not intended to be run more than once, and only at the beginning. Hence, it should be executed before any of the other scripts like 'comp\_stat\_vals.py' or 'ext\_raw\_measures.py' are used. This is because these scripts use the names of the files they are sourced from to create new files with names based on their source names; hence, for 'file\_renamer.py' to be useful, they should be used prior to other files being created that are based on the files that 'file\_renamer.py' wishes to rename. Hence, 'file\_renamer.py' is only needed to be used once. For this reason, it's also included within 'setup.cmd' as part of the setup process and is applied before any of the other scripts for the above reason.


%%%%%%%%%%%%%%%%%%
\section{'settings.py'}

%%%%%%%%%
\subsection{Overview}

\quad The purpose of this file is to hold many of the variables that are used throughout the rest of the script. In particular, there are many variable names (such as 'source\_dir') that hold the same values throughout all of the scripts. These variables contain values that include directory sources paths, paths to certain files that scripts output information to, lists of sensor names that have been given to us via the 'MVN User Manual', and so on; the common factor is that they are all referenced as being the same values across several different scripts and are thus interpreted as system constants.\\

\quad In storing these values in a separate file, we achieve three things:

\begin{enumerate}
	\item It reduces the amount of overall 'clutter' within the scripts, especially when we need to reference large variables such as those holding large lists of strings, which makes the scripts themselves both easier to debug and easier to maintain.
	\item For variables that are supposed to remain static, it reduces the possibility of accidentally changing them to suit the script they are currently being referenced in. For example, we are less likely to accidentally change the name of one of the 'raw\_measurements' when they are only accessed in other scripts and not modified and, if one is changed in 'settings.py', then this change is reflected out to all other scripts in the same way (e.g. preventing two scripts from each having their own versions of 'raw\_measurements', which could cause conflict in manipulating output files).
	\item If they are required to change for whatever reason (e.g. if a new user has their 'local\_dir' in a different location to the default value, or if the batch size to be used across numerous scripts is modified to be something else), then it's much easier to do so in a single 'settings' script rather than tracking down and modifying each respective variable in each script.
\end{enumerate}

\quad To access these values, each of the scripts calls the necessary variables from ‘settings.py’ in the 'import' section of the script. The idea of scripts only importing the variables that it needs was that it enhances clarity (i.e. if 'from settings import *' was used, we wouldn't as easily be able to see that 'local\_dir' comes from 'settings' as if we had used 'from settings import local\_dir'). Additionally, it's also recommended that any user using this project and ‘setup.cmd’ for the first time should first examine the relevant path names (such as 'local\_dir', 'results\_path', etc.) to ensure that the source ‘.mat’ files are contained in the expected location, the scripts can access the necessary output .’xlsx’ and ‘.csv’ files, and so on.




%%%%%%%%%%%%%%%%%%
\section{'predictions\_selector.py'}

%%%%%%%%%
\subsection{Overview}

\quad With so many file predictions being made and stored in 'model\_predictions.csv' as part of model predictions sets, it became necessary to have a way to sort through them all and return the rows that we are most interested in. This is why this script has been built: to filter rows of the table (each corresponding to a complete file prediction made using 'model\_predictor.py' or by extension the 'test\_altdirs.py' script) based on several arguments (e.g. the subject names we're interested in, the directory the subject was trained on, or the alt directories that the models were trained on if they are 'altdirs' rows) and, based on whether '--best' or '--worst' is provided, return the best or worst 'm' rows according to output metric 'n', where these are provided as part of '--best'/'--worst' (e.g. '--best=30,overall').\\

\quad In essence, this functions similarly to how an SQL query would operate as 'SELECT $<$a$>$ FROM model\_predictions WHERE $<$condition$>$’. However, the desire was to do this in Python so the whole pipeline would only require one language for implementation (no accounting for libraries built on top of languages like C++, e.g. for TensorFlow). Furthermore, this is easily possible via extensive use of the 'pandas' library to load in 'model\_predictions.csv' as a DataFrame object, which is excellent for the filtering of rows based on cell values, ordering rows by lowest/highest values in a specified column, and so on to make manipulation of the table as easy as using an SQL query. Additionally, this also means that anyone else running this system only needs to setup a single language/IDE in order to execute all of the scripts.\\

\quad The idea from building this script is having an easy way to see some of the 'most relevant' rows of the table to the user. Presently, this just takes the form of console output, though easy modification to have these lines written to file is possible. This script is especially useful for when we have many files to 'sift' through in order to get an idea of which are the best or worst performing on a given metric. For example, one particular application could be using the script to look at all the natural movement behaviour files that have been assessed on models build on NSAA and 6-minute walk files (totalling ~400 files) and selecting the best 20 of these according to which predicts the overall NSAA score of that file closest to the true value for that file. This has the potential to help us identify the types of natural behaviour files (e.g. sitting and eating, playing, sitting and moving on the floor, etc.) perform the best according to the metric. Another application could be, for a given subject name from the NSAA directory and on models trained on the same directory but left out of the training set completely, which options make the subject be predicted closest to the correct score (e.g. if the models’ data are upsampled, downsampled, trained on single-act files, etc.). We can see this script being used in particular in MPS 23.

%%%%%%%%%
\subsection{How it works}

\quad The script itself is fairly simple with no functions to call or classes to instantiate; rather, it executes a series of 'groups' of instructions that carries out the above-outlined tasks based on the script arguments. These can be summarised as follows:

\begin{enumerate}
	\item Loads in the 'model\_predictions.csv' file as a DataFrame object.
	\item Filters the rows of the table based on the 'sfn' argument, which removes all rows where the subject name doesn't match the value of 'sfn'; alternatively, if 'sfn'=’all’, keep all rows at this point.
	\item Filters the rows of the table based on the 'sd' argument, which removes all rows whose source directory column is different from the value of 'sd'.
	\item If the 'mtd' is given (i.e. if we're concerned with 'altdir' rows), filters the rows of the table based on this arg, which removes all rows whose ‘altdir’ column is different from the argument value. Note that the this argument is given as comma-separated values, which corresponds to the list values of the column in question.
	\item Based on whether the optional '--best' or '--worst' arguments are given (or both), extracts the first part of the argument (s) as the number of ‘--best’/‘--worst' lines in the table and the second part as the short name of the metric to use to determine which are the ‘--best'/‘--worst' (i.e. by deciding which of the output columns of the table to use to order the rows).
	\item For each of the remaining rows of the tables (i.e. after having been filtered by steps 1-4), we now filter the columns of the table: the first four columns are kept (the subject name, source directory, model trained directories, and measurements tested), followed by one of the output columns (the column in question is selected by the second part(s) of the ‘--best'/‘--worst' argument). These values are additionally preprocessed: e.g. if 'overall' is selected, then the absolute value of the difference between the true and predicted values in their respective columns are selected, while if we're using the 'percentage of predicted correct sequences' metric, the relevant column for 'Percentage of predicted <D, HC> sequences' is used based on the true D/HC label for the row.
	\item Creates a list of column names to create a new table of the top $n$ results that include the aforementioned 4 beginning column names from 'model\_predictions.csv', followed by column names of the output metrics with the names of the directory that the models that outputted this metric were trained using.
	\item Finally, select the top or bottom (or both) $n$ number of lines based on the selected column metric, depending on which of '--best' or '--worst' has been selected and the number of lines to extract from each of them, having reversed them if needed for percentage metrics ('pacp' and 'ppcs'), before printing out the selected rows to the console as a DataFrame object.
\end{enumerate}




%%%%%%%%%%%%%%%%%%
\section{'dis\_3d\_pos.py'}

%%%%%%%%%
\subsection{Overview}

\quad One desire for the data that we have received as '.mat' files is to be able to plot the subject portrayed within the file as a real-time 3D plot. The aim of this is to hopefully allow us to do two things:

\begin{enumerate}
	\item Visualize the subject within the data as doing certain activities in order to provide a reference (along with the console 'Plotting time...' output) as to what activities are taking place at which time; this is particularly useful as it helped in creating the Google annotations sheet.
	\item In plotting this, it easily allow for anomalies within the data file to be detected; for example, if the subject suddenly 'jumps' position or the limbs appear extremely contorted, it might indicate corrupted data which might need to be 'cut out' of the file (or have the whole file discarded).
\end{enumerate}

\quad Though this functionality also exists within the 'comp\_stat\_vals.py' script, it was felt necessary to also provide the functionality as a separate script within the system; hence, a lot of the code that was required by the '--dis\_3d\_pos' optional argument within 'comp\_stat\_vals.py' is repeated for this script.

%%%%%%%%%
\subsection{How it works}

\quad This script involves a series of basic steps that the data goes through in order to display a dynamic, 3D plot to the user. Hence, we shall explain it here as these steps which include the following:

\begin{enumerate}
	\item Loads in a '.mat' file corresponding to the 'dir' and 'fn' arguments provided to the script. This is read in as a DataFrame object and is returned from 'preprocessing()' and passed to 'display\_3d\_positions()'.
	\item Extracts the values from the 'position' column and reads this in as a 'positions' matrix (of shape (\# of samples, 69)), separates the columns of this new matrix into tuples of $x$, $y$, and $z$ axes for each segment within positions for every sample, define connected segments via tuples of pairs of values, sets the boarders of the 3D plot (i.e. the $x$$‘y$/$z$ mins/maxes), plots the 3D figure from the first sample with connections between points defined by the tuples of pairs of values, and animates it by fetching a new sample to plot every '1/sampling\_rate' sections so the figure is animated in real-time while outputting to console the current time-stamp of the figure in seconds.
	\item After ~5 seconds where the data is sourced, extracted, reconfigured to work in 3D, and animated, a new window will appear. This is the 3D plot that runs in real time. Note that one should also see as a console output the time stamp in seconds of where the plot currently is at (i.e. how far through the positions matrix it is). There is no current way to pause, slow down, or speed up the plotting, though one can change the viewing perspective by left clicking and dragging with the cursor or zoom in and out by right clicking and dragging with the cursor.
\end{enumerate}



%%%%%%%%%%%%%%%%%%
\section{'file\_mover.py'}

%%%%%%%%%
\subsection{Overview}

\quad To enable the working of certain batch scripts, it became a necessity to build into the batch files the ability to relocate files that are located anywhere on a user's PC to the proper sub directory of the local directory in order to have the data pipeline run properly. For example, if there was a source '.mat' file for 'D9V2' subject as an NSAA file (i.e. the second NSAA assessment done for subject 'D9') located somewhere on a user's PC, we wish to be able to copy it over to the $‘<$local directory$>$\textbackslash NSAA\textbackslash matfiles’ subdirectory. However, while we are able to do this potentially in a batch file via the 'move' command, we also wish to be able to change the location of where to copy the file to depend on the type of file we are working with (e.g. if the file is an NMB file, it would be placed in a different location within the local directory than if it was an NSAA file); additionally, we also wish to make use of the 'local\_dir' variable stored in 'settings.py' so one wouldn't have to modify a variable within a batch file if the local directory location was changed.\\

\quad For the above reasons, it was evident that it was simply easier to implement the 'move file' functionality in its own separate Python script. The intention, however, is to only ever use this file as part of a batch file or the ‘assess\_nsaa\_nmb\_file.py’ script as the first step in placing a file in the correct location to be used within the data pipeline.

%%%%%%%%%
\subsection{How it works}

\quad As this is a short script with a singular purpose, it's worth outlining the simple steps, as the program runs in a procedural manner:

\begin{enumerate}
	\item Takes in as arguments the name of the directory within the local directory to place the file within based on the type of file (e.g. 'NMB', 'NSAA', '6minwalk-matfiles', etc.) and the complete or local path (relative to the ‘$<$project directory$>$\textbackslash source\textbackslash batch\_files’ directory) to the file we wish to move.
	\item Checks for argument validity for the 'dir' argument and, given it is one of the allowed options, adds the strings to the 'local\_dir' variable based on the 'dir' argument so that 'local\_dir' now points to the correct 'inner' directory to store the copied source '.mat' file in.
	\item Attempts to copy the file given as the argument to the program (as a complete or relative file path) to the new value of 'local\_dir', and throws an exception if it cannot locate the file by the path given.
\end{enumerate}



%%%%%%%%%%%%%%%%%%
\section{'assess\_nsaa\_nmb\_file.py'}

%%%%%%%%%
\subsection{Overview}

\quad As part of the finished deliverables for the project, we wanted to create a 'wrapper' Python script that was able to assess a single file (either an NSAA or NMB file) wherever it was located within a user's local system on the models that we have selected as our 'final' chosen models (i.e. those that are contained within '$<$project directory$>$\textbackslash source\textbackslash rnn\_models\_final'). The idea of this script is that it would act as the primary tool that someone would use who only wants to assess a single file on the models that we have built and chosen as the best possible models for the job.\\

\quad While we could have implemented this functionality as a batch script, it was felt that it would be easier implemented as a Python script. This made things like conditional calling of other scripts, argument parsing, and so on simpler to program than if we were using a batch script. It also allowed for dynamic user interaction through inputs to the script, which meant that the script could be written in a more user-friendly way. In other words, for this script we do away with taking in arguments and instead ask for user input at points throughout the script execution. The hope is that it makes it easier to use for any user and can simply run it with only the project directory obtained and a '.mat' file somewhere on their system of which they wish to assess.\\

\quad As the script is meant to not require the local directory, this posed a potential problem to calling the other scripts (such as 'comp\_stat\_vals.py') which require the files to be located within the local directory in order to operate them. To get around this, we make use of the 'file\_mover.py' functionality within ‘assess\_nsaa\_nmb\_file.py’ where, if it doesn't see a local directory where it's expecting (based on the 'local\_dir' variable value in 'settings.py'), as would be the case where the user doesn't have the local directory, it instead creates a local directory with the same name, with the corresponding inner directories, and places the file from wherever the user specified into here. This then allows the subsequent scripts to operate upon this file as normal.

%%%%%%%%%
\subsection{How it works}

\quad The primary operation of the scripts is to take in user input and, from the various inputs, create strings that are passed in turn to the 'os.system()' function to call each script in turn with the correct arguments. Again, as this is a fairly simple script in its execution with no function calls or object creations, we can summarize the script as a series of several steps:

\begin{enumerate}
	\item Gets from the user the user-specified path to the '.mat' file which the script shall be assessing (can be either an absolute path or a path relative to the '$<$project directory$>$\textbackslash source' directory.
	\item Gets from the user whether the file is of an NSAA assessment or a natural movement behaviour file.
	\item Executes 'file\_mover.py' to move the file to the required subdirectory of the local directory (based on the NSAA\textbackslash NMB choice specified) or, if the directory doesn't exist, creates the required directory and subdirectories and then copies the file to the required subdirectory.
	\item Executes 'file\_renamer.py' to rename the now-copied file if required.
	\item Gets from the user the comma-separated measurements (raw or computed statistical values) to use to assess the file.
	\item Executes 'ext\_raw\_measures.py' if required to extract the raw measurements from the file.
	\item Executes 'comp\_stat\_vals.py' and 'ft\_sel\_red.py' if required to extract the computed statistical values and reduce the dimensionality of the file’s computed statistical values.
	\item Gets from the user whether or not they wish to use models built on 'alt\_dirs' and/or built solely on non-'V2' files.
	\item Based on the inputs given by the user regarding 'alt\_dirs' and 'V2' files, execute 'model\_predictor.py' to assess the file's measurement data on the appropriate models, display the results to console, and write the results to 'model\_predictions\_newfiles.csv'.
\end{enumerate}




%%%%%%%%%%%%%%%%%%
\section{Additional batch scripts}

\quad Along with the Python scripts that make up the system pipeline, we also make use of several batch scripts for automating some of the tasks and for setup. As these aren't particularly long or complicated, it isn't worth creating a separate section for each, but rather a single section covering all of them along with when we would use them:

\begin{itemize}
	\item \underline{\textit{'setup.cmd'}}: This script runs the necessary 'pip' package installation commands to setup all the external libraries needed for running the project. Specific versions of the packages are used to match the exact versions used as part of this project to avoid potential complications, although setting up the most recent versions of the packages would most likely work just as well. We also run the necessary system scripts on all setup source directories. This requires that the user has setup the source directories ('NSAA', '6minwalk-matfiles', etc.) in a base directory that matches the name of the 'local\_dir' global variable stored in 'settings.py'. Assuming that, the rest of 'setup' will extract the statistical values from each file in every directory, along with reducing the features of these, standardizing the names of the files, extracting all raw measurements from every 'AD' file, and dividing up files to extract single activities from 'AD' files.
	\item \underline{\textit{'models\_no\_leftout.cmd'}}: At the point where we have found the optional model parameters to assess left-out subjects, we then wished to build new models but with no subjects left-out of training. Hence, this script is essentially an extension of MPS 20 where, instead of building models with left-out subjects as done in MPS 20, we build them with all subjects included. These models were then copied over to ‘rnn\_models\_final’ within ‘$<$project directory$>$\textbackslash source’ so they could be used by the ‘assess\_nsaa\_nmb.py’ script.
\end{itemize}

%%%%%%%%%
\subsection{Model prediction set scripts}

\quad In an effort to make the execution of the model predictions sets easier (which often require numerous new models to be created with 'rnn.py' and many separate file predictions to be made with 'model\_predictor.py'), we have created batch scripts to automate this process. This also holds the additional benefit where any user can inspect what arguments we have run each script with and also enables them to run them for themselves to see if comparable results can be obtained (obviously requiring the setup of all other files via 'comp\_stat\_vals.py' and other necessary scripts via ‘setup.cmd’ beforehand).\\

\quad The idea is that, for each model predictions set that we are running, all that is needed is therefore to just run the specified '.cmd' script. This will build the requisite models, though sometimes it won't build any new models but will instead rely on models built by previous '.cmd' scripts; hence, it's recommended that each model prediction set batch file be run in numerical ascending order. Once a given model predictions set’s batch file has been run, with the necessary models built and file predictions made, the results will appear in 'model\_predictions.csv' as the final rows in the table. It's also worth noting the time discrepancies between some of the '.cmd' files: some will only be calling 'model\_predictor.py' multiple times, which is comparatively quite quick to execute. However, those that call 'rnn.py' many times will take a lot longer; for example, 'model\_predictions\_set\_3.cmd' needs to build 60 separate RNN models, each of which may take 10-15 minutes to run (assuming the user is building them using a GPU), which could take 10-15 hours in total to execute the script.\\

\quad Finally, the scripts don't take any arguments, as the Python script parameters have been decided in advance. For example, prior to executing the batch scripts for model predictions sets 3 and up, we decided to test the models on the left-out subjects D3, D9, D11, D17, and HC6 (see the experiments results discussion set for an overview as to why these subjects were chosen). Hence, any changes that would be made to these '.cmd' scripts must modify each instance of the Python script that is called by the batch script in question in order to correctly alter these chosen script parameters.




















%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\part{Experiments and Results Discussion} 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Background\\}




%%%%%%%%%%%%%%%%%%
\section{Experiment Sets and Model Predictions Sets: Overviews and Differences}

\quad Broadly speaking, there are two categories of experimentation that we carry out for the system. The first are what we call ‘experiment sets’. These are generally speaking carried out using only the ‘rnn.py’ script and are done to ascertain the optimal model setups to be used later on in ‘model predictions sets’, the second category of experimentation. Experiment sets include determining the best raw measurements to use to build a system, the number of features from computed statistical values to use, the sequence length and sequence overlap, among other things. All of the results we obtain from the experiment sets are obtained only by the use of ‘rnn.py’ and the results are obtained by the output of the testing portion of the input data set. Therefore, the results that we obtain here are from testing data from files that it has seen before but data from those files that it hasn’t. In other words, the testing data may reflect a more optimistic view of the real-world results on new subjects than would be the case in actuality, and therefore this is the primary cause of disparity between performance in many experiment sets and the results from many model predictions sets; for example, we tend to see a lot higher classification accuracies and more accurate predicted overall NSAA scores in experiment sets than in model predictions sets. However, even though the results might be less accurate than on left-out subjects, the experiment sets where we see improvements in the models by changing some aspect of the models’ setup can be regarded to also similarly work on left-out subjects. The diagram below shows this relationship between the data that models are trained on, the script that builds the models, the final assessments made, and where we store the results:

\begin{center}
\includegraphics[scale=0.5]{project_figures/fig10_1}
\end{center}

\quad The second category of experimentation are ‘model predictions sets’. Unlike the experiment sets, these generally involve the use of two scripts: ‘rnn.py’ and ‘model\_predictor.py’. Furthermore, unlike examining the results from the testing data from built models, we instead use a complete file from a subject left out of the data and do assessments on this subject using ‘model\_predictor.py’. These also make use of pre-built models, so many of the model predictions sets involve both the building of models with different settings set and also the loading of these recently-built models to assess a left-out subject on. These model predictions sets include testing the performance of different models built on different measurements, the impact of introducing noise to the data set in an effort to help generalisation, the leaving-out of anomalies from the data set, and so on. As these model prediction sets generally involve one or more subjects that have been left out of training completely, the results we obtain here are generally more indicative of real-world assessments of new subjects when the project has been completed and if the system is being used as an assessment tool. The diagram below shows the relationship between data sets, the scripts utilized, and where the results of model predictions sets are stored:

\begin{center}
\includegraphics[scale=0.5]{project_figures/fig10_2}
\end{center}





%%%%%%%%%%%%%%%%%%
\section{Experimentation Replication: How to Carry Out the Experiment Sets and Model Prediction Sets Outlined Below}

\quad Prior to outlining all parts of the experimentation undertaken for this project, it’s preferable to outline for any prospective user the requisite steps needed to replicate the experimentation as best they can. This could be to validate the results that we claim to have achieved here, or to instead carry out further experimentation of their own and would thus benefit from a brief walk-through of the steps necessary to do so. Before we do any of this, however, we should note a few prerequisite steps that must be undertaken if they haven’t already been by the user. This includes downloading the project directory, all parts of the local directory, setting up Python and PyCharm, the running of the ‘setup.cmd’ batch script, and enabling TensorFlow to use any available GPU. Further details of the steps necessary can be found within the ‘System Setup’ chapter of the report.\\

\quad Once all the necessary setup steps have been done and with the project and local directories in place with the data sent through the data pipeline via the use of ‘setup.cmd’, we are ready to use the ‘rnn.py’ and ‘model\_predictor.py’ scripts in order to undertake experimentation.\\

\quad To replicate any of the experiment sets below, one must do the following:

\begin{enumerate}
	\item For a given experiment set, locate the necessary experiments within ‘RNN Results.xlsx’ that were used as part of the experiment set. For example, for experiment set 3 outlined below, we can see that experiments 13 to 39 were used for this set.
	\item Locate the necessary experiments within ‘RNN Results.xlsx’ (located at ‘$<$project directory$>$\textbackslash documentation \textbackslash RNN Results.xlsx’). In the above case, one would find the rows within the table that correspond to the column ‘Experiment Number’ ranging from 13 to 39. Each row represents a single experiment undertaken (e.g. a model built from a certain data set, with certain data preprocessing hyperparameters, etc.) and the script and argument combinations needed to run it in turn to obtain the results found in that row’s ‘Results’ column. Also note that we store the hyperparameter settings to the right of the ‘Results’ columns so one can ensure that they have the same hyperparameter settings as the results obtained below.
	\item For each of experiments in the relevant rows, ensure that each of the scripts and argument combinations are run. Note that they must be run in the order in which they are presented (i.e. the cell to the right of ‘Experiment Number’ must be run first, following by the next right cell, and so on). This is because many of the scripts rely on the results of the previous one in order to work (e.g. ‘comp\_stat\_vals.py’ must be run before ‘ft\_sel\_red.py’). Also note that these are not required to be run before every experiment, but rather is a requirement to \underline{have been} run beforehand. Hence, for experiments 13 to 39 we only need to run ‘python ext\_raw\_measures.py NSAA all all’ once, and not once per row (as this command just produces the raw measurement data and, while it is required to have been run for these experiments, it doesn’t need to be repeatedly called). However, for several of the other commands (e.g. ones concerning ‘rnn.py’), these must be run each time as they will very often differ at least slightly from the other experiments in the set and thus create different models (these differences are often what we are testing).
	\item Once the user has ensured the requisite setup script combinations have been run followed by the ‘rnn.py’ variation we need, the user will be presented with a console output. Ensuring that the user has the hyperparameters setup that matches the hyperparameter settings outlined in the rightmost columns, the user can then directly compare the results for the model in question to the relevant row that has just been run and, specifically, compare the console outputs to the output displayed in the ‘Results’ column. Alternatively, if one wishes to instead carry out their own experiment results, simply disregard the final step about comparing to the previously obtained results for a given experiment and instead append the results of the model, the hyperparameter settings, etc., to the end of the table, with an adequate explanation of the purpose of the experiment in the ‘Description’ column.
\end{enumerate}

\quad To replicate any of the model predictions sets below, one must do the following:

\begin{enumerate}
	\item As all model predictions sets have their ‘rnn.py’ and ‘model\_predictor.py’ steps all automated via batch scripts then, assuming the user has successfully run the entirety of the ‘setup.cmd’ script, one must simply launch and run the relevant batch script for the set. For example, if the user wishes to replicate model predictions set 3, one would simply run ‘model\_predictions\_set\_3.cmd’ which would run the required ‘rnn.py’ and ‘model\_predictor.py’ variants to setup the models and test them with complete files.
	\item The results obtained by the running of the ‘model\_predictor.py’ scripts via the batch script will be outputted as rows to ‘model\_predictions.csv’, with one row per subject prediction; these rows are written at the end of the table. To establish which of these final rows we are concerned with, note how many times the batch script has called ‘model\_predictor.py’: this will correspond to the number of rows at the end of the table we are concerned with for this specific model predictions set that we are replicating.
	\item Locate the previous rows within the table that signify the previous run of this model predictions set, the results of which we will discuss below. The user can do this by noting that the values in the ‘Short file name’ for the given rows will match those we just now obtained. Once the rows corresponding to the previously run iteration of the model predictions set has been found, we can directly compare the values held in each of the metrics columns (i.e. that include all columns except the first five). Doing so allows one to validate the results previously obtained and as discussed in detail below.
\end{enumerate}




%%%%%%%%%%%%%%%%%%
\section{Experiment Sets Table of Results (Drawn from ‘RNN Results.xlsx’)}

\quad The results contained within ‘RNN Results.xlsx’ can be seen below in a column-shortened form. Note that the experiment number in the table below corresponds to the same experiment number in ‘RNN Results.xlsx’. Hence, for more information about a single row in the table below (e.g. RNN model parameters used, the scripts and exact arguments used to obtain the results, etc.), please see the row in ‘RNN Results’ corresponding to the same experiment number. Note that ‘Data Shape’ is the total shape of the ‘x’ data that is used in the model and includes both the training and testing components. Also note the different parts of the shape: the first is the number of samples, the second number is the sequence length, and the third is the number of features of each sequence.

\begin{center}
\includegraphics[scale=1]{project_figures/fig10_3}
\end{center}

\begin{center}
\includegraphics[scale=1]{project_figures/fig10_4}
\end{center}

\begin{center}
\includegraphics[scale=1]{project_figures/fig10_5}
\end{center}

\begin{center}
\includegraphics[scale=1]{project_figures/fig10_6}
\end{center}






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Experiment Sets\\}


%%%%%%%%%%%%%%%%%%
\section{Experiment Set 1: Performance of RNNs on Different Source Data}

\begin{center}
\includegraphics[scale=0.4]{project_figures/fig10_7}
\end{center}

\quad The purpose of this experiment set is to determine whether and how well RNN models regress on different types of source data. By this, we mean data that is in the same format (i.e. source ‘.mat’ files in the same organizational structure) but representing different measurements from different source directories. These are thus:

\begin{itemize}
	\item \underline{\textit{'NSAA : AD'}} - Statistical values extracted from the ‘all-data’ (‘AD’) files by the ‘comp\_stat\_vals.py’ script, whose files are sourced from the ‘NSAA’ directory; hence these are stat values of the subjects performing the NSAA activities, which are then used to train model(s).
	\item \underline{\textit{‘direct\_csv : DC’ }} - These are the joint angle values (i.e. raw measurements, not computed stat values) that are sourced from the data cube; this data cube contains the 6-minute walk data from various subjects, several of which aren’t included in the standard joint angle files of subjects doing the 6-minute walk.
	\item \underline{\textit{‘direct\_csv : JA’}} - Similar to ‘direct\_csv : DC’ as described above, these use the same source directory type and raw measurement, but contain joint angle files that aren’t necessarily included within the data cube.
	\item \underline{\textit{'6minwalk-matfiles : AD'}} - Again, uses the files corresponding to subjects’ 6-minute walk assessments, with the difference this time being that we aren’t using raw joint angle files either in the data cube format or as ‘loose’ files, but rather computing statistical values via ‘comp\_stat\_vals.py’ script; in this sense, it’s the same as ‘NSAA : AD’ models but using different assessment data (6-minute walk rather than NSAA).
\end{itemize}

\quad For the second and third diagrams above, with regards to the output type, all these file types and directory sources outlined above are used to train models to regress on the overall NSAA score for a given sequence from a file. This value is able to range between 0 and 34 (though based on how the assessment is done, it generally ranges between 15 and 34). Hence, for a given type of source data, if it has a MAE = 0.5, that means that, on test data of sequences from files of the given type of source data, the model predicts for each of them a score of between 0 and 34 on average ‘0.5’ away from the true value for that sequence (the true value for a sequence being the overall NSAA score of the file that the sequence comes from). However, for the first diagram above, it tests the different files types on its ability to classify whether a sequence that come from a file are from a ‘D’ or ‘HC’ subject.


%%%%%%%%%
\subsection{Results Discussion}

\quad In using just the raw joint angle values from ‘DC’ or ‘JA’ files, we achieve an approximate \textbf{99\%} accuracy (i.e. for each sequence of 60 rows of 66 joint angle values, the model can predict with 99\% accuracy whether it came from a ‘D’ file or an ‘HC’ file); however, looking at more measurements (e.g. position, accelerometer values, etc.), performing manual feature extraction via computing of the statistical values and \underline{then} reducing the dimensionality, and then training the model provides a much worse classification accuracy of \textbf{82.81\%} for data that comes from the same assessment (6-minute walk). This can also be seen when the same data sources are then used to train the RNN to perform regression for the overall NSAA score: the raw joint angle data gives a much smaller \textbf{MAE = 0.4037} (meaning that it predicts a score of between 0 and 34 which on average is 0.4037 away from the true value in either direction), compared with a much worse \textbf{MAE = 3.56} from 6-minute walk ‘AD’ statistical value files. A further observation can be made about the experiments concerning the raw joint angle files in that they were performing much better than we were expecting them to be: by simply considering only the joint angle measurement of a subjects suit data, given 1 second’s worth of an input sequence to the RNN, it can correctly classify whether the frame comes from a healthy control subject or one with DMD to a very high accuracy of 99\% and predict the overall NSAA score to within 0.4037 of the true value of between 0 and 34. This is extraordinarily high, much better than the ability of medical professionals and, most notably, this is only the first iteration of the experiments with raw measurement files.\\

\quad This large difference between raw measurements and computed statistical features may seem counterintuitive at first glance, as the former is just using data direct from the provided ‘.mat’ files, while with the latter we actually process it further to ideally extract more important features from the data. One possible reason for raw joint angle models performing so well could be that there aren’t that many subjects with these files that are provided to us in comparison with ‘AD’ files, so not as diverse a training set is used; this means that there would be a narrower range of overall NSAA scores to regress towards, making it easier for the model to approximate the true score with a smaller margin of error when just using the joint angle data. Another possible reason could be that the features that the RNN models extracts from its inputted data are more useful for it to train on and approximate an overall score than manually crafted features from ‘comp\_stat\_vals.py’: a prominent benefit of using neural networks is that they are traditionally noted to perform better with raw data than manually extracted features. Furthermore, when it trains on the stat value data from ‘comp\_stat\_values.py’ for the ‘AD’ file types, it still computes its own features within the RNN, and so this ‘features from features’ behaviour might have proved to be problematic for the network.\\

\quad It should also be noted that training the RNN with raw joint angle data requires far fewer training epochs (~20) than for statistical values (~100). This will primarily be due to the larger amount of raw joint angle data that is fed through the network; in the case of training on all files within the data cube, the $x$ input shape is (8470, 60, 66) while for the corresponding ‘AD’ files we only have a shape of (552, 10, 30) due to how computing statistical values dramatically reduces the raw amount of available data. This decrease in the amount of available data also might help to account for the reduction in accuracy when using data sourced from ‘AD’ files.\\

\quad Overall, given our limited data and without any way to currently increase it, we come to several conclusions:

\begin{enumerate}
	\item RNN models can successfully regress and classify on data 6-minute walk and NSAA assessment data.
	\item Not only this, but raw joint angles perform phenomenally well in both classification and regression tasks.
	\item The raw data measurements might be better put through RNNs than extracted statistical features.
	\item This is probably due to extracted statistical features being a lot smaller of a data set, RNNs better utilizing raw measurements over pre-computed features, and there being a small range of target values with files sourced as ‘JA’ or ‘DC’.
\end{enumerate}

\quad We shall shortly be examining ways to improve the performance of models trained on computed statistical values by increasing sequence overlap, modifying sequence length, and so on.




%%%%%%%%%%%%%%%%%%
\section{Experiment Set 2: Performance of RNNs for Single Activity Classification}

\begin{center}
\includegraphics[scale=0.4]{project_figures/fig10_8}
\end{center}

\quad This experiment utilizes the same source data types as experiment set 1. Hence, we won’t discuss what each of these data types represent in the $x$-axes of the graphs above, as these are exactly the same files as used previously. The difference with this experiment set, however, is that it is looking at a different output type: while experiment set 1 looked at performance for classification of D/HC labels and overall NSAA scores for given test sequences, this experiment set looks at predicting multiple classification values for a single sequence. Here, the models are trained to predict a sequence of 17 values of numbers of either 0, 1, or 2 (i.e. the ‘acts’ output type). These represent the single activity scores for a single sequence that correspond to the single activity scores of the file that the sequence is sourced from. It’s worth noting that, as the overall NSAA score is the sum of these, a sequence will have an overall NSAA score that is equal to the sum of the same sequence’s 17 single act scores.\\

\quad Furthermore, when predicting single act scores, the RNN architecture will be different: for D/HC classification or overall NSAA score regression (‘dhc’ and ‘overall’ output types, respectively), there will be only 1 output neuron (though predicting different ranges of values for the two tasks), while for single acts there are 17 output neurons, 1 for each single act it is predicting. The overall aim of this experiment is thus to see if a model is able to predict, given a sequence of values from a file (that corresponds to a subject, e.g. ‘D4’), what that file’s single-act scores are. Note that the two metrics that we are using (that are computed by each RNN model over all its testing data) is individual activity accuracy and all activities accuracy: the former is the percentage of activities in the testing data set that were correctly predicted to be a 0, 1, or 2, while the latter is the percentage of whole activity sets (i.e. single RNN output of 17 values for a single test sequence) that were correctly predicted. 

%%%%%%%%%
\subsection{Results Discussion}

\quad In line with the results from experiment set 1, we can see here that raw joint angle data is much more useful in predicting the single act scores for test data of sequences than computed stat values from ‘AD’ files. This is most likely down to the same reasons as for the other output types; namely, comparatively small about of ‘AD’ data, neural networks better utilizing raw measurements, etc. Given that it is consistent with the previous experiment set results in determining the comparative performance for different source data types, the results of this experiment set is therefore more about whether it is possible for these data types to train a RNN to predict sequences of values that correspond to activities that the sequence being tested might not be from (i.e. the sequence will be from part of a file that, at most, corresponds to one activity the subject is performing, and therefore can only try to assess what its likely other single activity scores are). The result is that it performs well, especially with joint angle data, predicting approximately \textbf{99\%} of the individual activity scores and \textbf{98\%} of the all activity sequence scores.\\

\quad From this, we can draw several conclusions:

\begin{enumerate}
	\item The different types of source data can be used to train a model to accurately predict single act values.
	\item Like the previous two output types, raw joint angle measurements do this better than ‘AD’ stat values.
\end{enumerate}




%%%%%%%%%%%%%%%%%%
\section{Experiment Set 3: Raw Measurements for All Output Types}

\begin{center}
\includegraphics[scale=0.4]{project_figures/fig10_9}
\end{center}

\quad With this experiment set, we now turn our attention to looking exclusively at raw measurements. This is primarily due to the much better performance of models trained on raw measurements compared with ones trained on computed statistical values stat values from ‘AD’ files, as seen in experiment sets 1 and 2. The question we thus ask is: can we show similarly high performance when we train models on types of raw measurements from the suit data other than just joint angles? To this end, we look at 9 raw measurements in total that are contained within the ‘AD’ files and that are recorded by the suit during use: ‘position’, ‘velocity’, ‘acceleration’, ‘angularAcceleration’, ‘angularVelocity’, ‘sensorFreeAcceleration’, ‘sensorMagneticField’, ‘jointAngle’, and ‘jointAngleXZY’. Unlike the joint angle measurements, we don’t have the other raw measurements in separate, unique files. Therefore, we make extensive use of the ‘ext\_raw\_measures.py’ script in order to extract, for every file and for every raw measurement, the measurement data and store them in separate files as ‘.csv’ files. From here, these are then able to train and test an RNN model in the same way.\\

\quad For this experiment set, we are exclusively concerning ourselves now with NSAA assessment files rather than 6-minute walk files. This is more a choice based on the intended direction of the project to be more concerned with assessing and making predictions concerning the NSAA assessments and how that is connected to natural movement data (more on this later) than the 6-minute walk data. Thus, each of the entries along the $x$-axis of the graphs represents a single model trained on that particular raw measurement data for every NSAA file we have available. Currently, we are just looking at how the models performs on testing data from subjects it has already seen before, rather than complete subjects being left out of the training set, though this shall be explored later on with ‘left out’ subjects in trained models for various model predictions sets. Furthermore, though each of the models are trained with the same number of files and each with the same sequence length (due to the fact that at every time step the suit records all raw measurements), the feature size will vary; in other words, if the training shape to the RNN models are of shape ($x$, $y$, $z$), $x$ and $y$ will always be the same between raw measurements but $z$ will vary: for raw measurements based on segment measurements of the suit $z$ will be 69, while for angle-based measurements it will be 66 and for sensor readings it will be 51.


%%%%%%%%%
\subsection{Results Discussion}

\quad A consistent finding in all of the above graphs is that there are three raw measurements that are far and away better than the others: ‘jointAngle’, ‘jointAngleXZY’, and ‘sensorMagneticField’. These measurements can be seen to perform better for all three output types we are training towards: D/HC classification (graph 1), overall NSAA score (graphs 2 – 5), and individual activities classification (graphs 6 – 7). Both ‘jointAngle’ and ‘jointAngleXZY’ raw measurements were to be expected to perform this well, as this is consistent with results obtained in experiment sets 1 and 2 (note that this isn’t using exactly the same files, as experiment sets 1 and 2 use joint angle exclusive files that were pre-extracted before this project’s inception, while the models trained on ‘jointAngle’ here were instead measurements extracted from the ‘AD’ files and represent somewhat different subjects than the ‘JA’ or ‘DC’ files).\\

\quad Position is another measurement that performs particularly well compared with the other 5 measurements, though for some metrics (e.g. MAE for overall NSAA score) it performs noticeably worse than ‘jointAngle’, ‘jointAngleXZY’, and ‘sensorMagneticField’. We can observe that these 4 ‘useful’ measurements predict on average \textbf{~97\%} of test sequences accurately for D/HC classification, compared with only \textbf{~73\%} for the other 5. Furthermore, when we look at models trained for overall NSAA score regression, we note an average MAE of \textbf{~1.5} for the useful 4 measurements, while we see an average MAE of \textbf{~4.3} for the other 5; and for single-activity classification for the individual activity accuracy, we see \textbf{~97\%} for the useful 4 and \textbf{~74\%} for the other 5. With respect to the performance of the useful 4 measurements, while we expect to see this performance for ‘jointAngle’ and ‘jointAngleXZY’, the usefulness of ‘sensorMagneticField’ and ‘position’ is slightly more surprising. For position, we can speculate this to be as a result of subjects with more severe Duchenne being more inclined to have their limbs and torso in positions which are very indicative of their condition, compared with the healthy-control patients. The ‘sensorMagneticField’ measurement, however, is slightly more mystifying and does not have an obvious explanation at this point. What’s more, our initial speculation was that the velocity and acceleration measurements would be fairly useful to distinguish between D and HC subjects and, moreover, help predict their overall NSAA scores, as we believed that, as these are key measurements of a subject’s potential for movement, they would be useful for training an RNN model. We found these, however, to perform comparatively badly on test sequences, which leads us to believe that either velocities and acceleration of movement between subjects don’t vary that much or that they aren’t as indicative of movement ability as we thought.\\

\quad Therefore, the conclusions we draw are as follows:

\begin{enumerate}
	\item ‘jointAngle’, ‘jointAngleXZY’, ‘position’, and ‘sensorMagneticField’ are useful raw measurements for building RNN models on, while ‘velocity’, ‘acceleration’, ‘angularVelocity’, ‘angularAcceleration’, and ‘sensorFreeAcceleration’ are not.
	\item sensorMagneticField’ is surprisingly well performing (more or less just as good as the ‘jointAngle’ measurements), while ‘position’ is still strong but slightly worse, and ‘velocity’ and ‘acceleration’ aren’t as useful as initially thought.
\end{enumerate}




%%%%%%%%%%%%%%%%%%
\section{Experiment Set 4: Sequence Overlap for Stat Values from AD Files}

\begin{center}
\includegraphics[scale=0.4]{project_figures/fig10_10}
\end{center}

\quad One of the problems with stat values extracted from ‘AD’ files via the ‘comp\_stat\_vals.py’ script was that it dramatically reduced the amount of available data we have for training: when we are computing the stat values over intervals of 1 second (the standard measurement used for this project thus far), we are essentially doing calculations over 60 rows of raw measurement data (due to the sampling rate of the suit being 60Hz and we chose 1 second’s worth of data) and producing as an output 1 row of data. While this theoretically contains much of the useful information of the 60 rows simply condensed into 1 row, it doesn’t change the fact that we have now reduced the amount of raw data that we feed into the RNN 60-fold. This is most likely a large factor in the comparatively-weak results of computed stat values seen in experiment sets 1 and 2. However, a way we chosen to get around this is by using a sequence overlap.\\

\quad Sequence overlap is essentially used here as follows: consider a 2D block of data that we have available that is produced by the ‘comp\_stat\_vals.py’ script and ‘ft\_sel\_red.py’ (to reduce the dimensionality of the data). This block has a number of rows equal to the number of produced rows of statistical value data over all files that we are using (e.g. all NSAA files available in the directory) and a number of columns equal to the reduced dimensionality produced by ‘ft\_sel\_red.py’ (e.g. from ~4000 from ‘comp\_stat\_vals.py’ to ~30 to be read in by ‘rnn.py’). Normally, we will take ‘slices’ of this block to produce smaller blocks with a number of rows now equal to ‘sequence length’ before moving on to the next block below it until all data is consumed (ignoring leftover data at the end of the block that won’t fit into a smaller block). This is the case with the diagram below on the left.

\begin{center}
\includegraphics[scale=0.2]{project_figures/fig10_11}
\end{center}

\quad However, if we consider a sequence overlap of 50\% (i.e. overlap proportion of 0.5), we are instead able to capture 7 blocks of data rather than 4. Scaled up to smaller sequence lengths relative to the number of data samples, this is approximately 50\% more data produced by ‘comp\_stat\_vals.py’; if we scale up to an overlap of 0.9, we have 900\% more data. Though we end up with a fair bit of redundancy with this approach (as the same vector of a data sample is used in numerous sequences), there are two primary benefits of doing this:

\begin{enumerate}
	\item Much more available data, which is useful to train the models to become much more accurate.
	\item With sequences that have no overlap, there is a high chance that one or more activities that occur in the NSAA assessment might be ‘cut’ along these lines (see the dashed lines in the above left image); this would mean the activity isn’t included in an entire sequence, while with a sufficient sequence length and sequence overlap, it’s more likely to be captured in its complete form in at least one of the sequences.
\end{enumerate}



%%%%%%%%%
\subsection{Results Discussion}

\quad Note that for this experiment set, we are only concerned with the overall NSAA score regression output type, as previous results show the performance of models on this output type are generally very consistent with results on other output types, hence there isn’t expected to be any need to repeat these experiments with the other output types here. Unsurprisingly, we see a massive increase in the performance with a larger sequence overlap. This will be in part due to the massive increase in available data: while the total data for NSAA files from stat values produced by ‘comp\_stat\_vals.py’ for a sequence overlap of 0 is (742, 10, 30) and shows a \textbf{MAE = ~2.9}, when the sequence overlap increases to 0.9 we have total data of shape (7420, 10, 30), ten times as many available sequences, which results in a \textbf{MAE = ~0.1}. Note that these results are still just on test data, so it shows that the models can generalize much better to new, unseen sequences when the overlap is much higher.\\

\quad We can therefore conclude two things:

\begin{enumerate}
	\item A large sequence overlap leads to better performance for computed stat values from ‘AD’ files for NSAA files.
	\item This is most likely due to the larger amount of available data and the increased window to capture complete activities.
\end{enumerate}




%%%%%%%%%%%%%%%%%%
\section{Experiment Set 5: Sequence Overlap for Stat Values from AD Files}

\begin{center}
\includegraphics[scale=0.4]{project_figures/fig10_12}
\end{center}

\quad This is the first experiment set that makes use of ‘mat\_act\_div.py’ to use the source ‘.mat’ files and the annotated Google sheet to experiment with what we call ‘single-act’ files. These are files that contain only a single NSAA assessment within it and no other activities. For example, if we take the ‘D4’ source ‘.mat’ file, we then create 17 new files containing the 17 NSAA assessments, with each new file being a name like ‘D4\_act1.mat’, ‘D4\_act2.mat’, etc. This process obviously contains files that are a lot smaller than the source file, as well as cutting out a lot of the bits in between the assessments in the original file. For example, the original file will contain a complete recording of the suit data for a subject; this includes the data where a subject isn’t doing anything particularly relevant (e.g. standing around before beginning the next activity of the assessment or trying but failing to do one of the assessments). For more details on how this is done, see the section within the ‘Script Ecosystem Overview’ chapter on ‘mat\_act\_div.py’.\\

\quad The aim of this experiment was to build models that are trained on one type of raw measurement. This is similar to experiment set 3 but with two differences: we’re now assessing on the single activity score for the file in question (i.e. a value of 0, 1, or 2), and we’re now using files that contain a single activity from a subject rather than all of the assessments (in most cases, though some smaller source files exist that contain only a handful of activities). We wanted to see whether or not the same raw measurements were as comparatively useful for single act source files as they were for the original files that contained all of the data; in a sense, we wanted to see if the measurements were as consistent when we removed a lot of the ‘non-assessment’ data from the source files (that is, data within the source files that don’t have an activity being undertaken). If this was the case, then we can conclude fairly conclusively that these raw measurements are informative for NSAA assessments. Also, it’s worth noting that the ‘y’ labels assigned to each sequence coming from a ‘single-act’ file is now based on not only the file name but also what activity it represents; these two features are used to lookup the relevant value from the annotated Google sheet to get the assessed value of the activity.


%%%%%%%%%
\subsection{Results Discussion}

\quad As expected, the raw measurements that proved to be the ‘best’ in experiment set 3 (‘jointAngle’, ‘jointAngleXZY’, ‘sensorMagneticField’, and ‘position’) proved to be the best here as well, with the other 5 raw measurements heavily underperformed in comparison. This can be seen by the ‘useful’ 4 raw measurements obtaining a \textbf{MAE = ~0.15} while the others obtaining a \textbf{MAE = ~0.37}. Hence, this performs as we would expect, and the removal of a lot of the data between assessments from the source files does not affect the relative importance of some of the measurements. We can therefore conclude the following:

\begin{enumerate}
	\item The relative importance of ‘jointAngle’, ‘jointAngleXZY’, ‘sensorMagneticField’, and ‘position’ as raw measurements compared to the other 5 is consistent with using just 'single-act' files rather than the complete source files.
\end{enumerate}





%%%%%%%%%%%%%%%%%%
\section{Experiment Set 6: Different Sequence Lengths w/ Overlaps for Raw Measurements}

\begin{center}
\includegraphics[scale=0.4]{project_figures/fig10_13}
\end{center}

\quad For raw measurements, we have been (up until now) using a sequence length of ‘60’ as default for all of the experiment sets thus far. This represents a time window for the sequence of 1 second, as the suit is sampling at 60Hz, which therefore means that there are 60 rows of data stored in the source ‘.mat’ files every 1 second. Therefore, we wish to know whether or not this is a good time window to create each sequence with; however, with a given amount of source data, if we increase the sequence length, then we decrease the number of overall samples we can take from this finite block. Therefore, it was decided to use a scaling sequence overlap; that is, we increase the sequence overlap in proportion to the sequence length to keep the number of samples more-or-less constant (accounting for differing numbers of left-over data at the ends of every file that can’t be made into a sequence). This is done by the following formula: $y=1-\frac{1}{x}$, where $y=sequence\, overlap$ and $x=proportion\, of\, original\, sequence\, length$.\\

\quad For example, given our original sequence length of ‘60’, if we wish to experiment with a sequence length of ‘180’, then $x=3$ (as 60*3=180) and therefore $y=0.6667$; that is to say, we must set the sequence overlap proportion to 0.6667 to maintain a constant amount of samples (i.e. number of sequences) if we want a sequence length of ‘180’. The reason we do this is to hold the dataset set size as a constant, and therefore any changes in performance of differing sequence lengths would be due to sequence length alone. Also note that we again only assess the differing sequence length on the overall NSAA score output type (for reasons previously mentioned in the experiment set 5 discussion). Finally, we wish to explore differing sequence lengths on multiple different types of raw measurements and compared them side by side (hence why we plot them all together on the same graph); however, we exclude 5 of the less ‘useful’ measurements, as they’ve been determined in experiment sets 3 and 5 to be not as useful as the other 4; hence to save on time and graph readability, we don’t consider them here.

%%%%%%%%%
\subsection{Results Discussion}

\quad What we were looking for in the above graphs is a consistent pattern for a given graph among the four plotting lines of an increase or decrease in performance when we change the sequence length; in other words, whether performance increases or decreases in the same way for all raw measurements for a given sequence length. This would give us a strong indication of how sequence length alters the performance of a certain metric. However, not only can we not see any inter-raw-measurements patterns of change (i.e. patterns of a line changing over time) but we can’t see any discernible intra-raw-measurements pattern (i.e. can’t see any whole single lines changing particularly strongly with respect to the output metric and consistently with the other single lines). For example, looking at the graph for the MAE metric, none of the lines for any of the raw measurements show any consistent improvement or worsening with increasing sequence length; each of them improves at some point on increasing the sequence length and also worsens at some point on increasing the sequence length. Additionally, the lines aren’t consistent with each other: the improving or worsening performance with increasing the sequence length is often exclusive to one measurement and doesn’t necessarily translate to the others. From this, we can conclude:

\begin{enumerate}
	\item Increasing sequence length for data from raw measurements beyond 60 doesn’t show a consistent increase or decrease of performance amongst different useful raw measurement types; this is most likely due to the capturing window of 1 second being long enough to correctly assess for a given output type.
	\item To save on computational cost of training and testing on unnecessarily-longer sequences, we decide to keep the sequence length of 60 for the time being.
\end{enumerate}




%%%%%%%%%%%%%%%%%%
\section{Experiment Set 7: Number of Features Needed for Stat Value Data}

\begin{center}
\includegraphics[scale=0.35]{project_figures/fig10_14}
\end{center}

\quad We now turn our attention back to stat value data from ‘AD’ files (i.e. the output of ‘comp\_stat\_vals.py’ and ‘ft\_sel\_red.py’ as opposed to raw measurements from files obtained by ‘ext\_raw\_measures.py’). As a requirement of the pipeline, the data produced by ‘comp\_stat\_vals.py’ needs to have its dimensions dramatically reduced while keeping as much of the inherent variation of the data set still present. In the context of the data block diagram as seen in the section for experiment set 4, this is the ‘width’ of the block. As sequences are extracted from the source data ‘block’ by moving downwards, the number of features we chose to set is independent of the sequence length, sequence overlap, and discard proportion (more on this later), and so doesn’t affect the number of samples used in any of the experiment sets, including this one. The intention here, therefore, is to experiment with the making of a different data shape to feed into the RNN models (e.g. from (7420, 10, 30) to (7420, 10, 50)) and see how the performance of the models tested on the test set differs from the initial data shape.\\

\quad We have, up until now, been using ‘30’ as the default number of features per frame and per sequence. This is due to 30 features being the number of features that generally included over 99\% of the variance when using the ‘variance threshold’; hence, this was a good default number of features to use for stat value files for any of the choices of feature reduction techniques. Also, it should be noted that the above results are for when the choice of feature reduction technique for ‘ft\_sel\_red.py’ was ‘pca’; that is, every result above (and also for every previous experiment conducted) has used PCA (principal component analysis) to reduce the dimensions of outputs of ‘comp\_stat\_vals.py’ to a smaller value. Further exploration of different feature selection/reduction is possibly extension work for this project, though PCA was chosen here as the default feature selection/reduction technique based on success seen in similar studies.

%%%%%%%%%
\subsection{Results Discussion}

\quad Note that the above graphs are just for a single output type (the overall NSAA score) for reasons previously outlined and we are only interested in feature reduction for the statistical values from ‘comp\_stat\_vals.py’. From observing the above graphs, we can see a dramatic reduction in performance when we reduce the number of features from 30 to 10: we go from \textbf{MAE = ~0.1} to \textbf{MAE = ~0.17}, an increase in error of approximately 70\%. This is most likely due to 10 features being not enough for PCA to capture the vast amount of the variance within the data it is given; hence, when we feed this data into RNN models, it is not as able to make accurate predictions of overall NSAA scores because it doesn’t have as complete a picture as if it’s using 30 features. Alternatively, if we increase the number of features from 30 to 50, we can see a notable worsening of performance (from \textbf{MAE = ~0.1} to \textbf{MAE = ~0.13}); hence, even though by using 50 features and therefore capturing more of the inherent variance and therefore ‘characteristics’ of the original data from ‘comp\_stat\_vals.py’, this is evidently not enough to overcome the consequences of using higher-dimensional data for sequences to overcome the effects of the ‘curse of dimensionality’. Therefore, the following conclusions can be drawn:

\begin{enumerate}
	\item We shall continue with number of features to reduce to from data produced by ‘comp\_stat\_vals.py’ to 30.
	\item This is the ideal ‘middle ground’ between sequences that don’t capture enough of the variance (e.g. number of features = 10) and sequences that have too many features to effectively train on (e.g. number of features = 50).
\end{enumerate}



%%%%%%%%%%%%%%%%%%
\section{Experiment Set 8: Larger Sequence Lengths w/ Overlaps for Stat Values from ‘AD’ Files}

\begin{center}
\includegraphics[scale=0.35]{project_figures/fig10_15}
\end{center}

\quad In the same way that we looked at increasing the sequence lengths used for raw measurements in experiment set 6, we now look at the same thing but from computed statistical values of source files by ‘comp\_stat\_vals.py’ and subsequently reduced to ‘30’ features (as determined by experiment set 7). This time, however, we look at the effects of scaling the sequence overlap with sequence length (as done in experiment set 6) compared with not scaling the sequence overlap. We start with a sequence overlap of ‘0.9’ as standard (the value of which was determined by experiment set 4) for the initial default sequence length of 10. Note that the initial sequence length of 10 was chosen due to the initial lack of data available to use before the use of sequence overlaps. Also, it’s worth pointing out that this length of 10 corresponds to a sequence capture window of 10 seconds (this is due to every ‘row’ of stat value data representing 1 second’s worth of data due to them being calculated over 60 rows of raw data sampled at 60Hz from the suit), unlike the raw data whose sequence length of 60 corresponds to a sequence capture window of 1 second. We also again assess only on the overall NSAA score output type for reasons previously discussed.\\

\quad The primarily purpose of this experiment set is to see if the performance of the models improved with a longer sequence length for computed statistical values of files. However, the secondary purpose is to determine if a scaling overlap improves the performance of the model, as was previously assumed to do so for experiment set 6. This scaling overlap keeps the number of samples as a constant, as opposed to it decreasing due to the longer sequence lengths capturing more of the data per sequence. The experiments that have a scaling sequence overlap to keep the number of samples more-or-less constant are represented by the orange lines in the above graphs, while the ones with no scaling sequence overlap (and thus a progressively reducing number of overall samples) are represented by the blue lines.


%%%%%%%%%
\subsection{Results Discussion}

\quad We can see that in both scenarios where we either have a scaling sequence overlap or don’t, the performance of the model decreases with increasing sequence length. This is also significant because, for raw measurements, we can see that sequences are quite able to use longer sequence lengths and that ‘60’ seems to be an ideal point for these measurements. Therefore, from a smaller sequence length being more ideal for computed statistical values, we can conclude that this is a result of each row of data for computed statistical values containing more contextual data than a single row of raw measurement data, as opposed to it being down to the ideal data shape for an RNN to learn from (which is not the case as an RNN can learn quite well for a sequence length of ‘60’, as seen in experiment set 6). This is most likely due to the data contextual window growing too large. For example, when we increase the sequence length to 50 here, that means that each sequence takes in 50 seconds worth of source data, which most likely isn’t able to account for the minutia of details that differentiates sequences of different classifications, overall NSAA scores, etc. Additionally, our reasoning of using a scaling sequence overlap in here and in experiment set 6 is justified, as for every metric above, the increased sequence length performs better with a scaling sequence overlap, as opposed to having a static sequence overlap of ‘0.9’, though in both cases it is still not as good as keeping the sequence length to 10. Therefore, we can conclude the following:

\begin{enumerate}
	\item Increasing the sequence length beyond 10 is not ideal for computed statistical values, which shows that a contextual window of beyond 10 seconds makes learning for an RNN increasingly difficult.
	\item The use of a scaling sequence overlap shows better results than not using a scaling sequence overlap.
\end{enumerate}



%%%%%%%%%%%%%%%%%%
\section{Experiment Set 9: Smaller Sequence Lengths w/ Overlaps for Stat Values from ‘AD’ Files}

\begin{center}
\includegraphics[scale=0.35]{project_figures/fig10_16}
\end{center}

\quad Before moving onto the examining of larger sequence overlaps for raw measurements as opposed to just for extracted statistical values (as in experiment set 8), it’s necessary to ensure that a sequence length of 10 is indeed ideal for our setup, as the previous experiment set only concurred that a sequence length of 10 or larger could be the ideal length. Here, we look at these smaller sequence lengths, going down to a sequence length of 3 (i.e. three sequences of vectors of 30 numbers are fed into the RNN which represents a context windows of 3 seconds). The expectation prior to undertaking these experiments was that a sequence length of 3 would be too short to draw time-contextual inferences from by the RNN when learning, while it was felt that 3 seconds might be too short to make accurate predictions for D/HC classifications, overall NSAA scores, etc., for many sequences. Note that everything else remains as it was in experiment set 8 (i.e. same source directory, same output type for the models, etc.), with the above graphs showing the results when we are using a scaling sequence overlap to keep the number of samples as a relative constant (i.e. decreasing the overlap as we decrease the sequence length), which corresponds to the orange lines in the graphs in experiment set 8.

%%%%%%%%%
\subsection{Results Discussion}

\quad As we can see in the above graphs, the results evidently concur with our predictions: the performance for the overall NSAA score output type for models trained on extracted statistical values peak at a sequence length of around 10, with decreasing performance shown when we lower this value while keeping the number of samples relatively constant. As mentioned previously, this is most likely due to an RNN generally thriving on sequences of longer than 3 to get time-contextual information along with a context window of 3 seconds being not as useful as a 10 second window for NSAA scores. Furthermore, this knowledge that 10 seconds of context window for NSAA activity sequences as generally most useful will carry us over into the following experiment, where we shall attempt to use a similar context window with raw measurement data to see if we get comparable results. Therefore, we can conclude the following:

\begin{enumerate}
	\item Smaller sequence lengths for models built on extracted statistical values show worse performance, and therefore a sequence length of 10 for these sequences is ideal.
\end{enumerate}



%%%%%%%%%%%%%%%%%%
\section{Experiment Set 10: Very Large Sequence Lengths for Raw Measures w/ Discard Proportion}

\begin{center}
\includegraphics[scale=0.35]{project_figures/fig10_17}
\end{center}

\quad Having established that a 10 second contextual window was ideal for extracted statistical values, we now wish to see if this is something that is exclusive to computed statistical values from ‘comp\_stat\_vals.py’ or whether it exists for raw measurements as well; if it does, then the 10 second context window is more likely to be an inherent characteristic of sequences made from NSAA assessment files, regardless of what measurement types are drawn from these files. For raw measurements, each row of data that is going into the RNN represents 1/60th of a second, as was previously established due to the sampling rate of the suit producing the data being 60Hz. Therefore, to get 10 seconds worth of data from raw measurements, we would need 600 rows of data (i.e. a sequence length of 600) while computed statistical values only need a sequence length of 10. However, setting the sequence length to 600 for raw measurements presents us with two problems:

\begin{enumerate}
	\item The increasing of the sequence length for raw measurement data from 60 (as we have used as the default up until this point) to 600 will in turn reduce the data 10-fold due to there being far more of the data needed for each of the sequences.
	\item A sequence of several hundred has been previously established to be difficult for an RNN model to train on, along with being much more computational demanding and containing a lot of possibly redundant data.
\end{enumerate}

\quad The first problem is solved by using a sequence overlap of 0.9 (for increasing from a sequence length of 60 to 600); this keeps the number of samples at a relative constant as we increase the sequence size. This ensures that any changes that occur in performance will be down to just sequence length and not the number of samples used; see experiment set 4 for a more in-depth discussion of this. This allows us to change the data shape from (13365, 60, 66) to (13365, 600, 66). However, this does not solve the other problem of there being too-long sequences to feasibly train the RNN model on. What we want is a way for an RNN model to gain a longer contextual window (e.g. of 10 seconds, corresponding traditionally to a sequence length of 600) without lengthening the sequence length itself. In this sense, any increase in performance while increasing the context window will be entirely down to increasing the RNN’s contextual information for a given sequence and not the actual sequence length. This is done by the using ‘--discard\_prop’ optional argument supplied to ‘rnn.py’.\\

\quad What this does is fairly simple: for each sequence, once it has been extracted from the source data block (also accounting for sequence overlap if appropriate), we keep only every ‘nth’ line of the sequence. For example, assume we have a sequence of length 600 and have set the ‘--discard\_prop’ argument to 0.9. This means that 90\% of the data of the sequence is discarded, which results in keeping every 10th line in the sequence. This achieves two things:

\begin{enumerate}
	\item Discards 540 rows and we are left with 60 rows of data which, as a result of taking the ‘kept’ rows at even increments along the sequence, means that we still have context covering the 600 rows.
	\item By having even sampling of ‘kept’ rows of data, we also limit the possibility of missing any ‘important’ pieces of data that existing within the original 600 rows, as it’s likely that 1 or more of the kept lines would have captured some of this information.
\end{enumerate}

\quad We therefore use this technique (along with a corresponding sequence overlap) to allow us to experiment with much larger sequence lengths (up to 9600) with corresponding discard proportions to keep the actual sequence lengths (i.e. the number of frames per sequence going through the model) as a constant; these are shown by the orange lines in the graphs above. We also look at not using the discard proportions for increasing sequence lengths, though we can only go up to a certain sequence length before computational limits restrict us from continuing; for example, training models with data of a sequence length of 9600 with no discard proportion was expected to take between 3 and 4 days to train the model with the same number of epochs as the previous ones.\\

\quad It’s worth noting that the sequence length is scaling for all experiments done here so the number of samples is always kept relatively constant. This is true for both the blue and orange lines above, as in both scenarios with a sequence length of 120, the discard proportion is set to 0.5. Finally, we only use one raw measurement here (joint angles) to experiment with longer sequence lengths. This is mainly due to previous experiment sets showing us that the performance of the useful 4 raw measurements are almost always completely in-line with each other with respect to increases or decreases in performances with changing of independent variables. Therefore, to save on expensive experimentation time, we chose to evaluate the performance of models on only one raw measurement, safe in the knowledge that other raw measurements will most likely see the same change in performance.


%%%%%%%%%
\subsection{Results Discussion}

\quad One thing we can see from the graphs above is the noticeable increase in performance (e.g. the lowering of MAE on the test set) up to a sequence length of approximately 600. This is the point on the graph of the MAE metric where the discard proportion line shows an upwards inflection afterwards (i.e. when going up to a sequence length of 1200, the MAE gets higher); furthermore, past this point in the graph, the improvements are comparatively minimal for the extra time it takes to pre-processing the data. With very large sequence lengths and corresponding discard proportions, even though the amount of data that is input into the RNN models is constant, it takes longer and longer to prepare the data. Therefore, a compromise was reached in the \textbf{sequence length of 600} being the best choice given the proportional discard proportion and sequence overlap for how long it takes the model to compute.\\

\quad This is also a good length to have as it means the ‘ideal’ (in the sense of model performance and time needed to compute) context window for raw measurements with a high discard proportion and scaling sequence overlap is 10 seconds, which is the same as for computed statistical values. Another thing to note is that, for sequence lengths where we have results for both the blue and orange lines in the graphs, we see that the orange line (representing using a discard proportion) shows better results. This shows that, given the same sequence overlaps, when using a discard proportion to reduce the sequence length that is used to train the models we see better model performance; this is most likely due to RNN models not performing well for sequences of several hundred or longer. We can therefore conclude the following:

\begin{enumerate}
	\item The performance of models increases with higher sequence length and scaling discard proportion, with a good compromise of performance and computing cost being a sequence length of 600, a sequence overlap of 0.9, and a discard proportion of 0, which gives an ‘actual’ sequence length seen by the model of 60.
	\item The ideal context window of 10 seconds for extracted statistical values is more-or-less replicated here, with the 10 second context window (sequence length of 600) being a high-performance parameter setting, and therefore 10 seconds is a good window for data from NSAA files in any measurement form.
	\item Using discard proportion over not using it for identical sequence lengths shows better performance, which indicates that RNN models struggle to perform on sequences of too-high length.
\end{enumerate}











%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Model Predictions Sets\\}



%%%%%%%%%%%%%%%%%%
\section{Model Predictions Set 1: Natural Movement Files on Models Built on NSAA and Walk Files}

\begin{center}
\includegraphics[scale=0.5]{project_figures/fig11_1}
\end{center}

\begin{center}
\includegraphics[scale=0.5]{project_figures/fig11_2}
\end{center}

\quad With the models now built with many of the settings that we will most likely be sticking with for the duration of the project (i.e. with an ideal sequence length, overlap, discard proportion, the best raw measurements to use, etc.) ascertained by the previous experiment sets, we now move onto using these along with ‘model\_predictor.py’ to predict on whole files. Previously, in all experiment sets, the data that was reported was testing sequences from a mixture of the source files used to build the data set. This meant that the assessment was done on a sequence-by-sequence basis, where the assessment of each sequence was to be independent of each other. With ‘model\_predictor.py’, this is quite different: we instead provide a specific name for the file, which will use all of the data to do an assessment on (the difference being that for the previous experiment sets, there will be a mixture of target D/HC classifications, overall NSAA scores, etc. for the test sequences as they came from multiple different files, whereas with ‘model\_predictor.py’ there is a D/HC classification, overall NSAA score, etc., that is common among all of the sequences within the source file.\\

\quad For making predictions on whole files via ‘model\_predictor.py’, the broad sequence of steps is as follows:

\begin{enumerate}
	\item Split the source file into sequences, each with the same $y$ labels (as the same file has the same $y$ labels attached to it for each of its sequences).
	\item Assess each of these sequences on each of the models that we wish to be using, with a model for each measurement type and each output type (e.g. if we are assessing on 4 measurement types and 3 output types, there will be 12 models that each sequence is assessed on).
	\item For each of the output types, average the response over all sequences for a given measurement and then average these over all measurements to get a single prediction for the given output type.
\end{enumerate}

\quad Each assessment of a file made by ‘model\_predictor.py’ is then stored as a single row of results within ‘model\_predictions.csv’. This is what’s used for the following model predictions sets.\\

\quad For the first set, we are concerned with every file that we currently have of subjects as the ‘allmatfiles’ data set involved in what we call ‘natural movement behaviour’; that is, data captured by the suit of the subject doing activities that aren’t NSAA or 6-minute walk assessments, such as playing or eating. This amounts to over 400 files, with as many as 30 files for a single subject captured. Hence, for each of these files, we run it through ‘model\_predictor.py’ to make various predictions for different output types. This is helped by the use of ‘test\_altdirs.py’, which automates a lot of the process and doesn’t require the user to manually run ‘model\_predictor.py’ $>$400 times.\\

\quad While we shall get to assessing NSAA files on models built on NSAA files in the next model predictions set, here we wish to look at how well models that are built and tuned on NSAA files perform when presented with natural movement behaviour files. Obviously, the models have never seen any part of the files before (as the assessing files are from a different data set as those used to train the models), and the data it is now assessing on does not necessarily have characteristics of the original NSAA assessment files; for instance, the models will have been trained on sequences from files that do specific activities (i.e. one of the 17 NSAA assessment activities) or contain the subject walking. However, the natural movement behaviour will most likely be of movement characteristics that it’s never seen, and so the models are required to generalize their knowledge to other types of movement.


%%%%%%%%%
\subsection{Results Discussion}

%%%%%
\subsubsection{Console Output}

\quad The following covers the model predictions of rows 1 to 436 of ‘model\_predictions.csv; for information on a prediction-by-prediction basis, see these rows. Instead of showing the results contained in these rows individually, however, we instead computed several statistics from these rows for certain columns with the aim to provide an insight into how well models build on NSAA and 6-minute walk files predict on natural behaviour files. It’s also worth noting that, at present, the natural movement behaviour files only exist as raw joint angle files. Hence, the only raw measurement type of model we can use are the ones built on joint angle data (unlike predictions on NSAA files we shall discuss later, which uses 5 total input types for models).\\

\quad The first two console outputs as seen above takes the average of the difference between each file’s true value for overall NSAA score and predicted value. This is done by, for each relevant row of ‘model\_predictions.csv’, finding the absolute difference between the true and predicted overall NSAA value columns; this gives a measure of how well the model predicted the overall NSAA score for that file, with this difference being 0 if it predicted the correct score; this is then averaged over all the relevant rows. What we can see is that models built on NSAA files that predict natural behaviour files predict a file on average within \textbf{4.66} of its true value, whereas models built on 6-minute walk files predict within \textbf{5.03} of its true value. These are reasonably strong initial results, though are possibly slightly skewed given the prevalence of files within the 25 to 34 score range. This is also including every single natural behaviour file available, and so the scores might be negatively impacted by natural movement files that are more-or-less not possible to infer an exact score (e.g. a file of a subject sitting very still for long periods).\\

\quad The second two console outputs are the percentage of the total number of file assessments made that have the correct D/HC predicted classification. For each of the files, ‘model\_predictor.py’ takes the most common ‘D’ or ‘HC’ label for the sequences for the D/HC output type and makes that the prediction for the file; for example, for a file that contains 100 sequences, if 60 of them are predicted as being from a ‘D’ file and 40 are predicted as being from an ‘HC’ file, then the overall prediction for the file is a ‘D’ label. We also have a ‘true’ D/HC classification for the file; this is just based on which of the two the file’s name begins with. Hence, the percentage of correctly predicted D/HC labels is the percentage of rows in ‘model\_predictions.csv’ that we are concerned with that have a predicted D/HC label that matches the true D/HC label. The results of \textbf{75.92\%} and \textbf{74.54\%} shown that in a majority of cases, natural movement files assessed on models built on either NSAA or 6-minute walk files get a correct classification; this is a notable good result that indicates, with further refinement and tuning, that these models would identify quite accurately what type of classification the subject is by simply observing natural movement data, even if the models were trained instead on NSAA files.\\

\quad With the third set of console outputs, we are still concerned with analysing the D/HC classification predictions. Unlike the second set, for each file we aren’t concerned with the single estimated classification of the file by the models; rather, we look at it on a sequence by sequence basis in that we want to see the percentage of correctly classified sequences for the file being tested on. We then compute the ‘percentage of predicted wrong sequences’; this is simply the difference of the percentage of correctly classified sequences and 100\%. For example, for ‘allmatfiles\textbackslash D10-001’, we have 77.81\% sequences predicted as being of ‘D’ label and 22.19\% predicted as being of a ‘HC’ label. Since they are all supposed to be ‘D’ sequences (since they all came from a ‘D’ file), it therefore got 22.19\% of the sequences wrong: this is the ‘percentage predicted wrong sequences’. We then repeat this over all other files from the natural movement behaviour data set and find the MAE of this set. This results in an error percentage of \textbf{28.72\%} on models built from NSAA files and \textbf{29.28\%} on models built from 6-minute walk files. This is fairly similar to the second set of console outputs in that it shows the models predict correct classes the majority of the time, though ideally this will approach 0\% for both model types (NSAA and 6-minute walk) as we continue to refine and improve the system.\\

\quad The final set of console outputs are more straightforward. For every assessed file (i.e. row we are concerned with in ‘model\_predictions.csv’), we get the value contained within the ‘Percent of acts correctly predicted’. This looks at the single act predictions for that file (i.e. a list of 17 values between 0 and 2 that the model believes are the correct single act scores for that file) and sees how many of those it got correct with respect to their true values; this then manifests as this ‘percent of acts correctly predicted’ score. We then repeat this over all the natural movement behaviour files and average these values to find the scores that appear on the fourth set of console outputs. The values of \textbf{80.84\%} and \textbf{75.28\%} are quite impressive: this means that on average the models that are trained on NSAA files predict ~13.7 of the 17 activities with the correct score, while the models trained on 6-minute walk files on average predict ~12.8 of the 17 activities. It’s also noted that particularly accurate scores for certain rows generally correspond with an overall NSAA prediction that is quite close to its true value, which makes sense as they are both involved with ascertaining NSAA scores; it’s just that the latter is predicting cumulative scores.

%%%%%
\subsubsection{Graph Output}

\quad Along with the console output that is produced by ‘graph\_creator.py’, as discussed above, we also have several graphs that have been produced. However, in comparison to previous experiment sets, it is somewhat harder to ascertain specific results from these graphs, as these contain ~400 points and therefore must rely on the more evident ‘trends’ shown within the graphs. The first two graphs show the true overall NSAA scores for each natural movement behaviour file along the $x$-axis and the files’ corresponding predicted overall NSAA scores along the $y$-axis by the NSAA models and 6-minute walk models. The closer these points are to the $y=x$ line projected through the middle of the plot the better, as this means they are closer to their true predicted values. From these graphs, we can see a tendency for points to hover around this line; however, there is still a great deal of variation around these areas. We can also see the model particularly struggles when presented with files that have an overall score of ‘3’; this is most likely due to these files being from an ‘outlier’ subject (due to the subject not completing many of the activities, which resulted in a lower score than they most likely should have). Additionally, we can see that files that have an overall score of ‘34’ (i.e. being from an ‘HC’ subject) are often assigned a score a fair bit lower of between 25 and 30. These observations from the two graphs for both types of models therefore give us good guidance of where to focus on improving the models.\\

\quad The next two graphs show the distribution of the percentage of correctly predicted sequence D/HC labels for every natural movement behaviour file we tested on. This is essentially looking at either the ‘Percentage of predicted ‘D’ sequences’ or ‘Percentage of predicted ‘HC’ sequences’ for each file (depending on whether the file is a ‘D’ or ‘HC’ file). We then plot the distribution of these percentages for each of the file predictions made for both types of models (NSAA or 6-minute walk), along with plotting the cumulative distribution lines. We can see from these graphs that there is high number of files where 100\% of its sequences were predicted with the correct label, which is doubly impressive given the models these files are testing on have never seen natural movement behaviour before. It also shows us that, in both model types cases, there is a wide distribution of scores, with a not insignificant number of files having fewer than 50\% of their sequences correctly classified (~100 files in both cases out of ~400 total). These graphs highlight that, while we have seen fairly positive results for the classification as shown in the second set of console outputs, there is still a lot of room for improvement.\\

\quad The final two graphs again show a distribution of files. This time, however, we are looking at the distribution of percentages of correctly predicted sequence individual acts labels (which corresponds to the distribution of scores used to compute the fourth set of console outputs). Here, we can see a noticeable difference between the two types of models: while the models built on NSAA files have ~50 files that have <50\% of the correctly predicted sequence individual acts labels, models build on 6-minute walk files have ~100 files that have <50\% of the correctly predicted sequence individual acts labels. This is particularly impressive with regards to the NSAA models and shows the models proficiency to learn these individual scores. The disparity between the two model types also makes sense: the 6-minute walks don’t contain any NSAA activities within them (only the walk activity) and so we are asking the model to make predictions for the individual activities for a subject when the model itself never sees any of these NSAA activities in the files; it’s only presented with sequences from subjects with corresponding individual activities. Because it can’t as easily draw any inferences about walk data to correspond to other activity scores, it makes sense that it doesn’t perform as well compared with a model that does see these activities.




%%%%%%%%%%%%%%%%%%
\section{Model Predictions Set 2: Model Performance on Left-Out vs Non-Left-Out Files}

\begin{center}
\includegraphics[scale=0.4]{project_figures/fig11_3}
\end{center}

\begin{center}
\includegraphics[scale=0.4]{project_figures/fig11_4}
\end{center}

\quad In this model predictions set, we look at 2 subjects (‘D3’ and ‘D11’) and look at how well models perform on predicting upon them when they have seen the files in training vs when they haven’t. These two subjects were chosen as their overall NSAA scores were mid-range (i.e. weren’t close to a perfect ‘34’ and weren’t outliers like ‘3’), though we intend to repeat this for numerous other subjects in the near-future, and each table represents the results concerning a single one of these two subjects. It should be noted that the models these files are tested upon are models built on NSAA files, along with the testing files being NSAA files themselves, so the models should be familiar with this sort of data.

%%%%%%%%%
\subsection{Results Discussion}

\quad We’ll first examine the results of the ‘D3’ subject. The first row is based on models that have already seen the ‘D3’ file in training and not only is it familiar with the subject, but it is also familiar in the particular data it is being tested on. This is only useful to us as a ‘baseline’ of how well the model could theoretically perform, as this is using some of the data that has been used for training for testing purposes. This therefore holds no practical use as a metric, as files that we would present to the models in a real-world setting would obviously not have been seen by the models before. Note that this uses all the measurements that we have decided upon using (extracted statistical values and the 4 ‘useful’ raw measurements) to make its estimations upon. Unsurprisingly, it predicts all of its sequences with the correct D/HC classification, along with all of all of the single act labels. However, it is still out on its estimation of the overall NSAA score by 3, which is somewhat surprising as not only should it be closer because the models are familiar with the file’s data, but the single-act scores are completely accurate; if these single act scores are correct, then should also have an overall score of a correct value as this is simply the accumulation of these values. This discrepancy of performance of different output types is possibly due to the single-act models being better trained to deal with sequences from this file; the cause of this is prompt for further investigation and shall be a proposed solution as seen in MPS 18.\\

\quad For the second line of the table, we then observe how well models predict on files that have been left-out completely of the training and testing process. This more accurately simulates what it would be like for the model in ‘production’; that is, assessing on new files in its intended capacity. This also uses the same input types to the models as the first line in the table and thus aggregates the results from the different input types to make a single prediction for the whole file for a specific output type. We can see a noticeably steep drop in performance of the models: it only gets 23.53\% of the single act predictions correctly, the number of correct sequence classifications drops 20\% (though it still correctly determines the file to be a ‘D’ file) and the predicted overall NSAA score drops to being 11 away from the real score, down from 3. This is particularly poor generalization performance for this newly-seen file for the models and the \textbf{results are heavy motivation to continue with generalization techniques to generalise the models’ learning inferences to new files}. It’s worth noting that this was a particularly low score for ‘D’ files (with most falling in the +20 range), so this might contribute somewhat to its poor performance, though further investigation into other ‘left out’ files will be conducted later to confirm this.\\

\quad The next 5 lines of the table again look at models that have had the ‘D3’ subject ‘left out’ of the model training (done by setting the ‘rnn.py’ optional argument to ‘--leave\_out=D3’). However, instead of using all 5 measurements for the three output types and aggregating their predictions, we instead look at individual measurements in turn; that is, a measurement is extracted from ‘D3’ (e.g. joint angle) and is tested on the three that correspond to this measurement (with ‘D3’ being left out of all of them) for each output type. The aim with this is to determine whether or not any particular measurements are more useful for generalizing to new, unseen files. At this point, the results from these rows are fairly inconsistent: while models built from the position and sensor magnetic field measurements perform better with respect to the single-act scores output, the joint angle and joint angle XZY measurements perform better with respect to the D/HC classification output type. Additionally, there is not a noticeable disparity of improvement for the overall NSAA score among the measurements (with the exception of sensor magnetic field). Hence, \textbf{at this stage, this is not enough information to draw any conclusions regarding the most ideal measurements for left-out subjects when using ‘model\_predictor.py’ and more of these ‘left out’ models for other subjects are thus needed}.\\

\quad Many of the same conclusions can be drawn for the ‘D11’ subject table. For the models that are trained on all measurements when seeing ‘D11’ as a left-out file (i.e. the 2nd row of the table), however, the models are a lot better at predicting more accurate outputs: there is a much higher percent of acts correctly predicted, higher percent of correct predicted sequences, and a closer true overall NSAA score compared with that of ‘D3’; here, the model predicts the ‘D11’ overall NSAA score correctly, rather than in the case of ‘D3’ where it is off by 11. This is particularly impressive for ‘D11’, as these models obviously have never seen this subject before. A likely cause for this increase in accuracy, however, is that ‘D11’ is closer to the mean overall NSAA score amongst the subject groups: \textbf{‘D11’ is more representative of the ‘average’ subject with Duchenne, hence it is easier to generalise to the true score of the subject than it would be for an outlier}. Again, however, when we look at the left-out models but only single-measurements (rows 3 to 7 of the table), we again cannot drawn any particular conclusions with respect to one measurement or another being more useful to model generalization of unseen data; again, more of these left-out subjects (rather than just ‘D3’ and ‘D11’) are most likely needed to draw any conclusions in this regard.



%%%%%%%%%%%%%%%%%%
\section{Model Predictions Set 3: Model Performance over the Chosen 5 Left-Out Subjects}

\begin{center}
\includegraphics[scale=0.45]{project_figures/fig11_5}
\end{center}

\quad With a majority of the programming work and options added to each of the scripts, we now move onto among the key stages of the experimentation: the testing of left-out subjects to assess and improve upon generalization performance of models to assess, unseen new subjects. To this end, we chose 5 subjects (‘D3’, ‘D9’, ‘D11’, ‘D17’, and ‘HC6’). This is because these subjects show a great variety of overall NSAA scores and cover most of the spectrum of various scores we see in real-world subjects. The reason why we chose 5 subjects and not the totality of the subjects available (which amounts to approximately 30 subjects) is the disproportionate amount of time to build and assess the models for a limited amount of improvement. For example, if we chose to assess on all 30 subjects left-out, that would mean this model predictions set would take at least 6 times as long to complete for not much of a better an idea of generalisation performance, as the 5 chosen subjects to leave out of models cover the majority of the spectrum of the true overall NSAA scores.\\

\quad For this set, we built a total of 60 models. This is due to each model corresponding to each output type (D/HC classification, overall NSAA score, individual act scores), for each measurement we are concerned with (position, sensor magnetic field, joint angle, and computed statistical values), and each set of these built for a different subject that has been left out of training (5 subjects, as outlined above). In other words, the about assessments have been done on models that specifically have not seen the subject in question, but \underline{have} seen the other 4. Hence, we have $3\, output\, types\, x\, 4\, measurements\, x\, 5\, subjects=60\, models$ for this experiment set.\\

\quad It should be noted that there are a few changes made between the previous model predictions set, as evidenced by the differing values for ‘D3’ and ‘D11’. The first change is that the true overall score for subjects ‘D2’ and ‘D3’ were found to be mistakenly labelled by the assessors, and so we’ve been working with slightly-incorrect data since then for these two subjects; the reference file which the scripts use to draw the NSAA labels has now been changed to reflect these new values. The second change is that we no longer use the ‘Joint Angle XZY’ measurement. This is due to learning that this is simply a different way of displaying the joint angle data within the suit data files and provides no more useful information to the user. What’s more, by including it within the measurements, it’s essentially ‘diluting’ the impact of the other measurements when they combine their assessments.

%%%%%%%%%
\subsection{Results Discussion}

\quad From the results seen above, we can see a propensity for the overall NSAA score predictions to tend towards the median overall value among all the subjects (~28); however, we can still see that the lower true value overall NSAA scores for patients tend to have predicted scores lower than this median value. This implies that \textbf{there is still some generalisation ability to unseen subjects amongst the models, though not to the degree for which we are aiming}. This will be attempted to be rectified by various methods in the coming model predictions sets. It should also be noted that we will be paying special attention to generalising to outside of the median overall NSAA value; as we can see above for a subject with a true value of 19, the model struggles to generalise to this degree, which is something that needs to be looked into. It should also be noted that this model predictions set will serve as the ‘standard’ way of assessing these 5 left-out subjects, the performance of which shall be referenced in many of the upcoming model predictions sets when a new technique is tried to help the models generalise to unseen subjects better.


%%%%%%%%%%%%%%%%%%
\section{Model Predictions Set 4: Model Performance for 5 Subjects for Specific Measurements}

\begin{center}
\includegraphics[scale=0.45]{project_figures/fig11_6}
\end{center}

\quad This model predictions set is an extension of the work that was done in the previous set. It looks at the predictions for each of the subjects for their associated ‘left-out’ models but, along with using the aggregate predictions made by the models trained on the four measurements (now that we have decided to discount ‘jointAngleXZY’), we now look at how each of the measurements predict in turn. That is to say, each subject has one measurement type taken from it and loads the appropriate three models (one for each output type) for that measurement type and that ‘left-out’ subject, as opposed to using all measurements as the basis of the models. The aim therefore is to see if there is any particular relationship between the measurement type and its prediction ability.\\

\quad One benefit from this measurement set is that it doesn’t require us to build any more models than have already been build, as it simply uses all the models built for model predictions set 3 but we simply use fewer of the models per each of the rows in the above table (disregarding the included rows from the table in model prediction set 3). The rows copied over from model prediction set 3 also serve to show how well the aggregation ability of those rows work with respect to the true values for each given subject.

%%%%%%%%%
\subsection{Results Discussion}

\quad The main takeaway from this experiment set is that \textbf{there is no obvious measurement type that is ‘dragging down’ the aggregate predictions seen in model predictions set 3 at this stage}. For the overall NSAA score predictions, each measurement generally predicts within 1 or 2 of each other and no measurement type is generally closer to the true overall NSAA value than any of the others. The same can be said for the percentage of correctly predicted sequences (for output type ‘dhc’): there are no measurements that consistently perform better than the others to predict sequence D/HC labels. We can note two things from this metric, however: the aggregation effect for the metric for output type ‘dhc’ works in our favor, as for 4 of the 5 subjects it increases the average percentage from its predictions using single measurements to something higher with the aggregate prediction (for example, for subject D11, the average of the 4 single-measurement predictions was (85.62\% + 99.06\% + 72.97\% + 65.62\%) / 4 = 80.82\%, while the aggregate prediction over all 4 measurements was 100\%). This suggests to us that \textbf{the aggregation effect of all measurements to cover outlying predictions is useful at least for output type ‘dhc’ at this point in the experimentation}. This, however, is not observed in the same way for the percentage of acts correctly predicted (for output type ‘acts’), though it also does not show an average decrease when compared to predictions made by single measurements. Furthermore, for this metric there is again no particular measurement types that stand out as being notably and consistently better or worse than the others in being useful to estimate a higher percentage of correctly predicted single-act scores.\\

\quad Hence, at this point in the process with the models struggling to generalize to subjects that they haven’t previously seen, \textbf{we see no improvements by simplifying predictions by only considering one measurement at a time for a given subject over using an aggregation of the measurements}. Rather, for the percent of correctly predicted sequences output metric, we see an improvement in performance over 4 out of the 5 subjects when using this aggregation. Therefore, for upcoming model prediction sets, \textbf{we will continue to use measurement aggregations to make predictions for subjects left-out of the training process}.



%%%%%%%%%%%%%%%%%%
\section{Model Predictions Set 5: Comparable Performance of ‘FR\_’ vs. ‘FRC\_’ Files for ‘AD’ Models}

\begin{center}
\includegraphics[scale=0.3]{project_figures/fig11_7}
\end{center}

\quad One of the oversights of the feature reduction we perform on the computed statistical values (i.e. the ‘ft\_sel\_red.py’ script operating on the ‘AD’ output of ‘comp\_stat\_vals.py’) is that we have been reducing the file’s dimensionality on a file-by-file basis: as single files of computed statistical values come into the ‘ft\_sel\_red.py’ script, they are projected to a lower-dimensional space one file at a time. This mainly due to a programming oversight and the fact that we computed statistical values one file at a time, so it felt natural to reduce the dimensionality on a file-by-file basis as well. However, this does not guarantee that each file will be projected to the same dimensioned subspace via PCA (which is the feature reduction technique that we have been using thus far in ‘ft\_sel\_red.py’) and, moreover, it is extremely unlikely that each file will be on exactly the same feature subspace as any other; hence, they can be considered to contain different features than each other, even if they each contain the same number of features. Hence, we wanted to investigate whether or not this will have been an issue for the models by building models from computed statistical values with files having been reduced to the same lower-dimensional space; if these performed better than the previous ones, then the models’ learning potentials benefit from having computed statistical values sharing the same feature space.\\

\quad To carry out this prediction set, we first ran ‘ft\_sel\_red.py’ with the ‘--combine\_files’ optional argument set. This combines all the files vertically (i.e. stacks all the rows of all computed statistical values from all files available in the data set) on top of each other before performing dimensionality reduction with PCA before separating them back into their original files (i.e. with the same number of rows of data as before but with far fewer columns). These take the exact same form as their normally-reduced ‘.csv’ output counterparts in terms of shape, but each of these ‘newly’ reduced files share the same reduced dimensionality space. These files are placed in the same directory as the other files but with a ‘FRC\_’ (for feature-reduced via concatenation) at the beginning of the file name as opposed to ‘FR\_’ of the others. To get models specifically trained on these ‘FRC\_’ files, we build them in ‘rnn.py’ with the ‘--use\_frc’ optional argument to select these files (note: this is only done for models build on ‘AD’ rather than raw measurements as raw measurements don’t have their features reduced) and ‘model\_predictor.py’ then uses a similar ‘--use\_frc’ optional argument to select these models.

%%%%%%%%%
\subsection{Results Discussion}

\quad As we can see above, each of the 5 left-out subjects are tested on two groups models for the ‘AD’ (computed statistical values) measurement only for each of the three output types: one group contains models trained on ‘normally’ reduced computed statistical values (the ‘FR\_’ files), while the other contains the ‘concatenated’ version of the files (the ‘FRC\_’ files) which we described above. For the percent of acts correctly predicted metric (for the ‘single-acts’ output type), we see that the ‘FR\_’ files perform better for 3 of the subjects, while the ‘FRC\_’ performs better for 1 subject and they predict the same percentage for 1 subject. However, we see an improvement for 4 of the 5 subjects for the percentage of correctly predicted sequences (for the ‘dhc’ output type), suggesting that generally \textbf{models trained on computed statistical values to assess D/HC classification perform better if the stat values share the same feature subspace over all files}.\\

\quad However, we see that for the predicted overall score metric, we see that, much like for the metric for the ‘single-acts’ output type, the original version of the files (i.e. the ‘FR\_’ files) are consistently better to use to train the models to accurately assess for 4 out of the 5 subjects. As a result of these findings, even though the new ‘FRC\_’ files build models that perform better on left-out subjects for D/HC classification, this isn’t the main focus of these models, which places the predictions of accurate single-act scores and overall NSAA scores ahead of that of basic classification (as this is considered to be more important with respect to the project deliverables). As a result of these priorities, we therefore decide to continue to use the traditional way of reducing the dimensionality of files (i.e. on a file-by-file basis) and hence use the ‘FR\_’ files with which to build models.



%%%%%%%%%%%%%%%%%%
\section{Model Predictions Set 6: Chosen Subjects on Familiar vs Non-Familiar Models}

\begin{center}
\includegraphics[scale=0.4]{project_figures/fig11_8}
\end{center}

\quad It’s worth clarifying at this point what we mean by ‘familiar’ vs ‘non-familiar’ models. We consider the models that we have been using in the past several model predictions sets (i.e. where one subject was left out of training for any given model and we then use this model to assess using ‘model\_predictor.py’ the subject that was left-out of training) to be ‘non-familiar’ with respect to the left-out subject, as the model is non-familiar with all parts of the subject when it comes to assessing it. However, we also want a reference point with respect to how well the models could potentially do when it is said to have ‘understood’ the subject; at this point, this is done by seeing a lot of the subject’s data during the training process. Note that on average, the models that are familiar with the subject will have been trained on ~80\% of its data, as on average 20\% per subject will have been placed in the testing set by ‘rnn.py’ and so won’t have been used for testing.\\

\quad An important thing to note about these ‘familiar’ models that we will have built: while they show much better results than the standard non-familiar models, it is not a good indicator of the ability of the models to ‘generalize’ to new subjects (i.e. properly function in ‘production’ where we wish to assess new subjects, which is what the ‘non-familiar’ models aim to replicate). Thus, the ‘familiar’ models must be taken for what they are: an assessment of the ability of RNNs to learn from the subject data from computed statistical values and raw measurements, and not necessarily its ability to generalize to new subjects. This also provides us with essentially a ‘gold standard’: a target the models should aim for in their generalization capabilities.

%%%%%%%%%
\subsection{Results Discussion}

\quad It should be noted that, in the table above, the rows with a file name containing ‘(already seen)’ were the subject files tested on models that were trained on data that included the subject in question (and therefore no ‘--leave\_out=$<$subject name$>$’ value was set), while the ones not containing it were the same files tested on models that have not seen data from those subjects before (as done in model predictions set 3). Hence, these pairs of rows are exactly the same data, just assessed on a different set of 15 models (15 models being all combinations of each of the 5 measurements and for each of the 3 output types). Additionally, all rows that are ‘(already seen)’ file assessments are in fact trained on the same 15 models, as these 15 are just the models but with no subjects left out, so there is no reason (or way) to create models that both trained on all subjects and also exclusive to certain subjects during the assessment via ‘model\_predictor.py’.\\

\quad As we expected, we see an improvement for each of the subjects when assessed on models that have already seen some of the data of that subject before (again, due to the train/test split ratio of the models being 0.2, it’s likely that these models will have seen ~80\% of the subject files’ data during training). Notably, \textbf{for 3 of the 5 subjects, there is a significant increase in the percentage of acts correctly predicted when assessed on models familiar with the subjects}, while for the other 2 it is already very high and there being little to improve upon. The increase in D/HC classification potential is also highlighted in this set, with the ‘D17’ subject (which was previously misclassified when assessed by models not familiar with ‘D17’) now correctly classified as ‘D’. Finally, for all 5 of the subjects, \textbf{the overall NSAA scores much more closely approximate the true overall score when using models that have seen the subjects before}: in particular, ‘D3’ (which has a very low score relative to the average of all subjects) is much more closely approximated when using a model familiar with the subject, while for 2 of the 5 subjects they now achieve an exact match in scores when using ‘familiar’ models.\\

\quad Again, it’s necessary to temper the magnitude of the takeaways of this experiment set, as this is looking at models that we won’t really be using further along in the project; rather, these just help serve as a baseline for the kind of results we should be aiming to achieve with these left-out subjects when we further look at generalization techniques in upcoming model prediction sets. However, one thing that we can definitely take away is that \textbf{the model architecture shows an ability to learn D/HC classifications and individual/overall NSAA scores for a variety of severities of subjects with Duchenne, even if at this point it struggles to generalize to unseen patients}.




%%%%%%%%%%%%%%%%%%
\section{Model Predictions Set 7: Assessing Subjects Using Single-Act Files}

\begin{center}
\includegraphics[scale=0.85]{project_figures/fig11_9}
\end{center}

\begin{center}
\includegraphics[scale=0.4]{project_figures/fig11_10}
\end{center}

\quad One of the things we were curious to know was how the performance on certain NSAA activities were reflected in the overall performance of the subject: we wanted to know if there were certain single activities of the 17 undertaken by each subject that can be shown to be more ‘impactful’ to the overall assessment of the subject. Therefore, we wanted to see whether specific activities undertaken by subjects are indicative of their overall NSAA score, more so than other activities. This leads us to a hypothesis that motivates this model prediction set: \textbf{single-activity data that is consistent across multiple subjects at being able to approximate overall NSAA scores and D/HC classification would show these to be the more useful activities at assessing a subject}. The idea is that, if we found several activities of the 17 that are far better at predicting overall scores and classification than others, this could motivate assessors to pay special attention to the scoring of these activities during assessment or, alternatively or perhaps in addition, could result in the discarding of some or many of the 17 activities if they prove to have little or no correlation to the overall assessment.\\

\quad Conveniently, we have the single acts data files already in place and in the same ‘format’ as the source ‘.mat’ files for each subject (i.e. the files containing all 17 activities with ‘filler’ in between). For each of the 5 subjects, we then compare the results of these 17 activities against the assessment of the subjects done with their complete files (as done in model predictions set 3). If certain specific activities prove to be better at assessing the subject for all, or most, of the 5 left-out subjects, then we know that these activities are more useful to the assessment. It should be noted that we are testing these single-act files for each of the subjects (17 activities x 5 subjects = 85 file assessments) on the same models as was built for model predictions set 3; in other words, the ‘single-activity’ files are being tested on the same models used to predict the ‘all-activities’ files.\\

\quad It should be noted that we are not including ‘HC6’ in this model predictions set. This is more out necessity than anything else: at the present time, the reference sheet that we have for subjects does not contain entries for most of the activities for subject ‘HC6’ for their activities. Hence, rather than including only several of these activities, we omitted this subject from this model predictions set entirely. Conveniently, having an ‘HC’ subject for this set isn’t as necessary, as we are only comparing the performance of single act files to their corresponding all-act files, rather than seeing in general how the models perform in classification and regression statistics. It should also be noted that there are several activities missing from a few of the subjects; this is mainly due to the subjects having not performed those specific activities in their assessments. Rather than excluding these subjects completely, as it is only a few activities we instead decide to keep the subjects in this model predictions set and work around these missing values.\\

\quad Finally, an important thing to note here is the absence of using the ‘AD’ measurement (i.e. the computed statistical values). This is due to the inability to use ‘ft\_sel\_red.py’ for single-act ‘AD’ files. This is because many single activities that are extracted from their base files are very short in length (often around 1 second for activities such as ‘step up R’). Consequently, when we compute the statistical values over 1 second intervals, the result is that the file containing the computed statistical values for a given subject and a certain activity (e.g. ‘AD\_D3\_act7\_stats\_features.csv’) may contain only several rows of data; hence, these might have a shape of something like (3, 4000). As in many cases the number of rows are fewer than the target number of reduced columns (e.g. target of 30 columns), we can’t reduce the dimensions of these files, so we can’t use them in our RNN models (as we can’t viably setup a model with ~4000 columns). Therefore, as we are limited to the number of possible single-act files for the computed stat value measurement for activity files of >30 seconds in length (which doesn’t cover many of the activities), we decide to discard this measurement and instead compute models just based on position, joint angle, and sensor magnetic field.


%%%%%%%%%
\subsection{Results Discussion}

\quad As this table is fairly challenging to interpret at a glance, considering it has ~70 entries total, it was felt necessary to make use of the ‘predictions\_selector.py’ script in order to filter the lines based on certain metrics. We therefore ran the script three times: once to find the best 15 rows according to metric measuring the difference between the true and predicted overall NSAA values (for the ‘overall’ output type), the percentage of acts correctly predicted (for the ‘acts’ output type), and the percentage of predicted correct sequences (for the ‘dhc’ output type). The results can be seen below:

\begin{center}
\includegraphics[scale=0.2]{project_figures/fig11_11}
\end{center}

\begin{center}
\includegraphics[scale=0.2]{project_figures/fig11_12}
\end{center}

\begin{center}
\includegraphics[scale=0.2]{project_figures/fig11_13}
\end{center}

\quad As we are primarily focused on the regression capabilities of the model, our focus mainly lies with the metric shown in the first and third images. However, for the third image seen above (for the ‘acts’ output type), there are numerous lines that perform very well according to that metric; hence, we decide to at this point focus exclusively on the first image that shows the best activities for each subject ranked according to how closely they predict the overall NSAA score (with perfectly predicting resulting in a ‘0’ value, i.e. 0 difference between the true and predicted overall NSAA score). We saw a greater number of acts between 5 and 7 in this better performing region (i.e. files of act 5, act 6, or act 7 of each subject tended to better approximate the true overall NSAA score on models that hadn’t seen the subject before). However, we wanted to view this in a bit more detail and find out which activities tended to perform better or worse and see if there any trends.

\begin{center}
\includegraphics[scale=0.4]{project_figures/fig11_14}
\end{center}

\quad The graph above shows, for each activity number, the average difference between the true and predicted overall NSAA scores over the 4 subjects as plotted by the blue line, while the orange line is the average over both all subjects and all activities. Therefore, activities significantly below this line would indicate that they are more likely to be useful in predicting the overall NSAA score (and thus be a more useful activity to the assessment), and vice versa. One thing we can note is that activities 1, 2, 3, and 8 all perform significantly worse than the average. \textbf{These are the ‘stand’, ‘walk’, ‘stand up from chair’, and ‘descent box right’, and are the activities that show much lower correlation with the overall NSAA assessment for the subject, and hence are less likely to be useful in the assessment}. Conversely, we can see particularly good performance from activities 7, 9, and 10, which are the ‘climb box left’, ‘descend box left’ and ‘get to sitting’; these are \textbf{more likely to be activities that the assessor should pay special attention to according to the above results}.\\

\quad It should be noted before making any conclusions that these are far from complete recommendations for specialists due to:

\begin{itemize}
	\item Only considering the chosen 5 subjects (reduced to 4 due to not having the single-act data for ‘HC6’).
	\item Models struggling to generalize at this point, giving worse scores than if we were using models that generalized better.
	\item Cross-confirmation of single-act times entered into the reference table is needed; as multiple users entered certain times into the table, it’s advisable for everyone involved in creating it to confer and agree upon exact times for each subject’s individual activities in order to ensure accurate activity times from the files.
\end{itemize}

\quad While the above results give us an indication of what activities are the most and least useful, this does prove to potentially be fertile ground to draw conclusions about the NSAA assessment at a later point with more improved models and techniques, which we shall explore further in MPS 24.






%%%%%%%%%%%%%%%%%%
\section{Model Predictions Set 8: Model Performance over the 5 Subjects w/ Outlier Excluded}

\begin{center}
\includegraphics[scale=0.6]{project_figures/fig11_15}
\end{center}

\quad 	One of the features of this data set is that it contains one subject (‘D10’) that is an outlier; not for medical reasons (i.e. not because he suffers much worse with DMD in comparison with the other subjects), but rather as a consequence of him not undertaking the complete assessment. He instead only undertook 3 of the 17 activities as part of the NSAA assessment for what are believed to be personal reasons. Hence, a score of 3 for the overall NSAA score was recorded for that subject. This is significantly lower than the next lowest score of ‘15’ for subject ‘D2’, and does not reflect his ‘true’ overall NSAA score, which should be at least in the mid-teens. We believed that this may affect the models’ ability to generalise to new subjects, as it is forced to learn a wider range of scores for subjects: with the ‘D10’ included, the model has to learn scores of between ‘3’ and ‘34’, while with it not included it only has to learn how to predict overall scores of between ‘15’ and ‘34’. The idea here was that, as these particularly-low scores aren’t a true reflection of the subject in question (as the subject only achieved this score due to real-world circumstances), by discarding this subject from the training set, it will make it easier for the models to learn scores within this narrower range, while excluding an incorrectly assessed subject and hopefully better generalise.\\

\quad 	To achieve this, we simply modified the ‘rnn.py’ script to take in several subject names for the ‘--leave\_out’ argument and pass in the ‘D10’ subject as the second part of this argument. For example, for the ‘D3’ row above we built it with models from the ‘--leave\_out=D3’ argument, while for the ‘D3 (other\_lo=D10)’, we built it with the ‘--leave\_out=D3,D10’ argument. We then specified the ‘model\_predictor.py’ script to search for these models with multiple subjects left-out via the ‘--other\_lo’ optional argument.


%%%%%%%%%
\subsection{Results Discussion}

\quad 	The findings from this set of model predictions weren’t particularly promising. Focusing on the performance for the ‘overall’ output type, the predicted overall NSAA scores did not seem to improve with leaving the ‘D10’ subject out of training completely; rather, for 3 of the 5 subjects they got worse with the ‘HC6’ subject getting worse by being 3 further away from the true score. And while 2 of the 5 subjects showed an improvement in approximating the overall score better, \textbf{in leaving out the ‘D10’ subject we saw an increase in cumulative difference between the true and predicted scores from 20 to 23}. And while we saw performance increase for the percent of acts correctly predicted metric (for the ‘acts’ output type), we also saw a diminishing performance with respect to the ‘dhc’ output type: \textbf{when having ‘D10’ left out of the training set, only 3 of the 5 subjects have the correct label predicted, while including this subject we see 4 of the 5 subjects with the correct label}.\\

\quad Rather than helping the model to more easily learn the scores within what we would consider the ‘normal’ range (i.e. not including particularly low scores like ‘3’) and help with the generalization ability, in removing the ‘D10’ subject the models tend to perform worse. This can be most likely attributed to the ‘D10’ subject adding some much-needed noise to the data set. It’s been well documented that machine learning models often need noise in the data to better perform on unseen data and thus to be said to be truly ‘learning’. \textbf{Hence, including this subject with a particularly low score seems to help the models in predicting other subjects within the ‘normal’ range by adding a ‘noisy’ sample}. This also presents a good argument to be experimenting with adding noise to the data set, which we shall explore further in MPS 15.



%%%%%%%%%%%%%%%%%%
\section{Model Predictions Set 9: Model Performance for 5 Subjects on Single-Act-Concat Models}

\begin{center}
\includegraphics[scale=0.5]{project_figures/fig11_16}
\end{center}

\quad A feature of the NSAA assessment data files that we get as source data is that it contains the full recording of the subject’s NSAA assessment, though these are sometimes broken down into 2 separate files. While these usually contain all 17 of the activities performed by the subject, the files often contain data that is not quite as useful; for example, the subjects standing around before they undertake the next activity (possibly listening to instructions from the assessor about what activity to do at the time). Additionally, the start and end times of the activities that were recorded in the Google annotation sheet only include the start and end times for the activities that were completed and that could only include one of each activity within the time window. Hence, all data that was captured by the suit where the subject either failed to complete the activity or did the activity twice was also captured in these files. However, when ‘mat\_act\_div.py’ is used on these source files to divide up the files, we selected only the parts of the file where the activities were completed; hence, much of this data was discarded.\\

\quad An option that was created as part of the ‘mat\_act\_div.py’ script was the ability to ‘recombine’ the single-act ‘.mat’ files into a single file per subject again. Much like the source ‘.mat’ file, this recombined file (which we refer to as a ‘single-act-concat’ file), contains all the activities for the assessment done by the subject in question. However, where it differs from the original source file is that it has a lot of data cut out of it, data that includes the subject just standing around or trying but failing to complete activities one or multiple times. A lot of this data was therefore not considered to be particularly ‘useful’ for training the model, at least in comparison to the completed activities. Moreover, it was thought that some of this ‘less-useful’ data (e.g. subjects standing around for extended periods of time) would possibly bias the model away from learning important insights from the ‘real’ activities within the data, as the models would have to account for both the useful and less-useful data in training. To this end, the ‘mat\_act\_div.py’ script was run to divide up and recombine the files, with the ‘comp\_stat\_vals.py’ and ‘ext\_raw\_measures.py’ scripts being then used to obtain the computed stat values and raw measurements in the usual manner, respectively, followed by ‘rnn.py’ being trained with the ‘--use\_sac’ argument to use build models from these ‘single-act-concat’ versions of the computed statatistical values and raw measurements files, and finally ‘model\_predictor.py’ being run on the left-out subjects assessed on the models built with ‘--use\_sac’.

%%%%%%%%%
\subsection{Results Discussion}

\quad There are three types of ‘left-out’ subjects that we consider in ‘model\_predictions.csv’ for each subject. The first of the three is the ‘baseline’; i.e. the results obtained in model predictions set 3 for subjects that were assessed on models that weren’t built on ‘single-act-concat’ files, but rather files that didn’t have single-activities extracted from them. The second type were for ‘single-act-concat’ subject files that were assessed using models trained on these ‘single-act-concat’ files; hence, to predict on these subjects, one would first need to run ‘mat\_act\_div.py’ along with ‘comp\_stat\_vals.py’ and ‘ext\_raw\_measures.py’ before assessing these subjects. The third type was for standard subject files (i.e. the same as those used the first time as done for model predictions set 3 with no single activities extracted from them) but assessing on models built on ‘single-act-concat’ files; the idea of looking at both varieties of assessing file on models built on ‘single-act-concat’ files came from the desire to see whether models were any better or worse at assessing the subject when presented with files that contained this ‘less-useful’ data.\\

\quad However, we can very clearly see from the above tables that\textbf{ both varieties of assessed files (single-act-concat files or normal) assessed on models built on single-act-concat files performed worse than their counterparts which were assessed on models built from the normal data files as in MPS 3}. This was the case over all 3 metrics with no improvements anywhere, which was unlike any other model prediction set we’ve seen thus far where even with changes that produced overall worse models, they still showed some signs of improvement for some metric(s) and some subject(s). While disappointing that this new approach didn’t lead to any improvement in results, it isn’t entirely surprising. For one, we are excluding a lot of the data: in the process of extracting small pieces of the source files and knitting them back together, we reduce the amount of raw data we have available by a factor of between 5 and 10, depending on the subject (generally higher if the subject has a lower overall NSAA score as the subject generally takes more attempts at completing each activity). Even though we consider a lot of this data that we excluded ‘less-useful’, \textbf{the removal of not-as-useful data is not enough to overcome to loss in performance as a consequence of having much less overall data}. Furthermore, in the same way as in model predictions set 8 that removing a subject most likely showed worse results due to a comparable lack of ‘noise’ now present in the data set, there is a good chance that the same thing happened here, where the data of the subjects ‘standing around’ and other minor activities (that weren’t necessarily a part of the NSAA assessment such as raising one’s arm) may have added some much-needed ‘robustness’ to the data set.\\

\quad Additionally, another theory as to why models trained on ‘single-act-concat’ files performed not-as-well as models built on ‘normal’ files could be due to the fact that the capturing of activities that weren’t properly completed by the subject (which aren’t included as part of the ‘single-act-concat’ files) gave a fair bit of information about the subjects to the models. This is evidenced by the fact that for the lowest-scoring 2 of the 5 subjects, the models trained on ‘single-act-concat’ files predict a higher overall NSAA score than the models trained on the ‘normal’ files, where \textbf{the inclusion of the ‘failed’ activities most likely gave the models a better sense for the lower-scoring subjects that they should have a corresponding lower score}. Therefore, the main takeaway we have of this model predictions set is that \textbf{it is worthwhile using all of the available data captured by the bodysuit as opposed to only using the data showing the completed activities}.
























%\begin{figure}[tb]
%\centering
%\includegraphics[width = 0.4\hsize]{./figures/imperial}
%\caption{Imperial College Logo. It's nice blue, and the font is quite stylish. But you can choose a different one if you don't like it.}
%\label{fig:logo}
%\end{figure}

%Figure~\ref{fig:logo} is an example of a figure. 



%% bibliography
\bibliographystyle{apa}


\end{document}
